<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Quotes on Statistics, Data Visualization and Science • statquotes</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png">
<link rel="icon" type="”image/svg+xml”" href="../favicon.svg">
<link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" sizes="any" href="../favicon.ico">
<link rel="manifest" href="../site.webmanifest">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Roboto-0.4.10/font.css" rel="stylesheet">
<link href="../deps/JetBrains_Mono-0.4.10/font.css" rel="stylesheet">
<link href="../deps/Roboto_Slab-0.4.10/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Quotes on Statistics, Data Visualization and Science">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">statquotes</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.3</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/statquotes.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/statquotes-database.html">Creating, updating &amp; maintaining the statquotes data base</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/friendly/statquotes/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Quotes on Statistics, Data Visualization and Science</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/friendly/statquotes/blob/HEAD/vignettes/statquotes.Rmd" class="external-link"><code>vignettes/statquotes.Rmd</code></a></small>
      <div class="d-none name"><code>statquotes.Rmd</code></div>
    </div>

    
    
<ol style="list-style-type: decimal">
<li>
<p>You can see a lot, just by looking.</p>
<p>– <em>Yogi Berra</em> <sub>(<em>#data
visualization,vision</em>)</sub></p>
</li>
<li>
<p>Did you ever see such a thing as a drawing of a muchness?</p>
<p>– <em>Lewis Carroll, Alice in Wonderland</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>The critical requirement of an effective graphical display is
that it stimulate spontaneous perceptions of <em>structure</em> in
data.</p>
<p>– <em>S. Smith et al. 1990</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>Like good writing, producing an effective graphical display
requires an understanding of <em>purpose</em>—<em>what</em> is to be
communicated, and to <em>whom</em>.</p>
<p>– <em>Michael Friendly, Gallery of Data Visualization, 1991</em>
<sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>Have you ever seen voice mail?</p>
<p>– <em>The Hackers Test</em> <sub>(<em>#data
visualization,vision</em>)</sub></p>
</li>
<li>
<p>Graphics is the visual means of resolving logical problems.</p>
<p>– <em>Jacques Bertin, Graphics and Graphic Information Processing,
2011, p. 16.</em> <sub>(<em>#data
visualization,vision</em>)</sub></p>
</li>
<li>
<p>I have come to feel that my central interest is in data analysis,
which I take to include, among other things: procedures for analyzing
data, techniques for interpreting the results of such procedures, ways
of planning the gathering of data to make its analysis easier, more
precise or more accurate, and all the machinery and results of
(mathematical) statistics which apply to analyzing data.</p>
<p>– <em>NA, The Future of Data Analysis, Annals of Mathematical
Statistics, 1962, Vol. 33 (1), 1962. Page 2.</em></p>
</li>
<li>
<p>Many have forgotten that data analysis can, sometimes quite
appropriately, precede probability models, that progress can come from
asking what a specified indicator ( = a specified function of the data)
may reasonably be regarded as estimating. Escape from this constraint
can do much to promote novelty.</p>
<p>– <em>NA, The Future of Data Analysis, Annals of Mathematical
Statistics, 1962, Vol. 33 (1), 1962. Page 5.</em></p>
</li>
<li>
<p>If data analysis is to be well done, much of it must be a matter
of judgment, and ‘theory’ whether statistical or non-statistical, will
have to guide, not command.</p>
<p>– <em>John Tukey, The Future of Data Analysis, Annals of Mathematical
Statistics, 1962, Vol. 33 (1), 1962.</em></p>
</li>
<li>
<p>The physical sciences are used to ‘praying over’ their data,
examining the same data from a variety of points of view. This process
has been very rewarding, and has led to many extremely valuable
insights. Without this sort of flexibility, progress in physical science
would have been much slower. Flexibility in analysis is often to be had
honestly at the price of a willingness not to demand that what has
already been observed shall establish, or prove, what analysis suggests.
In physical science generally, the results of praying over the data are
thought of as something to be put to further test in another experiment,
as indications rather than conclusions.</p>
<p>– <em>John Tukey, The Future of Data Analysis, Annals of Mathematical
Statistics, 1962, Vol. 33 (1), 1962.</em></p>
</li>
<li>
<p>If one technique of data analysis were to be exalted above all
others for its ability to be revealing to the mind in connection with
each of many different models, there is little doubt which one would be
chosen. The simple graph has brought more information to the data
analyst’s mind than any other device. It specializes in providing
indications of unexpected phenomena.</p>
<p>– <em>John Tukey, The Future of Data Analysis, The Annals of
Mathematical Statistics, 1962, Vol. 33, No. 1. Page 49.</em>
<sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>The most important maxim for data analysis to heed…is this: Far
better an approximate answer to the right question, which is often
vague, than an exact answer to the wrong question, which can always be
made precise.</p>
<p>– <em>John Tukey, The Future of Data Analysis, The Annals of
Mathematical Statistics, 1962, Vol. 33, No. 1. Page 13-14.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>The greatest possibilities of visual display lie in vividness and
inescapability of the intended message. A visual display can stop your
mental flow in its tracks and make you think. A visual display can force
you to notice what you never expected to see.</p>
<p>– <em>John Tukey</em> <sub>(<em>#data
visualization,vision</em>)</sub></p>
</li>
<li>
<p>The purpose of [data] display is comparison (recognition of
phenomena), not numbers … The phenomena are the main actors, numbers are
the supporting cast.</p>
<p>– <em>John Tukey</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>…But it is not always clear <em>which</em> 1000 words.</p>
<p>– <em>John Tukey, 1973</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>The best thing about being a statistician is that you get to play
in everyone’s backyard.</p>
<p>– <em>John Tukey</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>The worst, i.e., most dangerous, feature of ‘accepting the null
hypothesis’ is the giving up of explicit uncertainty … Mathematics can
sometimes be put in such black-and-white terms, but our knowledge or
belief about the external world never can. <a href="doi:10.1214/ss/1177011945" class="uri">doi:10.1214/ss/1177011945</a></p>
<p>– <em>John Tukey, The Philosophy of Multiple Comparisons, Statistical
Science, 1991, 6, 100-116.</em>
<sub>(<em>#statistics,nhst</em>)</sub></p>
</li>
<li>
<p>Statisticians classically asked the wrong question–and were
willing to answer with a lie, one that was often a downright lie. They
asked “Are the effects of A and B different?” and they were willing to
answer “no”. All we know about the world teaches us that the effects of
A and B are always different–in some decimal place–for every A and B.
Thus asking “Are the effects different?” is foolish. What we should be
answering first is “Can we tell the direction in which the effects of A
differ from the effects of B?” In other words, can we be confident about
the direction from A to B? Is it “up”, “down” or “uncertain”? <a href="doi:10.1214/ss/1177011945" class="uri">doi:10.1214/ss/1177011945</a></p>
<p>– <em>John Tukey, The Philosophy of Multiple Comparisons, Statistical
Science, 1991, 6, 100-116.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Empirical knowledge is always fuzzy! And theoretical knowledge,
like all the laws of physics, as of today’s date, is always wrong-in
detail, though possibly providing some very good approximations indeed.
<a href="doi:10.1214/ss/1177011945" class="uri">doi:10.1214/ss/1177011945</a></p>
<p>– <em>John Tukey, The Philosophy of Multiple Comparisons, Statistical
Science, 1991, 6, 100-116.</em></p>
</li>
<li>
<p>The combination of some data and an aching desire for an answer
does not ensure that a reasonable answer can be extracted from a given
body of data.</p>
<p>– <em>John W. Tukey, Sunset Salvo, The American Statistician Vol. 40
(1), 1986.</em> <sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>The practical power of a statistical test is the product of its’
statistical power and the probability of use.</p>
<p>– <em>John W. Tukey, A Quick, Compact, Two Sample Test to Duckworth’s
Specifications</em> <sub>(<em>#science,power</em>)</sub></p>
</li>
<li>
<p>Since the aim of exploratory data analysis is to learn what seems
to be, it should be no surprise that pictures play a vital role in doing
it well.</p>
<p>– <em>John W. Tukey, John W. Tukey’s Works on Interactive Graphics.
The Annals of Statistics Vol. 30, No. 6 (Dec., 2002), pp. 1629-1639</em>
<sub>(<em>#data visualization,pictures,eda</em>)</sub></p>
</li>
<li>
<p>There is nothing better than a picture for making you think of
questions you had forgotten to ask (even mentally).</p>
<p>– <em>John W. Tukey &amp; Paul Tukey, John W. Tukey’s Works on
Interactive Graphics. The Annals of Statistics Vol. 30, No. 6 (Dec.,
2002), pp. 1629-1639</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>The greatest value of a picture is when it forces us to notice
what we never expected to see.</p>
<p>– <em>John Tukey, Exploratory Data Analysis, 1977, p. vi</em>
<sub>(<em>#data visualization,pictures,eda</em>)</sub></p>
</li>
<li>
<p>Exploratory data analysis can never be the whole story, but
nothing else can serve as the foundation stone – as the first step.</p>
<p>– <em>John Tukey, Exploratory Data Analysis, 1977, p.3.</em>
<sub>(<em>#statistics,data,data analysis</em>)</sub></p>
</li>
<li>
<p>Unless exploratory data analysis uncovers indications, usually
quantitative ones, there is likely to nothing for confirmatory data
analysis to consider.</p>
<p>– <em>John Tukey, Exploratory Data Analysis, 1977, p. 3.</em>
<sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>One thing the data analyst has to learn is how to expose himself
to what his data are willing–or even anxious–to tell him. Finding clues
requires looking in the right places and with the right magnifying
glass.</p>
<p>– <em>John Tukey, Exploratory Data Analysis, 1977, p. 21.</em>
<sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>In data analysis, a plot of y against x may help us when we know
nothing about the logical connection from x to y–even when we do not
know whether or not there is one–even when we know that such a
connection is impossible.</p>
<p>– <em>John Tukey, Exploratory Data Analysis, p. 131.</em>
<sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>Whatever the data, we can try to gain understanding by
straightening or by flattening. When we succeed in doing one or both, we
almost always see more clearly what is going on.</p>
<p>– <em>John Tukey, Exploratory Data Analysis, 1977, p. 148.</em>
<sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>A competent data analysis of an even moderately complex set of
data is a thing of trials and retreats, of dead ends and branches.</p>
<p>– <em>John Tukey, Computer Science and Statistics: Proceedings of the
14th Symposium on the Interface, p. 4.</em> <sub>(<em>#data,data
analysis</em>)</sub></p>
</li>
<li>
<p>There is no more reason to expect one graph to ‘tell all’ than to
expect one number to do the same.</p>
<p>– <em>John Tukey, Exploratory Data Analysis, 1977, p. 157</em>
<sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>There is no excuse for failing to plot and look.</p>
<p>– <em>John Tukey, Exploratory Data Analysis, 1997, p. 43</em>
<sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>One is so much less than two. [John Tukey’s eulogy of his
wife.]</p>
<p>– <em>John Tukey, The life and professional contributions of John W.
Tukey, The Annals of Statistics, 2001, Vol 30, p. 46.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>No one has ever shown that he or she had a free lunch. Here, of
course, “free lunch” means “usefulness of a model that is locally easy
to make inferences from”.</p>
<p>– <em>John Tukey, Issues relevant to an honest account of data-based
inference, partially in the light of Laurie Davies’ paper.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>If asymptotics are of any real value, it must be because they
teach us something useful in finite samples. I wish I knew how to be
sure when this happens.</p>
<p>– <em>John Tukey, Issues relevant to an honest account of data-based
inference, partially in the light of Laurie Davies’ paper.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Statistics is a science in my opinion, and it is no more a branch
of mathematics than are physics, chemistry, and economics; for if its
methods fail the test of experience–not the test of logic–they will be
discarded.</p>
<p>– <em>John Tukey, The life and professional contributions of John W.
Tukey, by David Brillinger, The Annals of Statistics, 2001, Vol 30.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>One Christmas Tukey gave his students books of crossword puzzles
as presents. Upon examining the books the students found that Tukey had
removed the puzzle answers and had replaced them with words of the
sense: “Doing statistics is like doing crosswords except that one cannot
know for sure whether one has found the solution.”</p>
<p>– <em>John Tukey, The life and professional contributions of John W.
Tukey, by David Brillinger, The Annals of Statistics, 2001, Vol 30,
p. 22.</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>A sort of question that is inevitable is: “Someone taught my
students exploratory, and now (boo hoo) they want me to tell them how to
assess significance or confidence for all these unusual functions of the
data. Oh, what can we do?” To this there is an easy answer: TEACH them
the JACKKNIFE.</p>
<p>– <em>John Tukey, We Need Both Exploratory and Confirmatory, The
American Statistician, Vol 34, No 1, p. 25.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>John Tukey’s eye for detail was amazing. When we were preparing
some of the material for our book (which was published last year), it
was most disconcerting to have him glance at the data and question one
value out of several thousand points. Of course, he was correct and I
had missed identifying this anomaly.</p>
<p>– <em>Kaye Basford</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Many students are curious about the ‘1.5 x IQR Rule’;, i.e. why
do we use Q1 - 1.5 x IQR (or Q3 + 1.5 x IQR) as the value for deciding
if a data value is classified as an outlier? Paul Velleman, a
statistician at Cornell University, was a student of John Tukey, who
invented the boxplot and the 1.5 x IQR Rule. When he asked Tukey, ‘Why
1.5?’, Tukey answered, ‘Because 1 is too small and 2 is too large.’
[Assuming a Gaussian distribution, about 1 value in 100 would be an
outlier. Using 2 x IQR would lead to 1 value in 1000 being an
outlier.]</p>
<p>– <em>Unknown</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>It is a rare thing that a specific body of data tells us as
clearly as we would wish how it itself should be analyzed.</p>
<p>– <em>John Tukey, Exploratory Data Analysis, 1977, p. 397.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Just which robust/resistant methods you use is not important–what
is important is that you use some. It is perfectly proper to use both
classical and robust/resistant methods routinely, and only worry when
they differ enough to matter. But, when they differ, you should think
hard.</p>
<p>– <em>John Tukey, Quoted by Doug Martin</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>I believe that there are many classes of problems where Bayesian
analyses are reasonable, mainly classes with which I have little
acquaintance.</p>
<p>– <em>John Tukey, The life and professional contributions of John W.
Tukey, The Annals of Statistics, 2001, Vol 30, p. 45.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>The twin assumptions of normality of distribution and homogeneity
of variance are not ever exactly fulfilled in practice, and often they
do not even hold to a good approximation.</p>
<p>– <em>John W. Tukey, The problem of multiple comparisons. 1973.
Unpublished manuscript, Dept. of Statistics, Princeton University.</em>
<sub>(<em>#normality</em>)</sub></p>
</li>
<li>
<p>If we need a short suggestion of what exploratory data analysis
is, I would suggest that: 1. it is an attitude, AND 2. a flexibility,
AND 3. some graph paper (or transparencies, or both).</p>
<p>– <em>John W. Tukey, Jones, L. V. (Ed.). (1986). The collected works
of John W. Tukey: Philosophy and principles of data analysis 1949-1964
(Vols. III &amp; IV). London: Chapman &amp; Hall.</em>
<sub>(<em>#eda,data visualization</em>)</sub></p>
</li>
<li>
<p>Three of the main strategies of data analysis are: 1. graphical
presentation. 2. provision of flexibility in viewpoint and in
facilities, 3. intensive search for parsimony and simplicity.</p>
<p>– <em>John W. Tukey, Jones, L. V. (Ed.). (1986). The collected works
of John W. Tukey: Philosophy and principles of data analysis 1949-1964
(Vols. III &amp; IV). London: Chapman &amp; Hall.</em> <sub>(<em>#data
visualization,data analysis</em>)</sub></p>
</li>
<li>
<p>Genius seems to consist merely in trueness of sight.</p>
<p>– <em>Ralph Waldo Emerson, Journals of Ralph Waldo Emerson, Entry
dated 1835, May 11</em> <sub>(<em>#data
visualization,vision</em>)</sub></p>
</li>
<li>
<p>The eye obeys exactly the action of the mind.</p>
<p>– <em>Ralph Waldo Emerson, Representative men. English traits.
Conduct of life, p.409</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>Vision is the art of seeing things invisible.</p>
<p>– <em>Johnathan Swift, 1711</em> <sub>(<em>#data
visualization,vision</em>)</sub></p>
</li>
<li>
<p>When there is no vision, the people perish.</p>
<p>– <em>Proverbs 29:18</em> <sub>(<em>#data
visualization,vision</em>)</sub></p>
</li>
<li>
<p>If I can’t picture it, I can’t understand it.</p>
<p>– <em>Albert Einstein</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>And those who have insight will shine brightly like the
brightness of the expanse of Heaven.</p>
<p>– <em>Daniel 12:3</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>The one thing that marks the true artist is a clear perception
and a firm, bold hand, in distinction from that imperfect mental vision
and uncertain truth which give up the feeble pictures and the lumpy
statues of the mere artisans on canvas or in stone.</p>
<p>– <em>Oliver Wendell Holmes (1860), The Professor at the Breakfast
Table Ticknor and Fields, Boston, MA</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>I like your motto: One picture is worth 1,000 denials.</p>
<p>– <em>Ronald Reagan to White House News Photographers Assn, 18 May
1983</em> <sub>(<em>#data visualization,pictures</em>)</sub></p>
</li>
<li>
<p>With brush you paint the possibilities with pens you scribe the
probabilities for in pictures we find insight while in numbers find we
strength.</p>
<p>– <em>Forrest W. Young</em> <sub>(<em>#data
visualization,eda,pictures</em>)</sub></p>
</li>
<li>
<p>A graphic should not only show the leaves it should show the
branches as well as the entire tree.</p>
<p>– <em>Jacques Bertin, The Semiology of Graphics, 1983. Translated by
W. J. Berg. University of Wisconsin Press : Wisconsin.</em>
<sub>(<em>#data visualization,eda</em>)</sub></p>
</li>
<li>
<p>Tables are like cobwebs, like the sieve of Danaides; beautifully
reticulated, orderly to look upon, but which will hold no conclusion.
Tables are abstractions, and the object a most concrete one, so
difficult to read the essence of.</p>
<p>– <em>Thomas Carlyle, Chartism, 1840, Chapter II, Statistics</em>
<sub>(<em>#data visualization,tables</em>)</sub></p>
</li>
<li>
<p>A judicious man looks at Statistics, not to get knowledge, but to
save himself from having ignorance foisted on him.</p>
<p>– <em>Thomas Carlyle, Chartism, 1840, Chapter II, Statistics</em>
<sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>Although geometrical representations of propositions in the
thermodynamics of fluids are in general use and have done good service
in disseminating clear notions in this science, yet they have by no
means received the extension in respect to variety and generality of
which they are capable.</p>
<p>– <em>J. Willard Gibbs, Graphical Methods in the Thermodynamics of
Fluids, 1873</em> <sub>(<em>#data
visualization,geometry</em>)</sub></p>
</li>
<li>
<p>Although we often hear that data speak for themselves, their
voices can be soft and sly.</p>
<p>– <em>Frederick Mosteller, Stephen Fienberg and Robert E. Rourke,
Beginning Statistics with Data Analysis 1983, Reading MA, p. 234</em>
<sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>Nocturne, of Chopin, so beautiful music. But few people will
appreciate the music if I just show them the notes. Most of us need to
listen to the music to understand how beautiful it is. But often that’s
how we present statistics; we just show the notes, we don’t play the
music.</p>
<p>– <em>Hans Rosling, OECD World Forum, Istanbul, June 2007</em>
<sub>(<em>#data visualization,statistics</em>)</sub></p>
</li>
<li>
<p>If an editor should print bad English he would lose his position.
Many editors are using and printing bad methods of graphic presentation,
but they hold their jobs just the same.</p>
<p>– <em>W. C. Brinton, Graphic methods of presenting facts 1914,
p. 3.</em> <sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>Around the turn of the century, Karl Pearson, an almost elemental
force for more and better statistical thought in all areas of life,
including with gusto, matters of social policy, was thinking and
lecturing about graphical methods. But later in Pearson’s life, and
certainly in the careers of R. A. Fisher and the other great statistical
minds of the first half of the century, there was a falling away of
interest in graphics and an efflorescence of devotion to analytical
mathematical methods. Indeed, for many years there was a contagious
<em>snobbery</em> against so unpopular, vulgar and elementary a topic as
graphics among academic statisticians and their students</p>
<p>– <em>William Kruskal</em> <sub>(<em>#data
visualization,statistics</em>)</sub></p>
</li>
<li>
<p>If statistical graphics, although born just yesterday, extends
its reach every day, it is because it replaces long tables of numbers
and it allows one not only to embrace at glance the series of phenomena,
but also to signal the correspondences or anomalies, to find the causes,
to identify the laws.</p>
<p>– <em>Emile Cheysson, c. 1877</em> <sub>(<em>#data
visualization,tables</em>)</sub></p>
</li>
<li>
<p>Numbers have an important story to tell. They rely on you to give
them a clear and convincing voice.</p>
<p>– <em>Stephen Few</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>The purpose of visualization is insight, not pictures.</p>
<p>– <em>Ben Shneiderman, Extreme visualization: squeezing a billion
records into a million pixels. In SIGMOD ’08: Proceedings of the 2008
ACM SIGMOD international conference on Management of data, pages 3-12,
New York, NY, USA, 2008. ACM.</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>I love taxonomies, categories, ways of dividing people into
groups.</p>
<p>– <em>Gretchen Rubin</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>Ballerinas are often divided into three categories: jumpers,
turners and balancers.</p>
<p>– <em>Robert Gottlieb</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>Mr. Funkhouser has made an extremely interesting and valuable
contribution to the history of statistical method. I wish, however, that
he could have added a warning, supported by horrid examples, of the
evils of the graphical method unsupported by tables of figures. Both for
accurate understanding, and particularly to facilitate the use of the
same material by other people, it is essential that graphs should not be
published by themselves, but only when supported by the tables which
lead up to them. It would be an exceedingly good rule to forbid in any
scientific periodical the publication of graphs unsupported by
tables.</p>
<p>– <em>John Maynard Keynes, Review of Funkhouser for The Economic
Journal</em> <sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>Without data you are just another person with an opinion.</p>
<p>– <em>Unknown (perhaps incorrectly attributed to W. Edwards
Deming)</em> <sub>(<em>#data visualization,data</em>)</sub></p>
</li>
<li>
<p>Without a plot you are just a person missing a convincing
argument.</p>
<p>– <em>Di Cook, 2016</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>Whatever relates to extent and quantity may be represented by
geometrical figures. Statistical projections which speak to the senses
without fatiguing the mind, possess the advantage of fixing the
attention on a great number of important facts.</p>
<p>– <em>Alexander von Humboldt</em> <sub>(<em>#data
visualization,vision</em>)</sub></p>
</li>
<li>
<p>Segnius irritant animos demissa per aures, Quam quae sunt oculus
subjecta fidelibus (Roughly: What we hear excites the mind less than
what we see).</p>
<p>– <em>Horace</em> <sub>(<em>#data
visualization,vision</em>)</sub></p>
</li>
<li>
<p>You see, but you do not observe. The distinction is clear.</p>
<p>– <em>Sherlock Holmes, The Adventures of Sherlock Holmes (1890), “A
Scandal in Bohemia”, p. 162</em> <sub>(<em>#data
visualization,vision</em>)</sub></p>
</li>
<li>
<p>Every picture tells a story.</p>
<p>– <em>Rod Stewart, 1971</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>A picture is worth a ten thousand words.</p>
<p>– <em>Fred R. Barnard, advertising trade journal Printers Ink, March
10 1927.</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>Un croquis vaut mieux qu’un long discours. Tr.: A good sketch is
better than a long speech.</p>
<p>– <em>Napoleon Bonaparte</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>A picture is worth a thousand numbers.</p>
<p>– <em>Anon</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>Show me your flowcharts and conceal your tables, and I shall
continue to be mystified. Show me your tables, and I won’t usually need
your flowcharts; they’ll be obvious.</p>
<p>– <em>Fred Brooks, The Mythical Man-Month</em> <sub>(<em>#data
visualization,pictures,tables</em>)</sub></p>
</li>
<li>
<p>Look here, upon this picture, and on this.</p>
<p>– <em>Shakespeare, Hamlet</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>I am only a picture-taster, the way others are wine- or
tea-tasters.</p>
<p>– <em>Bernard Berenson, Sunset and Twilight Harcourt, Brace &amp;
World, 1963</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>Getting information from a table is like extracting sunlight from
a cucumber.</p>
<p>– <em>Arthur B. Farquhar &amp; Henry Farquhar, Economic and
Industrial Delusions , 1891.</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>When a law is contained in figures, it is buried like metal in an
ore; it is necessary to extract it. This is the work of graphical
representation. It points out the coincidences, the relationships
between phenomena, their anomalies, and we have seen what a powerful
means of control it puts in the hands of the statistician to verify new
data, discover and correct errors with which they have been stained.</p>
<p>– <em>Emile Cheysson, Les methods de la statistique (1890),
34-35.</em> <sub>(<em>#data visualization,pictures</em>)</sub></p>
</li>
<li>
<p>Good design is obvious. Great design is transparent.</p>
<p>– <em>Joe Sparano, graphic designer for Oxide Design Co.</em>
<sub>(<em>#data visualization,design</em>)</sub></p>
</li>
<li>
<p>Content precedes design. Design in the absence of content is not
design, it’s decoration.</p>
<p>– <em>Jeffrey Zeldman, web designer and entrepreneur</em>
<sub>(<em>#data visualization,design</em>)</sub></p>
</li>
<li>
<p>Mankind is not a circle with a single center but an ellipse with
two focal points of which facts are one and ideas the other.</p>
<p>– <em>Victor Hugo</em> <sub>(<em>#data
visualization,ellipses,geometry</em>)</sub></p>
</li>
<li>
<p>So, Fabricius, I already have this: that the most true path of
the planet [Mars] is an ellipse, which Durer also calls an oval, or
certainly so close to an ellipse that the difference is insensible.</p>
<p>– <em>Johannes Kepler, 1605</em> <sub>(<em>#data
visualization,ellipses</em>)</sub></p>
</li>
<li>
<p>Programming graphics in X is like finding the square root of pi
using Roman numerals.</p>
<p>– <em>Henry Spencer</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>The purpose of computing is insight, not numbers.</p>
<p>– <em>Richard Hamming, Introduction To Applied Numerical
Analysis</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>… to be a good theoretical statistician one must also compute,
and must therefore have the best computing aids.</p>
<p>– <em>Frank Yates, Sampling Methods for Censuses and Surveys
1949</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>We [he and Halmos] share a philosophy about linear algebra: we
think basis-free, we write basis-free, but when the chips are down we
close the office door and compute with matrices like fury.</p>
<p>– <em>Irving Kaplansky, Paul Halmos: Celebrating 50 Years of
Mathematics</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>Seek computer programs that allow you to do the thinking.</p>
<p>– <em>George E. P. Box</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>If you only know how to use a hammer, every problem starts to
look like a nail. Stay away from that trap.</p>
<p>– <em>Richard B. Johnson</em>
<sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>[It is] best to confuse only one issue at a time.</p>
<p>– <em>Kernihan &amp; Ritchie</em>
<sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>The nice thing about standards is that there are so many of them
to choose from.</p>
<p>– <em>Andrew Tanenbaum, Computer Networks</em>
<sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>There are no routine statistical questions, only questionable
statistical routines.</p>
<p>– <em>David R. Cox</em>
<sub>(<em>#computing,statistics</em>)</sub></p>
</li>
<li>
<p>Be careful the environment you choose for it will shape you be
careful the friends you choose for you will become like them.</p>
<p>– <em>W. Clement Stone</em> <sub>(<em>#computing,tidy
data</em>)</sub></p>
</li>
<li>
<p>Be careless in your dress if you must, but keep a tidy soul.</p>
<p>– <em>Mark Twain</em> <sub>(<em>#computing,tidy
data</em>)</sub></p>
</li>
<li>
<p>I’m a tidy sort of bloke. I don’t like chaos. I kept records in
the record rack, tea in the tea caddy, and pot in the pot box.</p>
<p>– <em>George Harrison</em> <sub>(<em>#computing,tidy
data</em>)</sub></p>
</li>
<li>
<p>Thou shalt not sit with statisticians nor commit a Social
Science.</p>
<p>– <em>W.H. Auden</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>There are two kinds of statistics, the kind you look up and the
kind you make up.</p>
<p>– <em>Rex Stout</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Statistics are like alienists – they will testify for either
side.</p>
<p>– <em>Fiorello H. La Guardia</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>You may prove anything by figures.</p>
<p>– <em>Thomas Carlyle</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>To understand God’s thoughts we must study statistics, for these
are the measure of His purpose.</p>
<p>– <em>Florence Nightingale</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>You cannot feed the hungry on statistics.</p>
<p>– <em>David Lloyd George</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>A single death is a tragedy, a million deaths is a statistic.</p>
<p>– <em>Kurt Tucholsky, mis-attributed to Joseph Stalin, Franzosischer
Witz, 1925</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Statistics are like a bikini. What they reveal is suggestive, but
what they conceal is vital.</p>
<p>– <em>Aaron Levenstein</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Do not put faith in what statistics say until you have carefully
considered what they do not say.</p>
<p>– <em>William W. Watt</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Facts are stubborn things, but statistics are more pliable.</p>
<p>– <em>Mark Twain</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Statistics are figures used as arguments.</p>
<p>– <em>Leonard L. Levison</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Figures won’t lie, but liars will figure.</p>
<p>– <em>Unknown (though often misattributed to Mark Twain)</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>I always find that statistics are hard to swallow and impossible
to digest. The only one I can remember is that if all the people who go
to sleep in church were laid end to end they would be a lot more
comfortable.</p>
<p>– <em>Mrs Robert A. Taft</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Statistician: Delphic figure who lacks the necessary vocabulary
to converse with mere mortals.</p>
<p>– <em>Rod Nicolson, Psychology Software News</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Get the facts first, and then you can distort them as much as you
please.</p>
<p>– <em>Mark Twain</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>If you want to inspire confidence, give plenty of statistics. It
does not matter that they should be accurate, or even intelligible, as
long as there is enough of them.</p>
<p>– <em>Lewis Carroll</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>It is a truth very certain that when it is not in our power to
determine what is true we ought to follow what is most probable.</p>
<p>– <em>Rene Descartes</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Models are to be used, but not to be believed.</p>
<p>– <em>Henry Theill</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>The deepest sin of the human mind is to believe things without
evidence.</p>
<p>– <em>Thomas H. Huxley</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Man must learn to simplify, but not to the point of
falsification.</p>
<p>– <em>Aldous Huxley</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Since small differences in probability cannot be appreciated by
the human mind, there seems little point in being excessively precise
about uncertainty.</p>
<p>– <em>George E. P. Box &amp; G. C. Tiao, Bayesian inference in
statistical analysis, 1973. Addison-Wesley, Reading, MA, p. 65.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Some people hate the very name of statistics but I find them full
of beauty and interest. Whenever they are not brutalized, but delicately
handled by the higher methods, and are warily interpreted, their power
of dealing with complicated phenomena is extraordinary.</p>
<p>– <em>Francis Galton, Natual Inheritance 1889 p. 62</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>[Statistics are] the only tools by which an opening may be cut
through the formidable thicket of difficulties that bars the path of
those who pursue the Science of Man.</p>
<p>– <em>Francis Galton, Natural Inheritance, 1889.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Data analysis is an aid to thinking and not a replacement
for.</p>
<p>– <em>Richard Shillington</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Sometimes the only thing you can do with a poorly designed
experiment is to try to find out what it died of.</p>
<p>– <em>Ronald A. Fisher</em> <sub>(<em>#statistics,experimental
design</em>)</sub></p>
</li>
<li>
<p>The best time to plan an experiment is after you’ve done it.</p>
<p>– <em>Ronald A. Fisher</em> <sub>(<em>#statistics,experimental
design</em>)</sub></p>
</li>
<li>
<p>[The War Office kept three sets of figures:] one to mislead the
public, another to mislead the Cabinet, and the third to mislead
itself.</p>
<p>– <em>Herbert Asquith, Alistair Horne, Price of Glory</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Why are you testing your data for normality? For large sample
sizes the normality tests often give a meaningful answer to a
meaningless question (for small samples they give a meaningless answer
to a meaningful question)</p>
<p>– <em>Greg Snow, R-Help, 21 Feb 2014</em>
<sub>(<em>#statistics,normality,nhst</em>)</sub></p>
</li>
<li>
<p>The relevant question is not whether ANOVA assumptions are met
exactly, but rather whether the plausible violations of the assumptions
have serious consequences on the validity of probability statements
based on the standard assumptions</p>
<p>– <em>Gene V. Glass &amp; Percy D. Peckham &amp; James R. Sanders,
Consequences of Failure to Meet Assumptions Underlying the Fixed Effects
Analyses of Variance and Covariance, Review of Educational Research Vol.
42, No. 3 (Summer, 1972), pp. 237-288 , p. 237.</em>
<sub>(<em>#statistics,anova</em>)</sub></p>
</li>
<li>
<p>All models are wrong, but some are useful.</p>
<p>– <em>George E. P. Box</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Every model is an approximation.</p>
<p>– <em>George E. P. Box</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>The business of the statistician is to catalyze the scientific
learning process.</p>
<p>– <em>George E. P. Box</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Statisticians, like artists, have the bad habit of falling in
love with their models.</p>
<p>– <em>George E. P. Box</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>If there were a probability of only p = 0.04 of finding a crock
of gold behind the next tree, wouldn’t you go and look?</p>
<p>– <em>George E. P. Box</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>When the ratio of the largest to smallest observation is large
you should question whether the data are being analyzed in the right
metric (transformation).</p>
<p>– <em>George E. P. Box</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>A useful type of time series model is a recipe for transforming
serial data into white noise.</p>
<p>– <em>George E. P. Box</em> <sub>(<em>#statistics,time
series</em>)</sub></p>
</li>
<li>
<p>It is the data that are real (they actually happened!) The model
is a hypothetical conjecture that might or might not summarize and/or
explain important features of the data</p>
<p>– <em>George E. P. Box</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>It is not unusual for a well-designed experiment to analyze
itself.</p>
<p>– <em>George E. P. Box</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Discovering the unexpected is more important than confirming the
known.</p>
<p>– <em>George E. P. Box</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>We are drowning in information and starving for knowledge.</p>
<p>– <em>John Naisbitt, Megatrends</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>`Data! data!’ he cried impatiently. I can’t make bricks without
clay.</p>
<p>– <em>Arthur Conan-Doyle, Adventures of Sherlock Holmes “The Copper
Beeches”</em> <sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>I have no data yet. It is a capital mistake to theorize before
one has data.</p>
<p>– <em>Arthur Conan-Doyle, Adventures of Sherlock Holmes “A Scandal in
Bohemia”</em> <sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>This was an unexpected piece of luck. My data were coming more
quickly than I could have reasonably hoped.</p>
<p>– <em>Arthur Conan-Doyle, Memoirs of Sherlock Holmes, The Musgrave
Ritual</em> <sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>I have not all my facts yet, but I do not think there are any
insuperable difficulties. Still, it is an error to argue in front of
your data. You find yourself insensibly twisting them round to fit your
theories.</p>
<p>– <em>Arthur Conan-Doyle, His Last Bow, Wisteria Lodge</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>The only thing we know for sure about a missing data point is
that it is not there, and there is nothing that the magic of statistics
can do do change that. The best that can be managed is to estimate the
extent to which missing data have influenced the inferences we wish to
draw.</p>
<p>– <em>Howard Wainer</em> <sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>Big data can change the way social science is performed, but will
not replace statistical common sense.</p>
<p>– <em>Thomas Landsall-Welfare, Nowcasting the mood of the nation,
Significance v. 9(4), August 12, 2012, p. 28.</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>Baseball is ninety percent mental and the other half is
physical.</p>
<p>– <em>Yogi Berra</em> <sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>Whenever I see an outlier, I never know whether to throw it away
or patent it.</p>
<p>– <em>Bert Gunter, R-Help, 9/14/2015</em>
<sub>(<em>#data,outliers</em>)</sub></p>
</li>
<li>
<p>In God we trust. All others must bring data.</p>
<p>– <em>W. Edwards Deming</em> <sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>How do I love thee? Let me count the ways.</p>
<p>– <em>Elizabeth Barrett Browning, Sonnets from the Portuguese</em>
<sub>(<em>#data,counts</em>)</sub></p>
</li>
<li>
<p>Not everything that counts can be counted, and not everything
that can be counted counts.</p>
<p>– <em>William Bruce Cameron</em>
<sub>(<em>#data,counts</em>)</sub></p>
</li>
<li>
<p>Whenever you can, count.</p>
<p>– <em>Francis Galton</em> <sub>(<em>#data,counts</em>)</sub></p>
</li>
<li>
<p>It is difficult to understand why statisticians commonly limit
their inquiries to Averages, and do not revel in more comprehensive
views. Their souls seem as dull to the charm of variety as that of the
native of our flat English counties, whose retrospect of Switzerland was
that, if its mountains could be thrown into its lakes, two nuisances
would be got rid of at once.</p>
<p>– <em>Francis Galton, Natural Inheritance</em>
<sub>(<em>#data,averages</em>)</sub></p>
</li>
<li>
<p>While the individual man is an insoluble puzzle, in the aggregate
he becomes a mathematical certainty. You can, for example, never
foretell what any one man will do, but you can say with precision what
an average number will be up to. Individuals vary, but percentages
remain constant. So says the statistician.</p>
<p>– <em>Arthur Conan-Doyle, Sign of the Four</em>
<sub>(<em>#data,averages</em>)</sub></p>
</li>
<li>
<p>If you put a buttock on a hot plate and another one on an ice
cube, the average is good, but in reality your bottom is in trouble.</p>
<p>– <em>Grigore Moisil</em>
<sub>(<em>#data,averages</em>)</sub></p>
</li>
<li>
<p>The graphical method has considerable superiority for the
exposition of statistical facts over the tabular. A heavy bank of
figures is grievously wearisome to the eye, and the popular mind is as
incapable of drawing any useful lessons from it as of extracting
sunbeams from cucumbers.</p>
<p>– <em>Arthur B. Farquhar &amp; Henry Farquhar, Economic and
Industrial Delusions, 1891.</em>
<sub>(<em>#data,tables,vision</em>)</sub></p>
</li>
<li>
<p>Let it serve for table-talk.</p>
<p>– <em>William Shakespeare, The Merchant of Venice, Act III, Sc.
5.</em> <sub>(<em>#data,tables</em>)</sub></p>
</li>
<li>
<p>I drink to the general joy o’ the whole table.</p>
<p>– <em>William Shakespeare, Macbeth, Act III, Sc. 4.</em>
<sub>(<em>#data,tables</em>)</sub></p>
</li>
<li>
<p>Isolated facts, those that can only be obtained by rough estimate
and that require development, can only be presented in memoires; but
those that can be presented in a body, with details, and on whose
accuracy one can rely, may be expounded in tables.</p>
<p>– <em>E. Duvillard, Memoire sur le travail du Bureau de statistique
1806.</em> <sub>(<em>#data,tables</em>)</sub></p>
</li>
<li>
<p>Study without reflection is a waste of time reflection without
study is dangerous</p>
<p>– <em>Confuscius, Analects (551-479 BC)</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Things should be made as simple as possible, but not any
simpler</p>
<p>– <em>Albert Einstein</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>So much has already been written about everything that you can’t
find out anything about it.</p>
<p>– <em>James Thurber, 1961</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Theory into Practice.</p>
<p>– <em>Mao Tse-Tung, The Little Red Book</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Beauty is truth; truth, beauty. That is all ye know on Earth, and
all ye need to know.</p>
<p>– <em>John Keats, Ode on a Grecian urn</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>They consider me to have sharp and penetrating vision because I
see them through the mesh of a sieve.</p>
<p>– <em>Kahlil Gibran</em>
<sub>(<em>#science,vision</em>)</sub></p>
</li>
<li>
<p>The journalistic vision sharpens to the point of maximum impact
every event, every individual and social configuration; but the honing
is uniform.</p>
<p>– <em>George Steiner</em>
<sub>(<em>#science,vision</em>)</sub></p>
</li>
<li>
<p>Some people weave burlap into the fabric of our lives, and some
weave gold thread. Both contribute to make the whole picture beautiful
and unique.</p>
<p>– <em>Anon.</em> <sub>(<em>#science,pictures</em>)</sub></p>
</li>
<li>
<p>Time extracts various values from a painter’s work. When these
values are exhausted the pictures are forgotten, and the more a picture
has to give, the greater it is.</p>
<p>– <em>Henri Matisse</em>
<sub>(<em>#science,pictures</em>)</sub></p>
</li>
<li>
<p>God is in the details.</p>
<p>– <em>Mies van der Roche, New York Times August 19, 1969</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>The devil is in the details.</p>
<p>– <em>George Schultz</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>One has to be able to count if only so that at fifty one doesn’t
marry a girl of twenty.</p>
<p>– <em>Maxim Gorky, The Zykovs 1914</em>
<sub>(<em>#science,counts</em>)</sub></p>
</li>
<li>
<p>A man has one hundred dollars and you leave him with two dollars,
that’s subtraction.</p>
<p>– <em>Mae West, My Little Chickadee 1940</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>In the fields of observation chance favors only the prepared
mind.</p>
<p>– <em>Louis Pasteur</em> <sub>(<em>#science,data</em>)</sub></p>
</li>
<li>
<p>The eye of a human being is a microscope, which makes the world
seem bigger than it really is.</p>
<p>– <em>Kahlil Gibran, A Handful of Sand on the Shore</em>
<sub>(<em>#science,vision</em>)</sub></p>
</li>
<li>
<p>To the man who only has a hammer in the toolkit, every problem
looks like a nail.</p>
<p>– <em>Abraham Maslow</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Four hostile newspapers are more to be feared than a thousand
bayonets.</p>
<p>– <em>Napoleon Bonaparte, Maxims</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>When I’m working on a problem, I never think about beauty. I
think only how to solve the problem. But when I have finished, if the
solution is not beautiful, I know it is wrong.</p>
<p>– <em>Richard Buckminster Fuller</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>He who asks a question is a fool for five minutes he who does not
ask a question remains a fool forever.</p>
<p>– <em>Chinese Proverb</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>The great tragedy of science – the slaying of a beautiful
hypothesis by an ugly fact.</p>
<p>– <em>Thomas Huxley</em> <sub>(<em>#science,nhst</em>)</sub></p>
</li>
<li>
<p>Give a man to fish and he will eat for a day. Teach a man to fish
and he will eat for the rest of his life.</p>
<p>– <em>Chinese Proverb</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Give a man a fish and he will eat for a day. Teach a man to fish
and you lose a consulting job forever.</p>
<p>– <em>Howard Wainer, 2016</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>When you have eliminated the impossible, whatever remains,
however improbable, must be the truth.</p>
<p>– <em>Arthur Conan Doyle, The Sign of the Four (1890), Ch. 6</em>
<sub>(<em>#science,probability</em>)</sub></p>
</li>
<li>
<p>If you choose to represent the various parts in life by holes
upon a table, of different shapes—some circular, some triangular, some
square, some oblong—we shall generally find that the triangular person
has got into the square hole, the oblong into the triangular, and a
square person has squeezed himself into the round hole.</p>
<p>– <em>Sydney Smith, 1769-1845</em>
<sub>(<em>#science,geometry</em>)</sub></p>
</li>
<li>
<p>I know of scarcely anything so apt to impress the imagination as
the wonderful form of cosmic order expressed by the “Law of Frequency of
Error.” The law would have been personified by the Greeks and deified,
if they had known of it. It reigns with serenity and in complete
self-effacement, amidst the wildest confusion. The huger the mob, and
the greater the apparent anarchy, the more perfect is its sway. It is
the supreme law of Unreason. Whenever a large sample of chaotic elements
are taken in hand and marshaled in the order of their magnitude, an
unsuspected and most beautiful form of regularity proves to have been
latent all along.</p>
<p>– <em>Sir Francis Galton, Natural Inheritance London: Macmillan,
1889. Quoted in J. R. Newman (ed.) The World of Mathematics, New York:
Simon and Schuster, 1956. p. 1482.</em>
<sub>(<em>#science,normality</em>)</sub></p>
</li>
<li>
<p>In scientific thought we adopt the simplest theory which will
explain all the facts under consideration and enable us to predict new
facts of the same kind. The catch in this criterion lies in the world
“simplest.” It is really an aesthetic canon such as we find implicit in
our criticisms of poetry or painting. The layman finds such a law as
dx/dt = K(d<sup>2x/dy</sup>2) much less simple than “it oozes,” of which
it is the mathematical statement. The physicist reverses this judgment,
and his statement is certainly the more fruitful of the two, so far as
prediction is concerned. It is, however, a statement about something
very unfamiliar to the plainman, namely, the rate of change of a rate of
change.</p>
<p>– <em>John Burdon Sanderson Haldane, Possible Worlds, 1927.</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Oh, what a tangled web we weave, When first we practice to
deceive!</p>
<p>– <em>Sir Walter Scott</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Practice is the best of all instructors.</p>
<p>– <em>Publilius Syrus</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>We should go to the masses and learn from them, synthesize their
experience into better, articulated principles and methods, then do
propaganda among the masses, and call upon them to put these principles
and methods into practice so as to solve their problems and help them
achieve liberation and happiness.</p>
<p>– <em>Chairman Mao Zedong, “Get Organized!” (November 29, 1943),
Selected Works, Vol. III, p. 158.</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>An elementary demonstration is one that requires no knowledge—
just an infinite amount of intelligence.</p>
<p>– <em>Richard Feynman</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Science may be described as the art of systematic
over-simplification.</p>
<p>– <em>Karl Popper</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Science is like sex: sometimes something useful comes out, but
that is not the reason we are doing it.</p>
<p>– <em>Richard Feynman</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Humanists believe that the world has a fixed number of mysteries,
so that when one is solved, our sense of wonder is diminished.
Scientists believe that the world has endless mysteries, so that when
one is solved, there are always new ones to ponder.</p>
<p>– <em>D. O. Hebb, quoted by Steven Pinker</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Art and science encounter each other when they seek
exactitude.</p>
<p>– <em>Etienne-Jules Marey</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Circumstantial evidence is a very tricky thing. It may seem to
point very straight to one thing, but if you shift your own point of
view a little, you may find it pointing in an equally uncompromising
manner to something entirely different.</p>
<p>– <em>Sherlock Holmes, The Adventures of Sherlock Holmes (1892) “The
Boscombe Valley Mystery”</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>To find out what happens when you change something, it is
necessary to change it.</p>
<p>– <em>Box, Hunter, and Hunter, Statistics for Experimenters
(1978)</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>He who loves practice without theory is like the sailor who
boards ship without a rudder and compass and never knows where he may be
cast.</p>
<p>– <em>Leonardo da Vinci</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Science is built up of facts, as a house is with stones. But a
collection of facts is no more a science than a heap of stones is a
house.</p>
<p>– <em>Henri Poincare</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>The museum spreads its surfaces everywhere, and becomes an
untitled collection of generalizations that mobilize the eye.</p>
<p>– <em>Robert Smithson</em>
<sub>(<em>#science,generalizations</em>)</sub></p>
</li>
<li>
<p>In one word, to draw the rule from experience, one must
generalize; this is a necessity that imposes itself on the most
circumspect observer.</p>
<p>– <em>Henri Poincare, The Value of Science: Essential Writings of
Henri Poincare</em>
<sub>(<em>#science,generalizations</em>)</sub></p>
</li>
<li>
<p>If I have seen further, it is by standing on the shoulders of
giants.</p>
<p>– <em>Sir Isaac Newton, Letter to Robert Hooke, Feb. 5, 1676</em>
<sub>(<em>#science,generalizations</em>)</sub></p>
</li>
<li>
<p>Few intellectual pleasures are more keen than those enjoyed by a
person who, while he is occupied in some special inquiry, suddenly
perceives that it admits of a wide generalization, and that his results
hold good in previously-unsuspected directions.</p>
<p>– <em>Francis Galton, North American Review, 150 419-431 (1890)</em>
<sub>(<em>#science,generalizations</em>)</sub></p>
</li>
<li>
<p>The elegance of a theorem is directly proportional to the number
of ideas you can see in it and inversely proportional to the effort it
takes to see them.</p>
<p>– <em>George Polya</em>
<sub>(<em>#science,generalizations</em>)</sub></p>
</li>
<li>
<p>A mathematician, like a painter or a poet, is a master of
pattern. The mathematician’s patterns, like the painter’s or the poet’s,
must be beautiful; the ideas, like the colors or the words, must fit
together in a harmonious way. … There is no permanent place in the world
for ugly mathematics.</p>
<p>– <em>G. H. Hardy</em>
<sub>(<em>#science,generalizations</em>)</sub></p>
</li>
<li>
<p>An idea which can be used once is a trick. If it can be used more
than once it becomes a method.</p>
<p>– <em>George Polya and Gabor Szego</em>
<sub>(<em>#science,generalizations</em>)</sub></p>
</li>
<li>
<p>When the time is ripe for certain things, these things appear in
different places in the manner of violets coming to light in early
spring.</p>
<p>– <em>Farkas Bolyai, To his son Janos Bolyai, urging him to claim the
invention of non-Euclidean geometry without delay, quoted in Ming Li and
Paul Vitanyi, An introduction to Kolmogorov Complexity and Its
Applications, 1st ed., 1993, p. 83.</em>
<sub>(<em>#science,generalizations</em>)</sub></p>
</li>
<li>
<p>The only new thing in the world is the history you don’t
know.</p>
<p>– <em>Harry S. Truman, Quoted by David McCulloch</em>
<sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>So we beat on, boats against the current, borne back ceaselessly
into the past.</p>
<p>– <em>F. Scott Fitzgerald, The Great Gatsby (1925)</em>
<sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>Euclid alone has looked on beauty bare.</p>
<p>– <em>Edna St Vincent Millay</em>
<sub>(<em>#history,geometry</em>)</sub></p>
</li>
<li>
<p>The past only exists insofar as it is present in the records of
today. And what those records are is determined by what questions we
ask. There is no other history than that.</p>
<p>– <em>Wheeler, 1982:24</em> <sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>A generation which ignores history has no past and no future</p>
<p>– <em>Robert Heinlein</em> <sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>For my part, I consider that it will be found much better by all
parties to leave the past to history, especially as I propose to write
that history myself.</p>
<p>– <em>Winston Churchill</em> <sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>If you would understand anything, observe its beginning and its
development.</p>
<p>– <em>Aristotle</em> <sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>God alone knows the future, but only an historian can alter the
past.</p>
<p>– <em>Ambrose Bierce</em> <sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>Since God himself cannot change the past, he is obliged to
tolerate the existence of historians.</p>
<p>– <em>Attributed to Samuel Butler</em>
<sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>At the heart of good history is a naughty little secret: good
storytelling.</p>
<p>– <em>Stephen Schiff</em> <sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>It has been said that though God cannot alter the past,
historians can; it is perhaps because they can be useful to Him in this
respect that He tolerates their existence.</p>
<p>– <em>Samuel Butler, Erewhon Revisted</em>
<sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>History is moving statistics and statistics is frozen
history.</p>
<p>– <em>A. L. Schlozer, Theorie der Statistik 1804, p. 86</em>
<sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>Time flies like an arrow fruit flies like a banana.</p>
<p>– <em>Anthony G. Oettinger, Often mis-attributed to Groucho Marx</em>
<sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>Time is the longest distance between two places.</p>
<p>– <em>Tennessee Williams, The Glass Menagerie</em>
<sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>Those who make the worst use of their time are the first to
complain of its brevity.</p>
<p>– <em>Jean de La Bruyere, Les Caracteres</em>
<sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>The past is a foreign country: they do things differently
there.</p>
<p>– <em>L. P. Hartley, The Go-Between</em>
<sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>I never think of the future - it comes soon enough.</p>
<p>– <em>Albert Einstein</em>
<sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>The best way to predict the future is to invent it</p>
<p>– <em>Alan Kay</em> <sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>The future ain’t what it used to be</p>
<p>– <em>Yogi Berra</em> <sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>Look not mournfully into the past. It comes not back again.
Wisely improve the present. It is thine. Go forth to meet the shadowy
future, without fear.</p>
<p>– <em>Henry Wadsworth Longfellow</em>
<sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>Let him who would enjoy a good future waste none of his
present.</p>
<p>– <em>Roger Babson</em> <sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>When in doubt, predict that the present trend will continue.</p>
<p>– <em>Merkins Maxim</em> <sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>The only use of a knowledge of the past is to equip us for the
present. The present contains all that there is. It is holy ground; for
it is the past, and it is the future.</p>
<p>– <em>Alfred North Whitehead</em>
<sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>My past is my wisdom to use today. … my future is my wisdom yet
to experience. Be in the present because that is where life resides.</p>
<p>– <em>Gene Oliver, Life and the Artistry of Change</em>
<sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>I have realized that the past and future are real illusions, that
they exist in the present, which is what there is and all there is.</p>
<p>– <em>Alan Watts</em> <sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>The future is uncertain but the end is always near.</p>
<p>– <em>Jim Morrison</em> <sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>Time has no divisions to mark its passage, there is never a
thunder-storm of blare of trumpets to announce the beginning of a new
month or year. Even when a new century begins, it is only we mortals who
ring bells and fire off pistols.</p>
<p>– <em>Thomas Mann, The Magic Mountain (1924)</em>
<sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>Direction is more important than speed. We are so busy looking at
our speedometers that we forget the milestone.</p>
<p>– <em>Anonymous</em>
<sub>(<em>#history,milestones</em>)</sub></p>
</li>
<li>
<p>Only sixteen players have hit fifty or more homers in a season.
To me, that’s a very special milestone.</p>
<p>– <em>Mark McGwire</em>
<sub>(<em>#history,milestones</em>)</sub></p>
</li>
<li>
<p>As life runs on, the road grows strange with faces new – and near
the end. The milestones into headstones change, Neath every one a
friend.</p>
<p>– <em>James Russell Lowell</em>
<sub>(<em>#history,milestones</em>)</sub></p>
</li>
<li>
<p>This paper contains much that is new and much that is true.
Unfortunately, that which is true is not new and that which is new is
not true.</p>
<p>– <em>attributed to Wolfgang Pauli</em>
<sub>(<em>#reviews</em>)</sub></p>
</li>
<li>
<p>This book fills a much-needed gap.</p>
<p>– <em>Attributed to Moses Hadas</em>
<sub>(<em>#reviews</em>)</sub></p>
</li>
<li>
<p>Russell left the vast darkness of the topic unobscured</p>
<p>– <em>Alfred North Whitehead, Referring to Bertrand Russell</em>
<sub>(<em>#reviews</em>)</sub></p>
</li>
<li>
<p>Mathematicians have always been rather of a jealous nature, and
undoubtedly jealousy was a family characteristic of the Bernoullis.
There is some excuse for mathematicians, for their reputation stands for
posterity largely not on what they did, but on what their contemporaries
attributed to them.</p>
<p>– <em>Karl Pearson, The History of Statistics in the 17th and 18th
Centuries.</em> <sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>When choosing between two evils, I always like to take the one
I’ve never tried before.</p>
<p>– <em>Mae West, 1941</em> <sub>(<em>#ethics</em>)</sub></p>
</li>
<li>
<p>To err is human—but it feels divine!</p>
<p>– <em>Mae West (paraphrase of Alexander Pope, “To err is human, to
forgive devine”)</em> <sub>(<em>#data,averages</em>)</sub></p>
</li>
<li>
<p>Good judgment comes from experience experience comes from bad
judgment.</p>
<p>– <em>Fred Brooks</em> <sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>One must learn by doing the thing; for though you think you know
it, you have no certainty until you try.</p>
<p>– <em>Sophocles</em></p>
</li>
<li>
<p>There is a magic in graphs. The profile of a curve reveals in a
flash a whole situation—the life history of an epidemic, a panic, or an
era of prosperity. The curve informs the mind, awakens the imagination,
convinces.</p>
<p>– <em>Henry D. Hubbard, Foreword to Brinton (1939), Graphic
Presentation</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>Graphs carry the message home. A universal language, graphs
convey information directly to the mind. Without complexity there is
imaged to the eye a magnitude to be remembered. Words have wings, but
graphs interpret. Graphs are pure quantity, stripped of verbal sham,
reduced to dimension, vivid, unescapable.</p>
<p>– <em>Henry D. Hubbard, Foreword to Brinton (1939), Graphic
Presentation</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>Graphs are all inclusive. No fact is too slight or too great to
plot to a scale suited to the eye. Graphs may record the path of an ion
or the orbit of the sun, the rise of a civilization, or the acceleration
of a bullet, the climate of a century or the varying pressure of a heart
beat, the growth of a business, or the nerve reactions of a child.</p>
<p>– <em>Henry D. Hubbard, Foreword to Brinton (1939), Graphic
Presentation</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>The graphic art depicts magnitudes to the eye. It does more. It
compels the seeing of relations. We may portray by simple graphic
methods whole masses of intricate routine, the organization of an
enterprise, or the plan of a campaign. Graphs serve as storm signals for
the manager, statesman, engineer; as potent narratives for the actuary,
statist, naturalist; and as forceful engines of research for science,
technology and industry. They display results. They disclose new facts
and laws. They reveal discoveries as the bud unfolds the flower.</p>
<p>– <em>Henry D. Hubbard, Foreword to Brinton (1939), Graphic
Presentation</em> <sub>(<em>#data
visualization,pictures,vision</em>)</sub></p>
</li>
<li>
<p>The graphic language is modern. We are learning its alphabet.
That it will develop a lexicon and a literature marvelous for its
vividness and the variety of application is inevitable. Graphs are
dynamic, dramatic. They may epitomize an epoch, each dot a fact, each
slope an event, each curve a history. Wherever there are data to record,
inferences to draw, or facts to tell, graphs furnish the unrivalled
means whose power we are just beginning to realize and to apply.</p>
<p>– <em>Henry D. Hubbard, Foreword to Brinton (1939), Graphic
Presentation</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>In One Dimensions, did not a moving Point produce a Line with two
terminal points? In two Dimensions, did not a moving Line produce a
Square wit four terminal points? In Three Dimensions, did not a moving
Square produce - did not the eyes of mine behold it - that blessed
being, a Cube, with eight terminal points? And in Four Dimensions, shall
not a moving Cube - alas, for Analogy, and alas for the Progress of
Truth if it be not so - shall not, I say the motion of a divine Cube
result in a still more divine organization with sixteen terminal
points?</p>
<p>– <em>Edwin A. Abbott, Flatland: A Romance of Many Dimensions</em>
<sub>(<em>#data visualization,geometry</em>)</sub></p>
</li>
<li>
<p>To comport oneself with perfect propriety in Polygonal society,
one ought to be a Polygon oneself.</p>
<p>– <em>Edwin A. Abbott, Flatland: A Romance of Many Dimensions</em>
<sub>(<em>#data visualization,geometry</em>)</sub></p>
</li>
<li>
<p>True, said the Sphere; it appears to you a Plane, because you are
not accustomed to light and shade and perspective; just as in Flatland a
Hexagon would appear a Straight Line to one who has not the Art of Sight
Recognition. But in reality it is a Solid, as you shall learn by the
sense of Feeling.</p>
<p>– <em>Edwin A. Abbott, Flatland: A Romance of Many Dimensions</em>
<sub>(<em>#data visualization,geometry</em>)</sub></p>
</li>
<li>
<p>There are twenty one mystical dimensions of consciousness.
Enlightenment is abiding in the highest three dimensions of
consciousness.</p>
<p>– <em>Amit Ray, Enlightenment Step by Step</em> <sub>(<em>#data
visualization,geometry</em>)</sub></p>
</li>
<li>
<p>I would say time is definitely one of my top three favorite
dimensions.</p>
<p>– <em>Randall Munroe, xkcd</em>
<sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>Poetry is when an emotion has found its thought and the thought
has found its words.</p>
<p>– <em>Robert Frost</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>I don’t like to commit myself about heaven and hell - you see, I
have friends in both places.</p>
<p>– <em>Mark Twain</em></p>
</li>
<li>
<p>Errors using inadequate data are much less than those using no
data at all.</p>
<p>– <em>Charles Babbage, Circa 1850</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>From carefully compiled statistical facts more may be learned
[about] the moral nature of Man than can be gathered from all the
accumulated experiences of the preceding ages.</p>
<p>– <em>Henry Thomas Buckle, A History of Civilization in England,
1857/1898, p. 17</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Statistical thinking will one day be as necessary for efficient
citizenship as the ability to read and write.</p>
<p>– <em>H.G. Wells, Mankind in Making, 1903</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Statistical accounts are to be referred to as a dictionary by men
of riper years, and by young men as a grammar, to teach them the
relations and proportions of different statistical subjects, and to
imprint them on the mind at a time when the memory is capable of being
impressed in a lasting and durable manner, thereby laying the foundation
for accurate and valuable knowledge.</p>
<p>– <em>William Playfair, The Statistical Breviary (1801), 5-6.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Geography is only a branch of statistics, a knowledge of which is
necessary to the well-understanding of the history of nations, as well
as their situations relative to each other.</p>
<p>– <em>William Playfair, The Commercial and Political Atlas,
p. 29</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>No study is less alluring or more dry and tedious than
statistics, unless the mind and imagination are set to work, or that the
person studying is particularly interested in the subject; which last
can seldom be the case with young men in any rank of life.</p>
<p>– <em>William Playfair, The Statistical Breviary (1801), p. 16</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>DIAGRAMS are of great utility for illustrating certain questions
of vital statistics by conveying ideas on the subject through the eye,
which cannot be so readily grasped when contained in figures.</p>
<p>– <em>Florence Nightingale, Mortality of the British Army, 1857</em>
<sub>(<em>#data visualization,pictures,vision</em>)</sub></p>
</li>
<li>
<p>To give insight to statistical information it occurred to me,
that making an appeal to the eye when proportion and magnitude are
concerned, is the best and readiest method of conveying a distinct
idea.</p>
<p>– <em>William Playfair, The Statistical Breviary (1801), p. 2</em>
<sub>(<em>#data visualization,pictures,vision</em>)</sub></p>
</li>
<li>
<p>Regarding numbers and proportions, the best way to catch the
imagination is to speak to the eyes.</p>
<p>– <em>William Playfair, Elemens de statistique, Paris, 1802,
p. XX.</em> <sub>(<em>#data
visualization,pictures,vision</em>)</sub></p>
</li>
<li>
<p>The aim of my carte figurative is to convey promptly to the eye
the relation not given quickly by numbers requiring mental
calculation.</p>
<p>– <em>Charles Joseph Minard</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>Information that is imperfectly acquired, is generally as
imperfectly retained; and a man who has carefully investigated a printed
table, finds, when done, that he has only a very faint and partial idea
of what he has read; and that like a figure imprinted on sand, is soon
totally erased and defaced.</p>
<p>– <em>William Playfair, The Commercial and Political Atlas (p. 3),
1786</em> <sub>(<em>#data
visualization,pictures,tables</em>)</sub></p>
</li>
<li>
<p>Functional visualizations are more than innovative statistical
analyses and computational algorithms. They must make sense to the user
and require a visual language system that uses colour, shape, line,
hierarchy and composition to communicate clearly and appropriately, much
like the alphabetic and character-based languages used worldwide between
humans.</p>
<p>– <em>Matt Woolman, Digital Information Graphics</em> <sub>(<em>#data
visualization,pictures</em>)</sub></p>
</li>
<li>
<p>A people without the knowledge of their past history, origin and
culture is like a tree without roots</p>
<p>– <em>Marcus Garvey</em> <sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>The bushels of rings taken from the fingers of the slain at the
battle of Cannae, above two thousand years ago, are recorded; … but the
bushels of corn produced in England at this day, or the number of the
inhabitants of the country, are unknown, at the very time that we are
debating that most important question, whether or not there is
sufficient substance for those who live in the kingdom.</p>
<p>– <em>William Playfair, The Statistical Breviary (1801), p. 7-8</em>
<sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>How can the past and future be, when the past no longer is, and
the future is not yet? As for the present, if it were always present and
never moved on to become the past, it would not be time, but
eternity.</p>
<p>– <em>St. Augustine of Hippo, Confessions</em>
<sub>(<em>#history,time</em>)</sub></p>
</li>
<li>
<p>Every measurable thing, except numbers, is imagined in the manner
of continuous quantity. Therefore, for the mensuration of such a thing,
it is necessary that points, lines and surfaces, or their properties be
imagined. For in them, as the Philosopher has it, measure or ratio is
initially found, while in other things it is recognized by similarity as
they are being referred to by the intellect to the geometrical
entities.</p>
<p>– <em>Nicole Oresme, The Latitude of Forms</em> <sub>(<em>#data
visualization,geometry</em>)</sub></p>
</li>
<li>
<p>Any one who considers arithmetical methods of producing random
digits is, of course, in a state of sin.</p>
<p>– <em>John von Neumann, Various techniques used in connection with
random digits, Applied Mathematics Series, 1951, no 12, 36-38.</em>
<sub>(<em>#computing,random numbers</em>)</sub></p>
</li>
<li>
<p>With four parameters I can fit an elephant, and with five I can
make him wiggle his trunk.</p>
<p>– <em>John von Neumann, Quoted by Freeman Dyson</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>The punishment of every disordered mind is its own disorder.</p>
<p>– <em>St. Augustine of Hippo, Confessions</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>It is strange that only extraordinary men make the discoveries,
which later appear so easy and simple.</p>
<p>– <em>Georg C. Lichtenberg</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Two things are infinite: the universe and human stupidity; and
I’m not sure about the universe.</p>
<p>– <em>Albert Einstein</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Facts, however numerous, do not constitute a science. Like
innumerable grains of sand on the sea shore, single facts appear
isolated, useless, shapeless; it is only when compared, when arranged in
their natural relations, when crystallised by the intellect, that they
constitute the eternal truths of science</p>
<p>– <em>William Farr, “Observation,” Br. Ann. Med. 1 (1837): 693</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Tidy datasets are all alike, but every messy dataset is messy in
its own way</p>
<p>– <em>Hadley Wickham</em> <sub>(<em>#computing,tidy
data</em>)</sub></p>
</li>
<li>
<p>We cannot understand the world without numbers, and we cannot
understand it with numbers alone.</p>
<p>– <em>Hans Rosling</em> <sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>Bad data makes bad models. Bad models instruct people to make
ineffective or harmful interventions. Those bad interventions produce
more bad data, which is fed into more bad models.</p>
<p>– <em>Cory Doctorow, Machine Learning’s Crumbling Foundations, Aug
2021.</em> <sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>Whenever I am infuriated, I revenge myself with a new Diagram</p>
<p>– <em>Florence Nightingale, Letter 1857.8.19 to Sidney Herbert</em>
<sub>(<em>#data visualization,pictures</em>)</sub></p>
</li>
<li>
<p>Again I must repeat my objections to intermingling causation with
statistics. It might be to a certain extent admissible if you had no
sanitary head. But you have one, &amp; his report should be quite
separate. The statistician has nothing to do with causation: he is
almost certain in the present state of knowledge to err.</p>
<p>– <em>Florence Nightingale, Letter, March 1861</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>A Bayesian is one who, vaguely expecting a horse, and catching a
glimpse of a donkey, strongly believes he has seen a mule.</p>
<p>– <em>Stephen John Senn</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>What nature hath joined together, multiple regression cannot put
asunder.</p>
<p>– <em>Richard Nisbett</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Stepwise regression is probably the most abused computerized
statistical technique ever devised. If you think you need stepwise
regression to solve a particular problem you have, it is almost certain
that you do not. Professional statisticians rarely use automated
stepwise regression.</p>
<p>– <em>Leland Wilkinson, SYSTAT (1984). P. 196.</em>
<sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>It would help if the standard statistical programs did not
generate t statistics in such profusion. The programs might be written
to ask, “Do you really have a probability sample?”, “By what standard
would you judge a fitted coefficient large or small?” Or perhaps they
could merely say, printed in bold capitals beside each equation, “So
What Else Is New?”</p>
<p>– <em>Donald M. McCloskey, The Loss Function Has Been Mislaid: The
Rhetoric of Significance Tests, American Economic Review, Vol 75,
#2.</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>The documentation level of R is already much higher than average
for open source software and even than some commercial packages
(esp. SPSS is notorious for its attitude of “You want to do one of these
things. If you don’t understand what the output means, click help and
we’ll pop up five lines of mumbo-jumbo that you’re not going to
understand either.”)</p>
<p>– <em>Peter Dalgaard, R-help mailing list 4.2.2002</em>
<sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>S has forever altered the way people analyze, visualize, and
manipulate data …. S is an elegant, widely accepted, and enduring
software system, with conceptual integrity, thanks to the insight,
taste, and effort of John Chambers.</p>
<p>– <em>Association for Computing Machinery Software System Award</em>
<sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>Tradition among experienced S programmers has always been that
loops (typically ‘for’ loops) are intrinsically inefficient: expressing
computations without loops has provided a measure of entry into the
inner circle of S programming.</p>
<p>– <em>John Chambers, Programming With Data, p. 173.</em>
<sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>While the distribution and publication of Version 2 [of S] was
still evolving, parallel research work was starting to shape the next
major version. At first this seemed to be a move away from S altogether:
something called the “Quantitative Programming Environment” was
initially a separate research project, aimed more explicitly at
programming and trying to emphasize that users need not be statistically
sophisticated. By 1986, however, the decision was made to merge this
work with the facilities (especially the graphics) underlying S, to
produce a new version of S. (This explains, by the way, why the main
program for S is called Sqpe, another one of those little puzzles for
users.)</p>
<p>– <em>Unknown</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>This computationally intensive operation [bootstrapping] is not
one calculated to endear a user to a database administrator.</p>
<p>– <em>Leland Wilkinson, The Grammar of Graphics, p. 49.</em>
<sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>Most computer software is not yet intelligent enough to stop the
user doing something stupid. The old adage ‘Garbage In -&gt; Garbage
Out’ still hold good, and it must be realized that careful thought and
close inspection of the data are vital preliminaries to complicated
computer analysis.</p>
<p>– <em>Christopher Chatfield, Problem solving : a statistician’s
guide, 1988, p. 59.</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>Today…we have high-speed computers and prepackaged statistical
routines to perform the necessary calculations…statistical software will
no more make one a statistician than would a scalpel turn one into a
neurosurgeon. Allowing these tools to do our thinking for us is a sure
recipe for disaster.</p>
<p>– <em>Good &amp; Hardin, Common Errors in Statistics and How to Avoid
Them, p. ix</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>The generation of random numbers is too important to be left to
chance.</p>
<p>– <em>Robert R. Coveyou, Oak Ridge National Laboratory, 1969</em>
<sub>(<em>#computing,random numbers</em>)</sub></p>
</li>
<li>
<p>Landon Noll…has been tinkering with random number generators for
nearly a decade–an exercise in bringing order to chaos. “There’s a lot
of beauty in chaos,” Noll says. “The Grand Canyon wouldn’t be so popular
if it was just a uniform trench. The trick is controlling and managing
chaos and turning it into something useful.”</p>
<p>– <em>Tom McNichol, Wired, August, 2003, page 088.</em>
<sub>(<em>#computing,random numbers</em>)</sub></p>
</li>
<li>
<p>Someone has characterized the user of stepwise regression as a
person who checks his or her brain at the entrance of the computer
center.</p>
<p>– <em>D. R. Wittink, The application of regression analysis. Needham
Heights, MA: Allyn and Bacon. p. 259.</em>
<sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>The idea of optimization transfer is very appealing to me,
especially since I have never succeeded in fully understanding the EM
algorithm.</p>
<p>– <em>Andrew Gellman, Discussion, Journal of Computational and
Graphical Statistics, vol 9, p 49.</em>
<sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>This reminds me of the duality in statistics between computation
and model fit: better-fitting models tend to be easier to compute, and
computational problems often signal modeling problems. and ‘The Black
Swan’. Law, Probability and Risk (2008) 7, 151-163.</p>
<p>– <em>Andrew Gelman, Thoughts inspired by Nassim Taleb’s ‘Fooled by
Randomness’</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>It is a nontrivial exercise to correctly program even the
simplest split-plot model using PROC MIXED.</p>
<p>– <em>Jeremy Aldworth &amp; Wherly P Hoffman, Split-Plot Model With
Covariate: A Cautionary Tale, The American Statistician, 56,
284–289.</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>Sometimes the most important fit statistic you can get is
‘convergence not met’–it can tell you something is wrong with your
model.</p>
<p>– <em>Oliver Schabenberger, 2006 Applied Statistics in Agriculture
Conference.</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>It is obviously pointless to report or quote results to more
digits than is warranted. In fact, it is misleading or at the very least
unhelpful, because it fails to communicate to the reader another
important aspect of the result–namely its reliability! A good rule
(sometimes known as Ehrenberg’s rule) is to quote all digits up to and
including the first two variable digits.</p>
<p>– <em>Philipp K. Janert, Data Analysis with Open Source Tools,
O’Reilly, 2010.</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>Doubt is not a pleasant mental state, but certainty is a
ridiculous one.</p>
<p>– <em>Voltaire (1694-1778)</em>
<sub>(<em>#probability</em>)</sub></p>
</li>
<li>
<p>It is a part of probability that many improbable things will
happen.</p>
<p>– <em>Agathon, 445 - 400 BC, Chance News 7.02</em>
<sub>(<em>#probability</em>)</sub></p>
</li>
<li>
<p>Statistics make it clear the fact that one’s chances of being
hurt by a bear are far, far fewer than being struck by an auto almost
anywhere, or being mugged on a city street, for that matter. We pursue
our automotive, urban lives undaunted, often indifferent amid the police
and ambulance sirens, but in the Alaskan wilderness we lie awake
worrying about bears.</p>
<p>– <em>John Kauffmann, Alaska’s Brooks Range</em>
<sub>(<em>#probability</em>)</sub></p>
</li>
<li>
<p>Cougars can be dangerous, especially to unsupervised children,
but the chances of becoming a cougar victim are far less than becoming a
victim of lightning, honeybees, moose, deer, pit bulls, football,
snow-shoveling, or crossing the street in front of your house. For some
reason, we fear the true risks of being killed far less than the remote
risk of becoming prey.</p>
<p>– <em>Dennis L Olsen, Cougars, page 46.</em>
<sub>(<em>#probability</em>)</sub></p>
</li>
<li>
<p>Things which ought to be expected can seem quite extraordinary if
you’ve got the wrong model.</p>
<p>– <em>David Hand, Significance, 2014, 11, 36-39.</em>
<sub>(<em>#data,models</em>)</sub></p>
</li>
<li>
<p>In many applications, the data analyst has a dilemma: Should an
effect be classified as [fixed] and a BLUE obtained, or as [random] and
a BLUP obtained? The traditional distinction between fixed and random
effects is not helpful; it may, in fact, lead the data analyst to choose
the less efficient route.</p>
<p>– <em>Walter Stroup and D K Mulitze, Nearest Neighbor Adjusted Best
Linear Unbiased Prediction, 1991, The American Statistician, 45,
194–200.</em> <sub>(<em>#data,models</em>)</sub></p>
</li>
<li>
<p>What should be the distribution of random effects in a mixed
model? I think Gaussian is just fine, unless you are trying to write a
journal paper.</p>
<p>– <em>Terry Therneau, Speaking at useR 2007.</em>
<sub>(<em>#data,models</em>)</sub></p>
</li>
<li>
<p>Competent scientists do not believe their own models or theories,
but rather treat them as convenient fictions. …The issue to a scientist
is not whether a model is true, but rather whether there is another
whose predictive power is enough better to justify movement from today’s
fiction to a new one.</p>
<p>– <em>Steve Vardeman, Comment, 1987, Journal of the American
Statistical Association, 82 : 130-131.</em>
<sub>(<em>#data,models</em>)</sub></p>
</li>
<li>
<p>If you just rely on one model, you tend to amputate reality to
make it fit your model.</p>
<p>– <em>David Brooks</em> <sub>(<em>#data,models</em>)</sub></p>
</li>
<li>
<p>Statistical models are sometimes misunderstood in epidemiology.
Statistical models for data are never true. The question whether a model
is true is irrelevant. A more appropriate question is whether we obtain
the correct scientific conclusion if we pretend that the process under
study behaves according to a particular statistical model.</p>
<p>– <em>Scott Zeger, Statistical reasoning in epidemiology, American
Journal of Epidemiology, 1991</em>
<sub>(<em>#data,models</em>)</sub></p>
</li>
<li>
<p>It is not always convenient to remember that the right model for
a population can fit a sample of data worse than a wrong model – even a
wrong model with fewer parameters. We cannot rely on statistical
diagnostics to save us, especially with small samples. We must think
about what our models mean, regardless of fit, or we will promulgate
nonsense.</p>
<p>– <em>Leland Wilkinson, The Grammar of Graphics, p. 335.</em>
<sub>(<em>#data,models</em>)</sub></p>
</li>
<li>
<p>Fitting models to data is a bit like designing shirts to fit
people. If you fit a shirt too closely to one particular person, it will
fit other people poorly. Likewise, a model that fits a particular data
set too well might not fit other data sets well.</p>
<p>– <em>Rahul Parsa, Speaking to the Iowa SAS User’s Group</em>
<sub>(<em>#data,models</em>)</sub></p>
</li>
<li>
<p>You might say that there’s no reason to bother with model
checking since all models are false anyway. I do believe that all models
are false, but for me the purpose of model checking is not to accept or
reject a model, but to reveal aspects of the data that are not captured
by the fitted model.</p>
<p>– <em>Andrew Gelman, Some thoughts on the sociology of statistics,
2007.</em> <sub>(<em>#data,models</em>)</sub></p>
</li>
<li>
<p>When evaluating a model, at least two broad standards are
relevant. One is whether the model is consistent with the data. The
other is whether the model is consistent with the ‘real world.’</p>
<p>– <em>Kenneth Bollen, Structural Equations with Latent Variable</em>
<sub>(<em>#data,models</em>)</sub></p>
</li>
<li>
<p>The point of a model is to get useful information about the
relation between the response and predictor variables. Interpretability
is a way of getting information. But a model does not have to be simple
to provide reliable information about the relation between predictor and
response variables; neither does it have to be a data model. The goal is
not interpretability, but accurate information.</p>
<p>– <em>Leo Breiman, Statistical Modeling: The Two Cultures,
Statistical Science, Vol 16, p. 210.</em>
<sub>(<em>#data,models</em>)</sub></p>
</li>
<li>
<p>The goals in statistics are to use data to predict and to get
information about the underlying data mechanism. Nowhere is it written
on a stone tablet what kind of model should be used to solve problems
involving data. To make my position clear, I am not against models per
se. In some situations they are the most appropriate way to solve the
problem. But the emphasis needs to be on the problem and on the data.
Unfortunately, our field has a vested interest in models, come hell or
high water.</p>
<p>– <em>Leo Breiman, Statistical Modeling: The Two Cultures,
Statistical Science, Vol 16, p. 214.</em>
<sub>(<em>#data,models,statistics</em>)</sub></p>
</li>
<li>
<p>When nearest neighbor effects exist, the randomized complete
block analysis [can be] so poor as to deserver to be called
catastrophic. It [can not] even be considered a serious form of
analysis. It is extremely important to make this clear to the vast
number of researchers who have near religious faith in the randomized
complete block design.</p>
<p>– <em>Walt Stroup &amp; D Mulitze, Nearest Neighbor Adjusted Best
Linear Unbiased Prediction, The American Statistician, 45, 194–200.</em>
<sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>There are two books devoted solely to principal components,
Jackson (1991) and Jolliffe (1986), which we think overstates its value
as a technique.</p>
<p>– <em>Venables &amp; Ripley, Modern Applied Statistics with S, 4th
ed., page 305.</em> <sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>Understanding the split-plot isn’t everything. It’s the only
thing.</p>
<p>– <em>Oliver Schabenberger, Speaking at JSM 2008.</em>
<sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>Residual analysis is similarly unreliable. In a discussion after
a presentation of residual analysis in a seminar at Berkeley in 1993,
William Cleveland, one of the fathers of residual analysis, admitted
that it could not uncover lack of fit in more than four to five
dimensions. The papers I have read on using residual analysis to check
lack of fit are confined to data sets with two or three variables. With
higher dimensions, the interactions between the variables can produce
passable residual plots for a variety of models. A residual plot is a
goodness-of-fit test, and lacks power in more than a few dimensions. An
acceptable residual plot does not imply that the model is a good fit to
the data.</p>
<p>– <em>Leo Breiman, Statistical Modeling: The Two Cultures,
Statistical Science, Vol 16, p. 203.</em> <sub>(<em>#data,data
analysis</em>)</sub></p>
</li>
<li>
<p>I was profoundly disappointed when I saw that S-PLUS 4.5 now
provides Type III sums of squares as a routine option for the summary
method for aov objects. I note that it is not yet available for
multistratum models, although this has all the hallmarks of an oversight
(that is, a bug) rather than common sense seeing the light of day. When
the decision was being taken of whether to include this feature, because
the FDA requires it a few of my colleagues and I were consulted and our
reply was unhesitatingly a clear and unequivocal No, but it seems the
FDA and SAS speak louder and we were clearly outvoted.</p>
<p>– <em>Bill Venables, Exegeses on Linear Models</em>
<sub>(<em>#data,data analysis,lsmeans</em>)</sub></p>
</li>
<li>
<p>Some of us feel that type III sum of squares and so-called
LS-means are statistical nonsense which should have been left in
SAS.</p>
<p>– <em>Brian Ripley, Discussing features of S-Plus, S-news
5.29.1999</em> <sub>(<em>#data,data
analysis,lsmeans</em>)</sub></p>
</li>
<li>
<p>I think it would be interesting to ask people who use the results
from LSMEANS to explain what the results represent. My guess is that
less than 1% of the people who use LSMEANS know what they in fact
are.</p>
<p>– <em>Doug Bates, R-help mailing list, 16 Oct 2003</em>
<sub>(<em>#data,data analysis,lsmeans</em>)</sub></p>
</li>
<li>
<p>Doing applied statistics is never easy, especially if you want to
get it right.</p>
<p>– <em>Xiao-Li Meng, 2005 Joint Statistical Meetings</em>
<sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>I agree with the general message: “The right variables make a big
difference for accuracy. Complex statistical methods, not so much.” This
is similar to something Hal Stern told me once: the most important
aspect of a statistical analysis is not what you do with the data, it’s
what data you use.</p>
<p>– <em>Andrew Gelman, The most important aspect of a statistical
analysis is not what you do with the data, it’s what data you use,
2018.</em> <sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>Once upon a time, the phrase ‘statistical reduction of data’ was
used as a synonym for statistical analysis; it implied refining and
concentrating the data so as eventually to express the main features in
a much smaller number of means, indices, coefficients. … Today some
statisticians and some computer programs seem more disposed to undertake
‘statistical expansion of data’, perhaps with an original 96
observations leading to 25 pages of output.</p>
<p>– <em>D. J. Finney, Was This In Your Statistics Textbook: 1.
Agricultural Scientist And Statistician, Experimental Agriculture, 24,
153-161.</em> <sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>Data analysis is a tricky business – a trickier business than
even tricky data analysts sometimes think.</p>
<p>– <em>Bert Gunter, S-news mailing list, 6 Jun 2002</em>
<sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>A first analysis of experimental results should, I believe,
invariably be conducted using flexible data analytical
techniques–looking at graphs and simple statistics–that so far as
possible allow the data to ‘speak for themselves’. The unexpected
phenomena that such a approach often uncovers can be of the greatest
importance in shaping and sometimes redirecting the course of an ongoing
investigation.</p>
<p>– <em>George Box, Signal to Noise Ratios, Performance Criteria, and
Transformations, Technometrics, 30, 1–17</em> <sub>(<em>#data,data
analysis,eda</em>)</sub></p>
</li>
<li>
<p>When I was in graduate school, a fellow student who was writing
his dissertation with the late William G. Cochran passed along some of
Cochran’s advice: You make a bigger contribution to statistics if you
find a workable solution to an important unsolved problem than if you
find an optimal solution where a good one already exists.</p>
<p>– <em>Fred L. Ramsey and Daniel W. Schafer, The American
Statistician, 54, 78.</em> <sub>(<em>#data,data
analysis</em>)</sub></p>
</li>
<li>
<p>The six degrees of freedom for error provided by the 4x4 Latin
square have long been recognized as inadequate, at least by Fisher.
Something of the order of 12 error degrees of freedom would appear
desirable…unless the effects under investigation are large in comparison
with their experimental errors.</p>
<p>– <em>Frank Yates, Complex Experiments, Supplement to the Journal of
the Royal Statistical Society, 1935, Vol 2, No. 1.</em>
<sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>The old rule of trusting the Central Limit Theorem if the sample
size is larger than 30 is just that–old. Bootstrap and permutation
testing let us more easily do inferences for a wider variety of
statistics.</p>
<p>– <em>Tim Hesterberg</em> <sub>(<em>#data,data
analysis</em>)</sub></p>
</li>
<li>
<p>Scrutiny [of data] should take in the <em>names</em> of variates.
Analysis of variates y1 to y5 is not statistics; analysis of plant
height in centimeters, root weight in grams, etc., may be.</p>
<p>– <em>D. A. Preece, In discussion of C. Chatfield, “The initial
examination of data”, Journal of the Royal Statistical Society. Series A
(1985), p. 234.</em> <sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>To call in the statistician after the experiment is done may be
no more than asking him to perform a postmortem examination: he may be
able to say what the experiment died of.</p>
<p>– <em>R.A. Fisher, Sankya, Indian Statistical Congress, Vol 4,
p. 17.</em> <sub>(<em>#data,data analysis</em>)</sub></p>
</li>
<li>
<p>It is clear that a statistician who is involved at the start of
an investigation, advises on data collection, and who knows the
background and objectives, will generally make a better job of the
analysis than a statistician who was called in later on.</p>
<p>– <em>Christopher Chatfield, Problem solving : a statistician’s
guide, 1988, p. 12.</em> <sub>(<em>#data,data
analysis</em>)</sub></p>
</li>
<li>
<p>We really haven’t got any great amount of data on the subject,
and without data how can we reach any definite conclusions?</p>
<p>– <em>Thomas Alva Edison (1847-1931)</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>Small data…fits in memory on a laptop: &lt;10 GB. Medium
data…fits in memory on a server: 10 GB-1 TB. Big data…can’t fit in
memory on one computer: &gt;1 TB.</p>
<p>– <em>Hadley Wickham, Big Data Pipelines, 2015.</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>A <em>massive</em> data set is one for which the size,
heterogeneity, and general complexity cause serious pain for the
analyst(s).</p>
<p>– <em>J. Kettenring, Massive data sets…reflections on a workshop,
Computing Science and Statistics, Proceedings of the 33rd Symposium on
the Interface, Vol 33, 2001.</em> <sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>The Dirty Data Theorem states that “real world” data tends to
come from bizarre and unspecifiable distributions of highly correlated
variables and have unequal sample sizes, missing data points,
non-independent observations, and an indeterminate number of
inaccurately recorded values.</p>
<p>– <em>Unknown, Statistically Speaking, p. 282.</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>The Titanic survival data seem to become to categorical data
analysis what Fisher’s Iris data are to discriminant analysis.</p>
<p>– <em>Andreas Buja, A Word from the Editor of JCGS, Statistical
Computing &amp; Graphics Newsletter, V10, N1, p 32.</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>Consideration needs to be given to the most appropriate data to
be collected. Often the temptation is to collect too much data and not
give appropriate attention to the most important. Filing cabinets and
computer files world-wide are filled with data that have been collected
because they may be of interest to someone in future. Most is never of
interest to anyone and if it is, its existence is unknown to those
seeking the information, who will set out to collect the data again,
probably in a trial better designed for the purpose. In general, it is
best to collect only the data required to answer the questions posed,
when setting up the trial, and plan another trial for other data in the
future, if necessary.</p>
<p>– <em>P. Portmann &amp; H. Ketata, Statistical Methods for Plant
Variety Evaluation, p. 15.</em> <sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>We have found that some of the hardest errors to detect by
traditional methods are unsuspected gaps in the data collection (we
usually discovered them serendipitously in the course of graphical
checking).</p>
<p>– <em>Peter Huber, Huge data sets, Compstat ’94: Proceedings,
1994.</em> <sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>Every messy data is messy in its own way - it’s easy to define
the characteristics of a clean dataset (rows are observations, columns
are variables, columns contain values of consistent types). If you start
to look at real life data you’ll see every way you can imagine data
being messy (and many that you can’t)!</p>
<p>– <em>Hadley Wickham, R-help mailing list, 17 Jan 2008</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>What all practicing data analysts agree on is that the proportion
of project time spent on data cleaning is huge. Estimates of 75-90
percent have been suggested.</p>
<p>– <em>Unknown, Graphics of Large Datasets, p. 20.</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>That the ten digits do not occur with equal frequency must be
evident to any one making much use of logarithmic tables, and noticing
how much faster the first pages wear out than the last ones.</p>
<p>– <em>Simon Newcomb, Note on the frequencies of the different digits
in natural numbers, Amer. J. Math, 4, 39-40, 1881.</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>For a hundred years or so, mathematical statisticians have been
in love with the fact that the probability distribution of the sum of a
very large number of very small random deviations always converges to a
normal distribution. This infatuation tended to focus interest away from
the fact that, for real data, the normal distribution is often rather
poorly realized, if it is realized at all.</p>
<p>– <em>Unknown, Numerical Recipes in C, p 520.</em>
<sub>(<em>#data,normality</em>)</sub></p>
</li>
<li>
<p>I abhor averages. I like the individual case. A man may have six
meals one day and none the next, making an average of three meals per
day, but that is not a good way to live.</p>
<p>– <em>Louis Brandeis</em>
<sub>(<em>#data,averages</em>)</sub></p>
</li>
<li>
<p>The per capita gross national product of a nation…as a measure of
the comfort of individual lives is about as apt, say, as deciding how to
dress in the morning according to the mean annual temperature of the
region in which one lives. If one lives in the tropics this would work
well. But if one lives in Minnesota, where the temperature might be
thirty degrees below zero one morning and one hundred degrees above zero
another morning, one would be in danger of dying of exposure or of
prostration most of the time. The problem with aggregate statistics is
that they obscure both the extremes and patterns of distribution.</p>
<p>– <em>Paul Gruchow, Grass Roots, p. 44.</em>
<sub>(<em>#data,averages</em>)</sub></p>
</li>
<li>
<p>In former times, when the hazards of sea voyages were much more
serious than they are today, when ships buffeted by storms threw a
portion of their cargo overboard, it was recognized that those whose
goods were sacrificed had a claim in equity to indemnification at the
expense of those whose goods were safely delivered. The value of the
lost goods was paid for by agreement between all of those whose
merchandise had been in the same ship. This sea damage to cargo in
transit was known as ‘havaria’ and the word came naturally to be applied
to the compensation money which each individual was called upon to pay.
From this Latin word derives our modern word ‘average’.</p>
<p>– <em>M. J. Moroney, Facts from Figures, p. 34.</em>
<sub>(<em>#data,averages</em>)</sub></p>
</li>
<li>
<p>Some people hate the very name of statistics, but I find them
full of beauty and interest. Whenever they are not brutalised, but
delicately handled by the higher methods, and are warily interpreted,
their power of dealing with complicated phenomena is extraordinary. They
are the only tools by which an opening can be cut through the formidable
thicket of difficulties that bars the path of those who pursue the
Science of man.</p>
<p>– <em>Frances Galton, Natural Inheritance.</em>
<sub>(<em>#statistics,science</em>)</sub></p>
</li>
<li>
<p>The central limit theorem is often used to justify the assumption
of normality when using the sample mean and the the sample standard
deviation. But it is inevitable that real data contain gross errors.
Five to ten percent unusual values in a dataset seem to be the rule
rather than the exception (Hampel 1973). The distribution of such data
is no longer Normal.</p>
<p>– <em>A. S. Hedayat and Guoqin Su, Robustness of the Simultaneous
Estimators of Location and Scale From Approximating a Histogram by a
Normal Density Curve, The American Statistician, 2012, 66, p. 25.</em>
<sub>(<em>#data,outliers,normality</em>)</sub></p>
</li>
<li>
<p>Why is a particular record or measurement classed as an outlier?
Among all who handle and interpret statistical data, the word has long
been in common use as an epithet for any item among a dataset of N that
departs markedly from the broad pattern of the set.</p>
<p>– <em>David Finney, Calibration Guidelines Challenge Outlier
Practices, The American Statistician, 2006, Vol 60, No 4, p. 310.</em>
<sub>(<em>#data,outliers</em>)</sub></p>
</li>
<li>
<p>Dodge (2003) provided a definition of ‘outlier’ that is helpful
but far from complete: In a sample of N observations, it is possible for
a limited number to be so far separated in value from the remainder that
they give rise to the question whether they are not from a different
population, or that the sampling technique is at fault. Such values are
called outliers.</p>
<p>– <em>David Finney, Calibration Guidelines Challenge Outlier
Practices, The American Statistician, 2006, Vol 60, No 4, p. 310.</em>
<sub>(<em>#data,outliers</em>)</sub></p>
</li>
<li>
<p>The finding of an outlier is not necessarily a discovery of a bad
or misleading datum that may contaminate the data, but it may amount to
a comment on the validity of distributional assumptions inherent in the
form of analysis that is contemplated.</p>
<p>– <em>David Finney, Calibration Guidelines Challenge Outlier
Practices, The American Statistician, 2006, Vol 60, No 4, p. 312.</em>
<sub>(<em>#data,outliers</em>)</sub></p>
</li>
<li>
<p>If any observation has been classed as an outlier, the next step
should be if possible to infer the cause…attention should be given to
the possibility that laboratory and data management techniques have been
imperfect: improvements and safeguards for the future should be
considered.</p>
<p>– <em>David Finney, Calibration Guidelines Challenge Outlier
Practices, The American Statistician, 2006, Vol 60, No 4, p. 312.</em>
<sub>(<em>#data,outliers</em>)</sub></p>
</li>
<li>
<p>The motivation for any action on outliers must be to improve
interpretation of data without ignoring unwelcome truth. To remove bad
and untrustworthy data is a laudable ambition, but naive and untested
rules may bring harm rather than benefit.</p>
<p>– <em>David Finney, Calibration Guidelines Challenge Outlier
Practices, The American Statistician, 2006, Vol 60, No 4, p. 312.</em>
<sub>(<em>#data,outliers</em>)</sub></p>
</li>
<li>
<p>One cautious approach is represented by Bernoulli’s more
conservative outlook. If there are very strong reasons for believing
that an observation has suffered an accident that made the value in the
data-file thoroughly untrustworthy, then reject it; in the absence of
clear evidence that an observation, identified by formal rule as an
outlier, is unacceptable then retain it unless there is lack of trust
that the laboratory obtaining it is conscientiously operated by able
persons who have “…taken every care.”</p>
<p>– <em>David Finney, Calibration Guidelines Challenge Outlier
Practices, The American Statistician, 2006, Vol 60, No 4, p. 313.</em>
<sub>(<em>#data,outliers</em>)</sub></p>
</li>
<li>
<p>Treat outliers like children. Correct them when necessary, but
never throw them out.</p>
<p>– <em>Unknown., Top 12 Tip #2. Practical Stats’ Applied Environmental
Statistics course</em> <sub>(<em>#data,outliers</em>)</sub></p>
</li>
<li>
<p>There are a lot of statistical methods looking at whether an
outlier should be deleted…I don’t endorse any of them.</p>
<p>– <em>Barry Nussbaum, Significance, Apr 2017.</em>
<sub>(<em>#data,outliers</em>)</sub></p>
</li>
<li>
<p>All this discussion of deleting the outliers is completely
backwards. In my work, I usually throw away all the good data, and just
analyze the outliers.</p>
<p>– <em>Unknown pharmaceutical statistician, The American Statistician,
Vol 61, No 3, page 193.</em>
<sub>(<em>#data,outliers</em>)</sub></p>
</li>
<li>
<p>I have often thought that outliers contain more information than
the model.</p>
<p>– <em>Arnold Goodman, 2005 Joint Statistical Meetings</em>
<sub>(<em>#data,outliers</em>)</sub></p>
</li>
<li>
<p>Whatever actually happened, outliers need to be investigated not
omitted. Try to understand what caused some observations to be different
from the bulk of the observations. If you understand the reasons, you
are then in a better position to judge whether the points can
legitimately removed from the data set, or whether you’ve just
discovered something new and interesting. Never remove a point just
because it is weird.</p>
<p>– <em>Rob J. Hyndman, Omitting outliers, 2016</em>
<sub>(<em>#data,outliers</em>)</sub></p>
</li>
<li>
<p>Scholars feel the need to present tables of model parameters in
academic articles (perhaps just as evidence that they ran the analysis
they claimed to have run), but these tables are rarely interpreted other
than for their sign and statistical significance. Most of the numbers in
these tables are never even discussed in the text. From the perspective
of the applied data analyst, R packages without procedures to compute
quantities of scientific interest are woefully incomplete. A better
approach focuses on quantities of direct scientific interest rather than
uninterpretable model parameters. … For each quantity of interest, the
user needs some summary that includes a point estimate and a measure of
uncertainty such as a standard error, confidence interval, or a
distribution. The methods of calculating these differ greatly across
theories of inference and methods of analysis. However, from the user’s
perspective, the result is almost always the same: the point estimate
and uncertainty of some quantity of interest.</p>
<p>– <em>Kousuke Imai, Gary King, Oliva Lau, Toward a Common Framework
for Statistical Analysis and Development, Journal of Computational and
Graphical Statistics, 2008, v 17.</em>
<sub>(<em>#data,tables,uncertainty</em>)</sub></p>
</li>
<li>
<p>The purpose of plotting is to convey phenomena to the viewer’s
cortex, not to provide a place to lookup observed numbers.</p>
<p>– <em>Kaye Basford, John Tukey, Graphical Analysis of Multi-Response
Data, p. 373.</em> <sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>Had we started with this [quantile] plot, noticed that it looks
straight and not looked further, we would have missed the important
features of the data. The general lesson is important. Theoretical
quantile-quantile plots are not a panacea and must be used in
conjunction with other displays and analyses to get a full picture of
the behavior of the data.</p>
<p>– <em>John M. Chambers, William S. Cleveland, Beat Kleiner, Paul A.
Tukey, Graphical Methods for Data Analysis, p. 212.</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>Visualization for large data is an oxymoron–the art is to reduce
size before one visualizes. The contradiction (and challenge) is that we
may need to visualize first in order to find out how to reduce size.</p>
<p>– <em>Peter Huber, Massive datasets workshop: Four years after,
Journal of Computational and Graphical Statistics, Vol 8, 635–652.</em>
<sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>Pie charts have severe perceptual problems… If you want to
display some data, and perceiving the information is not so important,
then a pie chart is fine.</p>
<p>– <em>Unknown, S-Plus 2000 Programmer’s Guide, p. 349.</em>
<sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>Merely drawing a plot does not constitute visualization.
Visualization is about conveying important information to the reader
accurately. It should reveal information that is in the data and should
not impose structure on the data.</p>
<p>– <em>W. Huber, X. Li, and R. Gentleman, Bioinformatics and
Computational Biology Solutions using R and Bioconductor, p. 162.</em>
<sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>While the dendrogram has been widely used to represent distances
between objects, it cannot really be considered to be a visualization
method. Dendrograms do not necessarily expose structure that exists in
the data. In many cases they impose structure on the data, and when that
is the case it is dangerous to interpret the observed structure.</p>
<p>– <em>W. Huber, X. Li, and R. Gentleman, Bioinformatics and
Computational Biology Solutions using R and Bioconductor, p. 170.</em>
<sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>[When] you see excellent graphics, find out how they were done.
Borrow strength from demonstrated excellence. The idea for information
design is: Don’t get it original, get it right.</p>
<p>– <em>Edward Tufte</em> <sub>(<em>#data
visualization,design</em>)</sub></p>
</li>
<li>
<p>Graphical excellence is that which gives to the viewer the
greatest number of ideas in the shortest time with the least ink in the
smallest space.</p>
<p>– <em>Edward Tufte, The Visual Display of Quantitative Information,
1983.</em> <sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>Chartjunk does not achieve the goals of its propagators. The
overwhelming fact of data graphics is that they stand or fall on their
content, gracefully displayed. Graphics do not become attractive and
interesting through the addition of ornamental hatching and false
perspective to a few bars.</p>
<p>– <em>Edward Tufte, The Visual Display of Quantitative Information,
1983, p. 121.</em> <sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>A table is nearly always better than a dumb pie chart; the only
worse design than a pie chart is several of them, for then the viewer is
asked to compare quantities located in spatial disarray both within and
between pies…Given their low data-density and failure to order numbers
along a visual dimension, pie charts should never be used. Above all
else show the data.</p>
<p>– <em>Edward Tufte, The Visual Display of Quantitative Information,
1983, p. 178.</em> <sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>In medical research, too often the first published study testing
a new treatment provides the strongest evidence that will ever be found
for that treatment. As better controlled studies–less vulnerable to the
enthusiasms of researchers and their sponsors–are then conducted, the
treatment’s reported efficacy declines. Years after the initial study
[…] sometimes the only remaining issue is whether the treatment is in
fact harmful.</p>
<p>– <em>Edward Tufte, Beautiful Evidence, p. 144.</em>
<sub>(<em>#statistics,significance</em>)</sub></p>
</li>
<li>
<p>The preliminary examination of most data is facilitated by the
use of diagrams. Diagrams prove nothing, but bring outstanding features
readily to the eye; they are therefore no substitutes for such critical
tests as may be applied to the data, but are valuable in suggesting such
tests, and in explaining the conclusions founded upon them.</p>
<p>– <em>Ronald A Fisher, Statistical Methods for Research Workers,
p. 27.</em> <sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>Our statistical puritanism may incline us not to use shadows, but
we confess that a little bit of shadow is fun.</p>
<p>– <em>Dan Carr, Using Layering and Perceptual Grouping in Statistical
Graphics, Statistical Computing &amp; Graphics Newsletter, V. 10, N. 1,
p. 25.</em> <sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>We are not saying that the primary purpose of a graph is to
convey numbers with as many decimal places as possible. We agree with
Ehrenberg (1975) that if this were the only goal, tables would be
better. The power of a graph is its ability to enable one to take in the
quantitative information, organize it, and see patterns and structure
not readily revealed by other means of studying the data.</p>
<p>– <em>William Cleveland &amp; Robert McGill, Graphical Perception:
Theory, Experimentation, and Application to the Development of Graphical
Models, Journal of the American Statistical Association, 79, 531-554,
1984.</em> <sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>There was a controversy [in the 1920s]…about whether the divided
bar chart or the pie chart was superior for portraying the parts of a
whole. The contest appears to have ended in a draw. We conclude that
neither graphical form should be used because other methods are
demonstrably better.</p>
<p>– <em>William Cleveland &amp; Robert McGill, Graphical Perception:
Theory, Experimentation, and Application to the Development of Graphical
Models, Journal of the American Statistical Association, 79, 531-554,
1984.</em> <sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>Our conclusion about [choropleth] patch maps agrees with Tukey’s
(1979), who left little doubt about his opinions by stating, ‘I am
coming to be less and less satisfied with the set of maps that some
dignify by the name <em>statistical map</em> and that I would gladly
revile with the name <em>patch map</em>’.</p>
<p>– <em>William Cleveland &amp; Robert McGill, Graphical Perception:
Theory, Experimentation, and Application to the Development of Graphical
Models, Journal of the American Statistical Association, 79, 531–554,
1984.</em> <sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>It’s generally considered bad practice to use more than six
colors in a single display.</p>
<p>– <em>Ross Ihaka, R-help mailing list, 2004</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>The mere multiplicity of the attempts to deal with more than
three continuous dimensions by encoding additional variables into
glyphs, Chernoff faces, stars, Kleiner-Hartigan trees, and so on
indicates that each of them has met only with rather limited
success.</p>
<p>– <em>Peter Huber, Statistical graphics: history and overview,
Proceedings of the Fourth Annual Conference and Exposition, p. 674.</em>
<sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>Spatial patterns may be due to many sources of variation. In the
context of seeking explanations, John Tukey said that, “the unadjusted
plot should not be made.” In other words, our perceptual/cognitive
abilities are poor in terms of adjusting for known source of variations
and envisioning the resulting map. A better strategy is to control for
known sources of variation and/or adjust the estimates before making the
map.</p>
<p>– <em>Dan Carr, Survey Research Methods Section newsletter, July
2002.</em> <sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>It’s not easy to select more than a few clearly distinct colors.
Also, “distinct” is context-dependent, because: What will be the spatial
relationships of the different colors in your output? You can
successfully have fairly similar colors adjacent to each other, since
the contrast is more obvious when they’re adjacent. However, if you want
to use colors to track identity and difference across scattered points
or patches, then you need bigger separations between colors, since you
want to be able to see easily that patch “A” here is of the same kind as
patch “A” there and different from patch “B” somewhere else, when
mingled with patches of other kinds. And size matters. Big patches of
similar color (as on a map) can look quite distinct, while the same
colors used to plot filled circular blobs on a graph might be barely
distinguishable, and totally indistinguishable if used to plot colored
“.”s or “+”s. … It’s all very psycho-visual and success usually requires
experimentation!</p>
<p>– <em>Ted Harding, R-help mailing list, 2004</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>The concept of randomness arises partly from games of chance. The
word ‘chance’ derives from the Latin <em>cadentia</em> signifying the
fall of a die. The word ‘random’ itself comes from the French
<em>randir</em> meaning to run fast or gallop.</p>
<p>– <em>G. Spencer Brown, Probability and Scientific Inference, Chapter
VII, p. 35.</em> <sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>Statistics derives from a German term, ‘Statistik’, first used as
a substantive by the Gottingen professor Gottfried Achenwall in
1749.</p>
<p>– <em>Theodore M. Porter, The Rise of Statistical Thinking
1820-1900.</em> <sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>Strangely, the motto chosen by the founders of the Statistical
Society in 1834 was ‘Aliis exterendum’, which means ‘Let other thrash it
out.’ William Cochran confessed that ‘it is a little embarrassing that
statisticians started out by proclaiming what they will not do’.</p>
<p>– <em>Edmund A. Gehan and Noreen A. Lemak, Statistics in Medical
Research: Developments in Clinical Trials</em>
<sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>What accounts for the success of the [Iowa State] Stat Lab? I
believe that it is because it was not driven by the mathematics, but by
actual problems in biology, genetics, demography, economics, psychology,
and so on. To be sure, a real problems give rise to abstract problems in
statistical inference which have a fascination of their own. However,
for statistics to remain viable, statistical problems should have their
genesis in real, data-related problems.</p>
<p>– <em>Oscar Kempthorne, A conversation with Oscar Kempthorne,
Statistical Science, 1995, V 10, p. 335.</em>
<sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>You prepare yourself to win. You prepare yourself for the
possibility that you won’t win. You don’t really prepare yourself for
the possibility that you flip the coin in the air and it lands on its
edge and you get neither outcome.</p>
<p>– <em>Al Gore, On the 2004 presidential election, Chance News
10.01.</em> <sub>(<em>#history</em>)</sub></p>
</li>
<li>
<p>The invalid assumption that correlation implies cause is probably
among the two or three most serious and common errors of human
reasoning.</p>
<p>– <em>Stephen Jay Gould, The Mismeasure of Man</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>When noise is correlated it becomes music.</p>
<p>– <em>Anindya Roy, Personal communication</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>As I left consulting to go back to the university, these were the
perceptions I had about working with data to find answers to problems:
(a) Focus on finding a good solution–that’s what consultants get paid
for. (b) Live with the data before you plunge into modelling. (c) Search
for a model that gives a good solution, either algorithmic or data. (d)
Predictive accuracy on test sets is the criterion for how good the model
is. (e) Computers are an indispensable partner.</p>
<p>– <em>Leo Breiman, Statistical Modeling: The Two Cultures,
Statistical Science, Vol. 16, p. 201.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>I have always thought that statistical design and sampling from
populations should be the first courses taught, but all elementary
courses I know of start with statistical methods or probability. To me,
this is putting the cart before the horse!</p>
<p>– <em>Walter Federer, A Conversation with Walter T Federer,
Statistical Science, 2005, Vol 20, p. 312.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Bill Hunter told me that their editor wanted a title for their
book with sex appeal. Thus, “Statistics for Experimenters”, which is
pretty subliminal but it’s there.</p>
<p>– <em>Robert Easterling, The American Statistician, v 58, p 248.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>The only useful function of a statistician is to make
predictions, and thus to provide a basis for action.</p>
<p>– <em>W. Edwards Deming, W. A. Wallis, 1980. The Statistical Research
Group. Journal of the American Statistical Association, 75, 321.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>We must watch our own language. For example, “Type I error” and
“Type II error” are meaningless and misleading terms. Instead, try
“chance of a false alarm” and “a missed opportunity”.</p>
<p>– <em>Deborah J. Rumsey, Assessing Student Retention of Essential
Statistical Ideas: Perspectives, Priorities, and Possibilities, American
Statistician, Vol 62, No 1, p. 58.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Statistics prove near &amp; far that folks who drive like
crazy…are.</p>
<p>– <em>Anon, Burma Shave sign in the Advertising Museum in
Portland</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>It’s a random pattern. That’s the pattern.</p>
<p>– <em>Ed Chigliak, TV series “Northern Exposure”</em>
<sub>(<em>#statistics,random numbers</em>)</sub></p>
</li>
<li>
<p>Randomness is NOT the absence of a pattern.</p>
<p>– <em>Bill Venables, 1999 S-Plus User’s Conference</em>
<sub>(<em>#statistics,random numbers</em>)</sub></p>
</li>
<li>
<p>The statistics on sanity are that one out of every four Americans
is suffering from some form of mental illness. Think of your three best
friends. If they are okay, then it’s you.</p>
<p>– <em>Rita Mae Brown</em></p>
</li>
<li>
<p>It is proven that the celebration of birthdays is healthy.
Statistics show that those people who celebrate the most birthdays
become the oldest.</p>
<p>– <em>S. den Hartog, Ph D. Thesis University of Groningen</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Because no one becomes statistically self-sufficient after one
semester of study, I try to prepare students to become intelligent
consumers of the assistance that they will inevitably seek. Service
courses train future clients, not future statisticians.</p>
<p>– <em>Michael W. Tosset, “Statistical Science”, Feb 98, p. 24.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>If there was ever an idea in statistics which evokes the
reaction, “Why the hell didn’t I think of that,” it has to be the
bootstrap.</p>
<p>– <em>James R. Thompson, 1997 Interface Proceedings</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>The sensible statistician should be wary of other people’s
statistics. In particular, it is unwise to believe all official
statistics, such as government statements that the probability of a
nuclear meltdown is only one in 10,000 years (remember Chernobyl!).</p>
<p>– <em>Christopher Chatfield, Problem solving: a statistician’s guide,
1988, p 73.</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>The government are very keen on amassing statistics. They collect
them, add them, raise them to the n-th power, take the cube root and
prepare wonderful diagrams. But you must never forget that every one of
these figures comes in the first instance from the village watchman, who
just puts down what he damn pleases.</p>
<p>– <em>English judge on the subject of Indian statistics, Quoted in
Sir Josiah Stamp in Some Economic Matters in Modern Life, London: King
and Sons, 1929, pp. 258-259.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>It was always important for the biometrician to take part in the
field-work for most kinds of trials. A willingness to get our hands
dirty did much to dispel the distrust of the theoretician from Head
Office, as well as giving us an appreciation of the practical problems.
Trials were always carried out to simulate farming conditions as much as
possible. We had once advocated a change in wheat plot lengths from 2
chains to 3, based on results from uniformity trials. It seemed a very
good idea until a biometrician went to help harvest a very good crop and
found he had to lift and carry bags of over 100 lb, this being the yield
from each plot. But the local agriculturist in charge of the trial would
never have reported this; he would only have gone on grumbling about
those ‘theory guys’ in Head Office forever.</p>
<p>– <em>Jean Heywood (nee Miller), A History of Statistics in New
Zealand, edited by H.S.Roberts. p. 23-24.</em> <sub>(<em>#statistics,
biometry, expt design</em>)</sub></p>
</li>
<li>
<p>Econometrics has successfully predicted 14 of the last 3 economic
depressions.</p>
<p>– <em>David Hand, Speaking at Interface 2000.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>We feel that nothing can replace the value to a [corn] breeder of
careful study and understanding of his plants…More and more, we feel
that grave danger exists of statistics being used as a substitute for
critical observation and thought…Statistics have their place, a very
important one, but they can never serve as a substitute for close
association with plants. Their real value, it seems to us, is in
measuring precisely what we already know in a general way. Statistics
tends to be an office art based on machines and figures rather than a
field art based on living things.</p>
<p>– <em>Henry A. Wallace and William L. Brown, Corn and Its Early
Fathers, 1956, p. 123.</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>The great scientific weakness of America today is that she tends
to emphasize quantity at the expense of quality–statistics instead of
genuine insight–immediate utilitarian application instead of genuine
thought about fundamentals.</p>
<p>– <em>Henry A. Wallace and William L. Brown, Corn and Its Early
Fathers, 1956, p. 124.</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>It is easy to lie with statistics, but it is easier to lie
without them.</p>
<p>– <em>Frederick Mosteller</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>There are aspects of statistics other than it being
intellectually difficult that are barriers to learning. For one thing,
statistics does not benefit from a glamorous image that motivates
students to persist through tedious and frustrating lessons…there are no
TV dramas with a good-looking statistician playing the lead, and few
mothers’ chests swell with pride as they introduce their son or daughter
as “the statistician.”</p>
<p>– <em>Chap T. Le and James R. Boen, Health and Numbers: Basic
Statistical Methods</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>At its core statistics is not about cleverness and technique, but
rather about <em>honesty</em>. Its real contribution to society is
primarily <em>moral</em>, not technical. It is about doing <em>the right
thing</em> when interpreting empirical information. Statisticians are
<em>not</em> the world’s best computer scientists, mathematicians, or
scientific subject matter specialists. We <em>are</em> (potentially, at
least) the best at the <em>principled</em> collection, summarization,
and analysis of data.</p>
<p>– <em>Stephen B. Vardeman and Max D. Morris, Statistics and Ethics:
Some Advice for Young Statisticians, The American Statistician, vol 57,
p. 21.</em> <sub>(<em>#statistics,ethics</em>)</sub></p>
</li>
<li>
<p>Statistical analysis of data can only be performed within the
context of selected assumptions, models, and/or prior distributions. A
statistical analysis is actually the extraction of substantive
information from data and assumptions. And herein lies the rub,
understood well by Disraeli and others skeptical of our work: For given
data, an analysis can usually be selected which will result in
“information” more favorable to the owner of the analysis then is
objectively warranted.</p>
<p>– <em>Stephen B. Vardeman and Max D. Morris, Statistics and Ethics:
Some Advice for Young Statisticians, The American Statistician, vol 57,
p. 25.</em> <sub>(<em>#statistics,ethics</em>)</sub></p>
</li>
<li>
<p>Too much of what all statisticians do … is blatantly subjective
for any of us to kid ourselves or the users of our technology into
believing that we have operated ‘impartially’ in any true sense. … We
can do what seems to us most appropriate, but we can not be objective
and would do well to avoid language that hints to the contrary.</p>
<p>– <em>Steve Vardeman, Comment, 1987, Journal of the American
Statistical Association, 82, 130-131.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>The standard error of most statistics is proportional to 1 over
the square root of the sample size. God did this, and there is nothing
we can do to change it.</p>
<p>– <em>Howard Wainer, Improving Tabular Displays, With NAEP Tables as
Examples and Inspirations, Journal of Educational and Behavioral
Statistics, Vol 22, No. 1, pp. 1-30.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Suppose that Sir R. A. Fisher–a master of public relations–had
not taken over from ordinary English such evocative words as
“sufficient”, “efficient”, and “consistent” and made them into precisely
defines terms of statistical theory. He might, after all, have used
utterly dull terms for those properties of estimators, calling them
characteristics A, B, and C. … Would his work have had the same smashing
influence that it did? I think not, or at least not as rapidly.</p>
<p>– <em>William H. Kruskal, Formulas, Numbers, Words: Statistics in
Prose, The American Scholar, 1978</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Statistics state the status of the state.</p>
<p>– <em>Leland Wilkinson, The Grammar of Graphics, p. 165.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>My philosophy on lotteries is that while you actually have to buy
a ticket in order to win the lottery, buying a ticket does not
significantly increase your odds of winning.</p>
<p>– <em>Howie Smith, Prsonal communication</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>If you show your friends your confidence interval for the
standard error of the estimated length of the confidence interval of
your confidence about yourself, I guess one nice thing to ask to freak
them out is: “Can you construct a confidence interval for the confidence
level of my confidence?</p>
<p>– <em>Tony Baiching, Personal communication</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>To make the preliminary test on variances is rather like putting
to sea in a rowing boat to find out whether conditions are sufficiently
calm for an ocean liner to leave port.</p>
<p>– <em>George E. P. Box, Non-normality and Tests on Variances,
Biometrika, 40, 318-335.</em>
<sub>(<em>#statistics,nhst</em>)</sub></p>
</li>
<li>
<p>Statistics is, or should be, about scientific investigation and
how to do it better, but many statisticians believe it is a branch of
mathematics.</p>
<p>– <em>George Box, AmStat News, Oct 2000, page 11.</em>
<sub>(<em>#statistics,Box quotes</em>)</sub></p>
</li>
<li>
<p>These days the statistician is often asked such questions as “Are
you a Bayesian?” “Are you a frequentist?” “Are you a data analyst?” “Are
you a designer of experiments?”. I will argue that the appropriate
answer to ALL of these questions can be (and preferably should be)
“yes”, and that we can see why this is so if we consider the scientific
context for what statisticians do.</p>
<p>– <em>George E.P. Box</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>George Box: We don’t need robust methods. A good statistician
(particularly a Bayesian one) will model the data well and find the
outliers. John Tukey: They ran over 2000 statistical analyses at
Rothamsted last week and nobody noticed anything. A red light warning
would be most helpful.</p>
<p>– <em>George Box vs. John Tukey, Douglas Martin, 1999 S-Plus
Conference Proceedings.</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>We thus echo the classical Bayesian literature in concluding that
‘noninformative prior information’ is a contradiction in terms. The flat
prior carries information just like any other; it represents the
assumption that the effect is likely to be large. This is often not
true. Indeed, the signal-to-noise ratio s is often very low and then it
is necessary to shrink the unbiased estimate. Failure to do so by
inappropriately using the flat prior causes overestimation of effects
and subsequent failure to replicate them.</p>
<p>– <em>Erik van Zwet &amp; Andrew Gelman, A proposal for informative
default priors scaled by the standard error of estimates, The American
Statistician, 76, p. 7.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>Another reason for the applied statistician to care about
Bayesian inference is that consumers of statistical answers, at least
interval estimates, commonly interpret them as probability statements
about the possible values of parameters. Consequently, the answers
statisticians provide to consumers should be capable of being
interpreted as approximate Bayesian statements.</p>
<p>– <em>Donald B. Rubin, Bayesianly justifiable and relevant frequency
calculations for the applied statistician. Annals of Statistics,
12(4):1151-1172, 1984.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>In most cases the frequentist adopts numerical values because
they are convenient in that the calculations can be easily performed.
For instance, a reliability engineer will use an exponential
distribution or, if that is too gross, a Weibull. In the majority of
frequentist analyses there is little justification for the assumed
likelihood, and it is as subjective as any prior.</p>
<p>– <em>D. V. Lindley, Discussion, The American Statistician, August
1997, Vol. 51, page 265.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>In contrast to the logical development and intuitive
interpretations of the Bayesian approach, frequentist methods are nearly
impossible to understand, even for the best students. Consider
confidence intervals. Many instructors err in describing confidence
intervals and even some texts err. But whether texts or instructors err
in explaining them, students do not understand them. And they carry this
misunderstanding with them into later life. Calculating a confidence
interval is easy. But everyone except the cognoscenti believes that when
one calculates 95% confidence limits of 2.6 and 7.9, say, the
probability is 95% that the parameter in question lies in the interval
from 2.6 to 7.9. P values are nearly as obscure as confidence intervals.
… Students in frequentist courses may learn very well how to calculate
confidence intervals and P values, but they cannot give them correct
interpretations. I stopped teaching frequentist methods when I decided
that they could not be learned.</p>
<p>– <em>Donald A. Berry, Teaching Elementary Bayesian Statistics with
Real Applications in Science, The American Statistician, 51, p 242.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>Uniform priors on probabilities are ubiquitous. I agree that they
can be useful. However, if the probability in question is the prevalence
of HIV in California, it is ridiculous to assert that a prevalence of
100% is equally plausible with 0%, or that the chance that it is above
50% is the same as the chance that it is below 50%. It is particularly
noxious to call such a prior noninformative. Instead, it is
disinformative. The likelihood notwith standing, positing a uniform
prior for the probability that the sun will rise tomorrow is equally
ridiculous, given that we are all confident that it has risen all the
days of our lives.</p>
<p>– <em>Wesley O. Johnson, Comment: Bayesian Statistics in the Twenty
First Century, The American Statistician, Feb 2013, 67, p 10.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>The best way to convey to the experimenter what the data tell him
about theta is to show him a picture of the posterior distribution.</p>
<p>– <em>George E. P. Box &amp; G. C. Tiao, Bayesian Inference in
Statistical Analysis (1973)</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>If a prior is anything at all, it’s a belief prior to some data.
We can discuss how our priors differ from each other and argue about
whose prior is best justified. But when data come in and priors are
converted to posteriors, no one learns that their prior was wrong, any
more than seeing a coin landing heads shows me that calling odds of
50-50 was wrong.</p>
<p>– <em>Richard Gunton, Priors aren’t wrong, Significance, (2025), 22,
p. 43.</em> <sub>(<em>#bayesian</em>)</sub></p>
</li>
<li>
<p>If one could get some rational basis for obtaining the prior,
then there would be no problem. But people have seminars these days
about something where someone says, ‘I am going to use such and such a
prior’. Where does he get the prior? It is not data based. It is a
mathematical convenience or something like that. It is not even obtained
by using Bayes’ theorem. Why one should believe the outcome of using
this seems to be a very moot point.</p>
<p>– <em>Oscar Kempthorne, A conversation with Oscar Kempthorne,
Statistical Science, 1995, V 10, p. 333.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>In the design of experiments, one has to use some informal prior
knowledge. How does one construct blocks in a block design problem for
instance? It is stupid to think that use is not made of a prior. But
knowing that this prior is utterly casual, it seems ludicrous to go
through a lot of integration, etc., to obtain ‘exact’ posterior
probabilities resulting from this prior. So, I believe the situation
with respect to Bayesian inference and with respect to inference, in
general, has not made progress. Well, Bayesian statistics has led to a
great deal of theoretical research. But I don’t see any real
utilizations in applications, you know. Now no one, as far as I know,
has examined the question of whether the inferences that are obtained
are, in fact, realized in the predictions that they are used to
make.</p>
<p>– <em>Oscar Kempthorne, “A conversation with Oscar Kempthorne”,
Statistical Science, 1995, V 10, p. 334.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>I sometimes think that the only real difference between Bayesian
and non-Bayesian hierarchical modelling is whether random effects are
labeled with Greek or Roman letters.</p>
<p>– <em>Peter Diggle, Comment on Bayesian analysis of agricultural
field experiments, 1999, J. Royal Statistical Society B, 61,
691–746.</em> <sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>The practicing Bayesian is well advised to become friends with as
many numerical analysts as possible.</p>
<p>– <em>James Berger, Statistical Decision Theory and Bayesian
Analysis, p. 202.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>You just say “Bayesian,” and people think you are some kind of
genius.</p>
<p>– <em>Gary Churchill, Bayes offers a new way to make sense of
numbers, Science, 19 Nov 1999.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>Bayesian computations give you a straightforward answer you can
understand and use. It says there is an X% probability that your
hypothesis is true-not that there is some convoluted chance that if you
assume the null hypothesis is true, you’ll get a similar or more extreme
result if you repeated your experiment thousands of times. How does one
interpret THAT!</p>
<p>– <em>Steven Goodman, Bayes offers a new way to make sense of
numbers, Science, 19 Nov 1999.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>Bayesian methods are complicated enough, that giving researchers
user-friendly software could be like handing a loaded gun to a toddler;
if the data is crap, you won’t get anything out of it regardless of your
political bent.</p>
<p>– <em>Brad Carlin, Bayes offers a new way to make sense of numbers,
Science, 19 Nov 1999.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>If a study, even a statistically significant one, suggests that
pigs can fly, Bayes’s theorem allows researchers to combine the study’s
results mathematically with hundreds of years of knowledge about the
travel habits of swine.</p>
<p>– <em>David Leonhardt, New York Times, April 28, 2001</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>If the prior distribution, at which I am frankly guessing, has
little or no effect on the result, then why bother; and if it has a
large effect, then since I do not know what I am doing how would I dare
act on the conclusions drawn?</p>
<p>– <em>Richard W Hamming, The Art of Probability for Scientists and
Engineers, 1991, p. 298.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>If you read Bayesian polemics from the 1970s and 1980s, including
my own, it’s usually arrogant and even insulting. Some of the terms were
excessively pointed. For example, Bayesians identified which frequentist
methods were “incoherent”, or more accurately, lamented that none seemed
to be coherent. On the other hand, Bayesians were accused of being
“biased”. The rhetoric was not all that different from that of the
Fisher/Pearson duels. But we Bayesians have stopped saying derogatory
things, partly because we have changed and partly because frequentists
have been listening. When you’re walking beside someone you tend to be
cordial; when you’re trying to catch up to tell them something and they
are ignoring what you say, you sometimes yell.</p>
<p>– <em>Don Berry, Celebrating 70: An Interview with Don Berry,
Statistical Science, 2012, Vol. 27, No. 1, 144-159.</em>
<sub>(<em>#statistics,bayesian</em>)</sub></p>
</li>
<li>
<p>The traditional methods design of experiments are taught and/or
discussed in textbooks are not the ways design of experiments are or
should be used for real-world applications.</p>
<p>– <em>George Milliken, Applied Statistics in Agriculture Conference,
2009</em> <sub>(<em>#statistics,experimental design</em>)</sub></p>
</li>
<li>
<p>The statistician who supposes that his main contribution to the
planning of an experiment will involve statistical theory, finds
repeatedly that he makes his most valuable contribution simply by
persuading the investigator to explain why he wishes to do the
experiment, by persuading him to justify the experimental treatments,
and to explain why it is that the experiment, when completed, will
assist him in his research.</p>
<p>– <em>Gertrude Cox, Lecture in Washington 11 January 1951</em>
<sub>(<em>#statistics,experimental design</em>)</sub></p>
</li>
<li>
<p>An important distinction needs to be made between experimental
designs using complete blocks and those using incomplete blocks as
regards to three functions: 1. reducing the error mean square 2.
adjusting estimates closer to true values, and 3. refining rankings.
Complete blocks include all treatments whereas incomplete blocks include
a subset of the treatments. Both can reduce the residual error, but only
incomplete blocks can also adjust estimates of treatment effects closer
to the true values and thereby refine rankings among the treatments.
These adjusted estimates are usually more accurate than the raw averages
over replicates, but not always (exactly as is the case for accuracy
gain through more replication). Likewise, these adjusted rankings are
more likely to identify correctly the best treatments. By declaring a
smaller error but doing nothing to sharpen estimates or refine rankings,
complete block designs are rather impotent. It is ironic that scientists
rarely understand this huge difference between getting one benefit or
three from blocking.</p>
<p>– <em>Hugh G Gauch, Three Strategies for Gaining Accuracy, American
Scientist.</em> <sub>(<em>#statistics,experimental
design</em>)</sub></p>
</li>
<li>
<p>On a final note, we would like to stress the importance of
design, which often does not receive the attention it deserves.
Sometimes, the large number of modeling options for spatial analysis may
raise the false impression that design does not matter, and that a
sophisticated analysis takes care of everything. Nothing could be
further from the truth.</p>
<p>– <em>Hans-Peter Piepho, Martin P. Boer, Emlyn R. Williams,
Two-dimensional P-spline smoothing for spatial analysis of plant
breeding trials, “Biometrical Journal”, Feb 2022.</em>
<sub>(<em>#statistics,experimental design</em>)</sub></p>
</li>
<li>
<p>At this meeting for the hybrid corn industry, we have no
reservation about recommending a design which consists of a single
replicate of treatments at a given location. For some audiences, such a
statement can severely damage the reputation of the person making the
statement. University experiment station personnel in particular regard
replications within an environment as a necessary part of good research.
They are not. Sprague (1955) and many others have shown most researchers
otherwise.</p>
<p>– <em>R. E. Stucker &amp; D. R. Hicks, Experimental Design and Plot
Size Considerations for On-Farm Research, Proceedings of the 46th Annual
Corn and Sorghum Industry Research Conference , 1991, p. 60.</em>
<sub>(<em>#statistics,experimental design</em>)</sub></p>
</li>
<li>
<p>The message from a statistician’s point of view is very clear.
Replicate over environments, do not replicate within environments. This
is not news. At North Carolina State in the early 60s, any graduate
student interested in quantitative aspects of plant breeding and
genetics had a standard answer for the number of replicates needed in an
experiment: use one replicate if you’re estimating means, and use two
replicates if you’re estimating variances. Implicit in the answer was,
“the experiment will be evaluated in more than one environment”.</p>
<p>– <em>R. E. Stucker &amp; D. R. Hicks, Experimental Design and Plot
Size Considerations for On-Farm Research, Proceedings of the 46th Annual
Corn and Sorghum Industry Research Conference , 1991, p. 62.</em>
<sub>(<em>#statistics,experimental design</em>)</sub></p>
</li>
<li>
<p>Which I would like to stress are: (1) A significant effect is not
necessarily the same thing as an interesting effect. (2) A
non-significant effect is not necessarily the same thing as no
difference.</p>
<p>– <em>Christopher Chatfield, Problem solving : a statistician’s
guide, p. 51.</em>
<sub>(<em>#statistics,significance</em>)</sub></p>
</li>
<li>
<p>Rejection of a true null hypothesis at the 0.05 level will occur
only one in 20 times. The overwhelming majority of these false
rejections will be based on test statistics close to the borderline
value. If the null hypothesis is false, the inter-ocular traumatic test
[“hit between the eyes”] will often suffice to reject it; calculation
will serve only to verify clear intuition.</p>
<p>– <em>W. Edwards, Harold Lindman, Leonard J. Savage, Bayesian
Statistical Inference for Psychological Research, University of
Michigan</em>
<sub>(<em>#statistics,significance,nhst</em>)</sub></p>
</li>
<li>
<p>When statistical inferences, such as p-values, follow extensive
looks at the data, they no longer have their usual interpretation.
Ignoring this reality is dishonest: it is like painting a bull’s eye
around the landing spot of your arrow. This is known in some circles as
p-hacking, and much has been written about its perils and pitfalls.</p>
<p>– <em>Robert E Kass, Brian S. Caffo, Marie Davidian, Xiao-Li Meng,
Bin Yu, Nancy Reid., Ten Simple Rules for Effective Statistical
Practice, PLoS Comput Biol 12(6):e1004961.</em>
<sub>(<em>#statistics,significance,nhst</em>)</sub></p>
</li>
<li>
<p>The difference between “statistically significant” and “not
statistically significant” is not in itself necessarily statistically
significant. By this, I mean more than the obvious point about arbitrary
divisions, that there is essentially no difference between something
significant at the 0.049 level or the 0.051 level. I have a bigger point
to make. It is common in applied research–in the last couple of weeks, I
have seen this mistake made in a talk by a leading political scientist
and a paper by a psychologist–to compare two effects, from two different
analyses, one of which is statistically significant and one which is
not, and then to try to interpret/explain the difference. Without any
recognition that the difference itself was not statistically
significant.</p>
<p>– <em>Andrew Gelman, The difference between ‘statistically
significant’ and ‘not statistically significant’ is not in itself
necessarily statistically significant, 2005</em>
<sub>(<em>#statistics,significance</em>)</sub></p>
</li>
<li>
<p>The p-value is a concept so misaligned with intuition that no
civilian can hold it firmly in mind. Nor can many statisticians.</p>
<p>– <em>Matt Briggs, Why do statisticians answer silly questions that
no one ever asks?, Significance, Vol 9, No 1, p. 30.</em>
<sub>(<em>#statistics,significance,nhst</em>)</sub></p>
</li>
<li>
<p>A quotation of a p-value is part of the ritual of science, a
sprinkling of the holy waters in an effort to sanctify the data analysis
and turn consumers of the results into true believers.</p>
<p>– <em>William Cleveland, Visualizing Data, p. 177.</em>
<sub>(<em>#statistics,significance,nhst</em>)</sub></p>
</li>
<li>
<p>We should push for de-emphasizing some topics, such as
statistical significance tests–an unfortunate carry-over from the
traditional elementary statistics course. We would suggest a greater
focus on confidence intervals—these achieve the aim of formal hypothesis
testing, often provide additional useful information, and are not as
easily misinterpreted.</p>
<p>– <em>Gerry Hahn et. al, The Impact of Six Sigma Improvement–A
Glimpse Into the Future of Statistics, The American Statistician, August
1999.</em> <sub>(<em>#statistics,significance,nhst</em>)</sub></p>
</li>
<li>
<p>We statisticians must accept much of the blame for cavalier
attitudes toward Type I errors. When we teach practitioners in other
scientific fields that multiplicity is not important, they believe us,
and feel free to thrash their data set mercilessly, until it finally
screams “uncle” and relinquishes significance. The recent conversion of
the term “data mining” to mean a statistical <em>good</em> rather than a
statistical <em>evil</em> also contributes to the problem.</p>
<p>– <em>Peter Westfall, Applied Statistics in Agriculture (Proceedings
of the 13th annual conference), page 5.</em>
<sub>(<em>#statistics,significance,nhst</em>)</sub></p>
</li>
<li>
<p>While the main emphasis in the development of power analysis has
been to provide methods for assessing and increasing power, it should
also be noted that it is possible to have too much power. If your sample
is too large, nearly any difference, no matter how small or meaningless
from a practical standpoint, will be ‘statistically significant’.</p>
<p>– <em>Clay Helberg</em>
<sub>(<em>#statistics,significance,power,nhst</em>)</sub></p>
</li>
<li>
<p>Remember that a p-value merely indicates the probability of a
particular set of data being generated by the null model–it has little
to say about the size of a deviation from that model (especially in the
tails of the distribution, where large changes in effect size cause only
small changes in p-values).</p>
<p>– <em>Clay Helberg</em>
<sub>(<em>#statistics,significance,nhst</em>)</sub></p>
</li>
<li>
<p>Given what I know about data, models, and assumptions, I find
more than 2 significant digits of printout for a p-value to be
indefensible. (I actually think 1 digit is about the max).</p>
<p>– <em>Terry Therneau, S-news mailing list, 8 Nov 2000</em>
<sub>(<em>#statistics,significance</em>)</sub></p>
</li>
<li>
<p>In the calculus of real statistical inference, and by that I mean
actual data problems (which S was designed for), all p-values &lt; 10^-6
or so are identical. This is one of the few areas in fact where I like
SAS better: the creators of their PROCs are smart enough to print these
numbers as zero and leave it at that. There are no Gaussian
distributions in the real world, and the central limit theorem has
failed long, long before 10^-17.</p>
<p>– <em>Terry Therneau, S-news mailing list, 4 Apr 2002</em>
<sub>(<em>#statistics,significance,computing</em>)</sub></p>
</li>
<li>
<p>It’s a commonplace among statisticians that a chi-squared test
(and, really, any p-value) can be viewed as a crude measure of sample
size: When sample size is small, it’s very difficult to get a rejection
(that is, a p-value below 0.05), whereas when sample size is huge, just
about anything will bag you a rejection. With large n, a smaller signal
can be found amid the noise. In general: small n, unlikely to get small
p-values. Large n, likely to find something. Huge n, almost certain to
find lots of small p-values.</p>
<p>– <em>Andrew Gelman, The sample size is huge, so a p-value of 0.007
is not that impressive, 2009.</em>
<sub>(<em>#statistics,significance,nhst</em>)</sub></p>
</li>
<li>
<p>Work by Bickel, Ritov, and Stoker (2001) shows that
goodness-of-fit tests have very little power unless the direction of the
alternative is precisely specified. The implication is that omnibus
goodness-of-fit tests, which test in many directions simultaneously,
have little power, and will not reject until the lack of fit is
extreme.</p>
<p>– <em>Leo Breiman, Statistical Modeling: The Two Cultures,
Statistical Science, Vol 16, p. 203.</em>
<sub>(<em>#statistics,significance,nhst</em>)</sub></p>
</li>
<li>
<p>Visualizations act as a campfire around which we gather to tell
stories.</p>
<p>– <em>Al Shalloway, 2011</em> <sub>(<em>#eda,data
visualization</em>)</sub></p>
</li>
<li>
<p>If students have students have no experience with hands-on
[telescope] observing, they may take all data as ‘truth’ without having
an understanding of how the data are obtained and what could potentially
go wrong in that process, so I think it becomes crucially important to
give a glimpse of what’s happening behind the scenes at telescopes, so
they can be appropriately skeptical users of data in the future.</p>
<p>– <em>Colette Salyk, Sky &amp; Telescope, Apr 2022 p. 31.</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>Normality is a myth; there never was, and never will be, a normal
distribution. This is an over-statement from the practical point of
view, but it represents a safer initial mental attitude than any in
fashion during the past two decades.</p>
<p>– <em>R. C. Geary, Testing for normality, 1947. Biometrika 34 :
209-242.</em> <sub>(<em>#nhst,normality</em>)</sub></p>
</li>
<li>
<p>Furthermore, the mere declaration that the interaction is or is
not significant is far too coarse a result to give agronomists or plant
breeders effective insight into their research material.</p>
<p>– <em>Hugh G. Gauch Jr., Model selection and validation for yield
trials with interaction, 1988. Biometrics 44 : 705-715.</em></p>
</li>
<li>
<p>Analysis of variance … stems from a hypothesis-testing
formulation that is difficult to take seriously and would be of limited
value for making final conclusions.</p>
<p>– <em>Herman Chernoff, Comment, 1986. The American Statistician 40(1)
: 5-6.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>The peculiarity of … statistical hypotheses is that they are not
conclusively refutable by any experience.</p>
<p>– <em>Richard B. Braithwaite, Scientific Explanation. A Study of the
Function of Theory, Probability and Law in Science (p. 151), 1953.
Cambridge University Press.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>…“no batch of observations, however large, either definitively
rejects or definitively fails to reject the hypothesis H0.</p>
<p>– <em>Richard B. Braithwaite, Scientific Explanation. A Study of the
Function of Theory, Probability and Law in Science (p. 160), 1953.
Cambridge University Press.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>what John Dewey called ‘the quest for certainty’ is, in the case
of empirical knowledge, a snare and a delusion.</p>
<p>– <em>Richard B. Braithwaite, Scientific Explanation. A Study of the
Function of Theory, Probability and Law in Science (p. 163), 1953.
Cambridge University Press.</em>
<sub>(<em>#knowledge,uncertainty</em>)</sub></p>
</li>
<li>
<p>The ultimate justification for any scientific belief will depend
upon the main purpose for which we think scientifically–that of
predicting and thereby controlling the future.</p>
<p>– <em>Richard B. Braithwaite, Scientific Explanation. A Study of the
Function of Theory, Probability and Law in Science (p. 174), 1953.
Cambridge University Press.</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Most readers of The American Statistician will recognize the
limited value of hypothesis testing in the science of statistics. I am
not sure that they all realize the extent to which it has become the
primary tool in the religion of Statistics.</p>
<p>– <em>David Salsburg, The Religion of Statistics as Practiced in
Medical Journals, 1985. The American Statistician, 39, 220-223.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>We are better off abandoning the use of hypothesis tests entirely
and concentrating on developing continuous measures of toxicity which
can be used for estimation.</p>
<p>– <em>David Salsburg, Statistics for Toxicologists, 1986. New York,
Marcel Dekker, Inc.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>I do not think that significance testing should be completely
abandoned … and I don’t expect that it will be. But I urge researchers
to provide estimates, with confidence intervals: scientific advance
requires parameters with known reliability estimates. Classical
confidence intervals are formally equivalent to a significance test, but
they convey more information.</p>
<p>– <em>Nigel G. Yoccoz, Use, Overuse, and Misuse of Significance Tests
in Evolutionary Biology and Ecology. Bulletin of the Ecological Society
of America, Vol. 72, No. 2 (Jun., 1991), pp. 106-111.</em>
<sub>(<em>#nhst,significance,uncertainty</em>)</sub></p>
</li>
<li>
<p>In marked contrast to what is advocated by most statisticians,
most evolutionary biologists and ecologists overemphasize the potential
role of significance testing in their scientific practice. Biological
significance should be emphasized rather than statistical significance.
Furthermore, a survey of papers showed that the literature is
infiltrated by an array of misconceptions about the use and
interpretation of significance tests. … By far the most common error is
to confound statistical significance with biological, scientific
significance… Statements like ‘the two populations are significantly
different relative to parameter X (P=.004)’ are found with no mention of
the estimated difference. … Most biologists and other users of
statistical methods still seem to be unaware that significance testing
by itself sheds little light on the questions they are posing.</p>
<p>– <em>Nigel G. Yoccoz, Use, Overuse, and Misuse of Significance Tests
in Evolutionary Biology and Ecology. Bulletin of the Ecological Society
of America, Vol. 72, No. 2 (Jun., 1991), pp. 106-111.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>Tests appear to many users to be a simple way to discharge the
obligation to provide some statistical treatment of the data.</p>
<p>– <em>H. V. Roberts, For what use are tests of hypotheses and tests
of significance, 1976. Communications in Statistics, Series A,
5:753-761.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>We shall marshal arguments against [significance] testing,
leading to the conclusion that it be abandoned by all substantive
science and not just by educational research and other social sciences
which have begun to raise voices against the virtual tyranny of this
branch of inference in the academic world.</p>
<p>– <em>Louis Guttman, The illogic of statistical inference for
cumulative science, 1985. Applied Stochastic Models and Data Analysis
1:3-9.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>In practice, of course, tests of significance are not taken
seriously.</p>
<p>– <em>Louis Guttman, The illogic of statistical inference for
cumulative science, 1985. Applied Stochastic Models and Data Analysis
1:3-9.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Since a point hypothesis is not to be expected in practice to be
exactly true, but only approximate, a proper test of significance should
almost always show significance for large enough samples. So the whole
game of testing point hypotheses, power analysis notwithstanding, is but
a mathematical game without empirical importance.</p>
<p>– <em>Louis Guttman, The illogic of statistical inference for
cumulative science, 1985. Applied Stochastic Models and Data Analysis
1:3-9.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>…lack of interaction in analysis of variance and … lack of
correlation in bivariate distributions–such nullities would be quite
surprising phenomena in the usual interactive complexities of social
life.</p>
<p>– <em>Louis Guttman, What is not what in statistics, 1977. The
Statistician, 26:81-107.</em>
<sub>(<em>#nhst,correlation</em>)</sub></p>
</li>
<li>
<p>Estimation and approximation may be more fruitful than
significance in developing science, never forgetting replication.</p>
<p>– <em>Louis Guttman, What is not what in statistics, 1977. The
Statistician, 26:81-107.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>[the normal distribution] is seldom, if ever, observed in
nature.</p>
<p>– <em>Louis Guttman, What is not what in statistics, 1977. The
Statistician, 26:81-107.</em> <sub>(<em>#normality</em>)</sub></p>
</li>
<li>
<p>The test of statistical significance in psychological research
may be taken as an instance of a kind of essential mindlessness in the
conduct of research.</p>
<p>– <em>D. Bakan, The test of significance in psychological research,
1966. Psychological Bulletin 66: 423-437.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>…the test of significance has been carrying too much of the
burden of scientific inference. It may well be the case that wise and
ingenious investigators can find their way to reasonable conclusions
from data because and in spite of their procedures. Too often, however,
even wise and ingenious investigators…tend to credit the test of
significance with properties it does not have.</p>
<p>– <em>D. Bakan, The test of significance in psychological research,
1966. Psychological Bulletin 66: 423-437.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>…a priori reasons for believing that the null hypothesis is
generally false anyway. One of the common experiences of research
workers is the very high frequency with which significant results are
obtained with large samples.</p>
<p>– <em>D. Bakan, The test of significance in psychological research,
1966. Psychological Bulletin 66: 423-437.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>…there is really no good reason to expect the null hypothesis to
be true in any population … Why should any correlation coefficient be
exactly .00 in the population? … why should different drugs have exactly
the same effect on any population parameter?</p>
<p>– <em>D. Bakan, The test of significance in psychological research,
1966. Psychological Bulletin 66: 423-437.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>…we need to get on with the business of generating … hypotheses
and proceed to do investigations and make inferences which bear on them,
instead of … testing the statistical null hypothesis in any number of
contexts in which we have every reason to suppose that it is false in
the first place.</p>
<p>– <em>D. Bakan, The test of significance in psychological research,
1966. Psychological Bulletin 66: 423-437.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>the tests of null hypotheses of zero differences, of no
relationships, are frequently weak, perhaps trivial statements of the
researcher’s aims … in many cases, instead of the tests of significance
it would be more to the point to measure the magnitudes of the
relationships, attaching proper statements of their sampling variation.
The magnitudes of relationships cannot be measured in terms of levels of
significance.</p>
<p>– <em>Leslie Kish, Some statistical problems in research design,
1959. American Sociological Review 24: 328-338.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>There are instances of research results presented in terms of
probability values of ‘statistical significance’ alone, without noting
the magnitude and importance of the relationships found. These attempts
to use the probability levels of significance tests as measures of the
strengths of relationships are very common and very mistaken.</p>
<p>– <em>Leslie Kish, Some statistical problems in research design,
1959. American Sociological Review 24: 328-338.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>One reason for preferring to present a confidence interval
statement (where possible) is that the confidence interval, by its
width, tells more about the reliance that can be placed on the results
of the experiment than does a YES-NO test of significance.</p>
<p>– <em>Mary G. Natrella, The relation between confidence intervals and
tests of significance, 1960. American Statistician 14 : 20-22, 33.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Confidence intervals give a feeling of the uncertainty of
experimental evidence, and (very important) give it in the same units …
as the original observations.</p>
<p>– <em>Mary G. Natrella, The relation between confidence intervals and
tests of significance, 1960. American Statistician 14 : 20-22, 33.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>The current obsession with .05 … has the consequence of
differentiating significant research findings and those best forgotten,
published studies from unpublished ones, and renewal of grants from
termination. It would not be difficult to document the joy experienced
by a social scientist when his F ratio or t value yields significance at
.05, nor his horror when the table reads ‘only’ .10 or .06. One comes to
internalize the difference between .05 and .06 as ‘right’ vs. ‘wrong,’
‘creditable’ vs. ‘embarrassing,’ ‘success’ vs. ‘failure’.</p>
<p>– <em>James K. Skipper Jr., Anthony L. Guenther and Gilbert Nass, The
sacredness of .05: A note concerning the uses of statistical levels of
significance in social science. The American Sociologist 2 : 16-18.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>…blind adherence to the .05 level denies any consideration of
alternative strategies, and it is a serious impediment to the
interpretation of data”</p>
<p>– <em>James K. Skipper Jr., Anthony L. Guenther and Gilbert Nass, The
sacredness of .05: A note concerning the uses of statistical levels of
significance in social science. The American Sociologist 2 : 16-18.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>… surely, God loves the .06 nearly as much as the .05.</p>
<p>– <em>R. L. Rosnow and R. Rosenthal, Statistical procedures and the
justification of knowledge and psychological science, 1989. American
Psychologist 44: 1276-1284.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>How has the virtually barren technique of hypothesis testing come
to assume such importance in the process by which we arrive at our
conclusions from our data?</p>
<p>– <em>G. R. Loftus, On the tyranny of hypothesis testing in the
social sciences, 1991. Contemporary Psychology 36: 102-105.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Despite the stranglehold that hypothesis testing has on
experimental psychology, I find it difficult to imagine a less
insightful means of transitting from data to conclusions.</p>
<p>– <em>G. R. Loftus, On the tyranny of hypothesis testing in the
social sciences. Contemporary Psychology 36: 102-105.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Whereas hypothesis testing emphasizes a very narrow question (‘Do
the population means fail to conform to a specific pattern?’), the use
of confidence intervals emphasizes a much broader question (‘What are
the population means?’). Knowing what the means are, of course, implies
knowing whether they fail to conform to a specific pattern, although the
reverse is not true. In this sense, use of confidence intervals subsumes
the process of hypothesis testing.</p>
<p>– <em>G. R. Loftus, On the tyranny of hypothesis testing in the
social sciences. Contemporary Psychology 36: 102-105.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>This remarkable state of affairs [overuse of significance
testing] is analogous to engineers’ teaching (and believing) that light
consists only of waves while ignoring its particle characteristics—and
losing in the process, of course, any motivation to pursue the most
interesting puzzles and paradoxes in the field.</p>
<p>– <em>G. R. Loftus, On the tyranny of hypothesis testing in the
social sciences, 1991. Contemporary Psychology 36: 102-105.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>The result is that non-statisticians tend to place undue reliance
on single ‘cookbook’ techniques, and it has for example become
impossible to get results published in some medical, psychological and
biological journals without reporting significance values even if of
doubtful validity. It is sad that students may actually be more confused
and less numerate at the end of a ‘service course’ than they were at the
beginning, and more likely to overlook a descriptive approach in favour
of some inferential method which may be inappropriate or incorrectly
executed.</p>
<p>– <em>C. Chatfield, The initial examination of data, 1985. Journal of
the Royal Statistical Society, Series A 148: 214-253.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>‘Common sense’ is not common but needs to learnt systematically…
A ‘simple analysis’ can be harder than it looks…. All statistical
techniques, however sophisticated, should be subordinate to subjective
judgement.</p>
<p>– <em>C. Chatfield, The initial examination of data, 1985. Journal of
the Royal Statistical Society, Series A 148: 214-253.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Thus statistics should generally be taught more as a practical
subject with analyses of real data. Of course some theory and an
appropriate range of statistical tools need to be learnt, but students
should be taught that Statistics is much more than a collection of
standard prescriptions.</p>
<p>– <em>C. Chatfield, The initial examination of data, 1985. Journal of
the Royal Statistical Society, Series A 148: 214-253.</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>More fundamentally students should be taught that instead of
asking ‘What techniques shall I use here?,’ they should ask ‘How can I
summarize and understand the main features of this set of data?’</p>
<p>– <em>C. Chatfield, The initial examination of data, 1985. Journal of
the Royal Statistical Society, Series A 148: 214-253.</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>All statistical techniques, however sophisticated, should be
subordinate to subjective judgement.</p>
<p>– <em>C. Chatfield, The initial examination of data, 1985. Journal of
the Royal Statistical Society, Series A 148: 214-253.</em></p>
</li>
<li>
<p>…it has … become impossible to get results published in some
medical, psychological and biological journals without reporting
significance values even when of doubtful validity.</p>
<p>– <em>C. Chatfield, The initial examination of data, 1985. Journal of
the Royal Statistical Society, Series A 148: 214-253.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>…to make measurements and then ignore their magnitude would
ordinarily be pointless. Exclusive reliance on tests of significance
obscures the fact that statistical significance does not imply
substantive significance.</p>
<p>– <em>I. R. Savage, Nonparametric Statistics. Journal of the American
Statistical Association, 52, 331-344.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Null hypotheses of no difference are usually known to be false
before the data are collected … when they are, their rejection or
acceptance simply reflects the size of the sample and the power of the
test, and is not a contribution to science”</p>
<p>– <em>I. R. Savage, Nonparametric Statistics. Journal of the American
Statistical Association, 52, 331-344.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>too many users of the analysis of variance seem to regard the
reaching of a mediocre level of significance as more important than any
descriptive specification of the underlying averages Our thesis is that
people have strong intuitions about random sampling; that these
intuitions are wrong in fundamental respects; that these intuitions are
shared by naive subjects and by trained scientists; and that they are
applied with unfortunate consequences in the course of scientific
inquiry. We submit that people view a sample randomly drawn from a
population as highly representative, that is, similar to the population
in all essential characteristics. Consequently, they expect any two
samples drawn from a particular population to be more similar to one
another and to the population than sampling theory predicts, at least
for small samples.</p>
<p>– <em>Amos Tversky &amp; Daniel Kahneman, Belief in the law of small
numbers. Psychological Bulletin, 76(2), 105-110.</em>
<sub>(<em>#sampling</em>)</sub></p>
</li>
<li>
<p>People have erroneous intuitions about the laws of chance. In
particular, they regard a sample randomly drawn from a population as
highly representative, that is, similar to the population in all
essential characteristics. The prevalence of the belief and its
unfortunate consequences for psychological research are illustrated by
the responses of professional psychologists to a questionnaire
concerning research decisions</p>
<p>– <em>Amos Tversky &amp; Daniel Kahneman, Belief in the law of small
numbers. Psychological Bulletin, 76(2), 105-110.</em>
<sub>(<em>#sampling</em>)</sub></p>
</li>
<li>
<p>the statistical power of many psychological studies is
ridiculously low. This is a self-defeating practice: it makes for
frustrated scientists and inefficient research. The investigator who
tests a valid hypothesis but fails to obtain significant results cannot
help but regard nature as untrustworthy or even hostile.</p>
<p>– <em>Amos Tversky &amp; Daniel Kahneman, Belief in the law of small
numbers. Psychological Bulletin, 76(2), 105-110.</em>
<sub>(<em>#nhst,power</em>)</sub></p>
</li>
<li>
<p>Significance levels are usually computed and reported, but power
and confidence limits are not. Perhaps they should be.</p>
<p>– <em>Amos Tversky &amp; Daniel Kahneman, Belief in the law of small
numbers. Psychological Bulletin, 76(2), 105-110.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>The emphasis on significance levels tends to obscure a
fundamental distinction between the size of an effect and its
statistical significance.</p>
<p>– <em>Amos Tversky &amp; Daniel Kahneman, Belief in the law of small
numbers. Psychological Bulletin, 76(2), 105-110.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Statistical hypothesis testing is commonly used inappropriately
to analyze data, determine causality, and make decisions about
significance in ecological risk assessment,… It discourages good
toxicity testing and field studies, it provides less protection to
ecosystems or their components that are difficult to sample or
replicate, and it provides less protection when more treatments or
responses are used. It provides a poor basis for decision-making because
it does not generate a conclusion of no effect, it does not indicate the
nature or magnitude of effects, it does address effects at untested
exposure levels, and it confounds effects and uncertainty…. Risk
assessors should focus on analyzing the relationship between exposure
and effects….</p>
<p>– <em>Glenn W. Suter, Abuse of hypothesis testing statistics in
ecological risk assessment, 1996. Human and Ecological Risk Assessment
2: 331-347.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>I argued that hypothesis testing is fundamentally inappropriate
for ecological risk assessment, that its use has undesirable
consequences for environmental protection, and that preferable
alternatives exist for statistical analysis of data in ecological risk
assessment. The conclusion of this paper is that ecological risk
assessors should estimate risks rather than test hypothesis</p>
<p>– <em>Glenn W. Suter, Abuse of hypothesis testing statistics in
ecological risk assessment, 1996. Human and Ecological Risk Assessment
2: 331-347.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>The purpose of an experiment is to answer questions. The truth of
this seems so obvious, that it would not be worth emphasizing were it
not for the fact that the results of many experiments are interpreted
and presented with little or no reference to the questions that were
asked in the first place.</p>
<p>– <em>T. M. Little, Interpretation and presentation of results, 1981.
Hortscience 16: 637-640.</em></p>
</li>
<li>
<p>The idea that one should proceed no further with an analysis,
once a non-significant F-value for treatments is found, has led many
experimenters to overlook important information in the interpretation of
their data.</p>
<p>– <em>T. M. Little, Interpretation and presentation of results, 1981.
Hortscience 16: 637-640.</em> <sub>(<em>#nhst,anova</em>)</sub></p>
</li>
<li>
<p>the null-hypothesis models … share a crippling flaw: in the real
world the null hypothesis is almost never true, and it is usually
nonsensical to perform an experiment with the sole aim of rejecting the
null hypothesis.</p>
<p>– <em>Jum Nunnally, The place of statistics in psychology, 1960.
Educational and Psychological Measurement 20 : 641-650.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>If rejection of the null hypothesis were the real intention in
psychological experiments, there usually would be no need to gather
data.</p>
<p>– <em>Jum Nunnally, The place of statistics in psychology, 1960.
Educational and Psychological Measurement 20 : 641-650.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Closely related to the null hypothesis is the notion that only
enough subjects need be used in psychological experiments to obtain
‘significant’ results. This often encourages experimenters to be content
with very imprecise estimates of effects.</p>
<p>– <em>Jum Nunnally, The place of statistics in psychology, 1960.
Educational and Psychological Measurement 20 : 641-650.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>We should not feel proud when we see the psychologist smile and
say ‘the correlation is significant beyond the .01 level.’ Perhaps that
is the most that he can say, but he has no reason to smile.</p>
<p>– <em>Jum Nunnally, The place of statistics in psychology, 1960.
Educational and Psychological Measurement 20 : 641-650.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>the finding of statistical significance is perhaps the least
important attribute of a good experiment.</p>
<p>– <em>D. T. Lykken, Statistical significance in psychological
research, 1968. Psychological Bulletin 70 : 151-159.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Editors must be bold enough to take responsibility for deciding
which studies are good and which are not, without resorting to letting
the p value of the significance tests determine this decision.</p>
<p>– <em>D. T. Lykken, Statistical significance in psychological
research, 1968. Psychological Bulletin 70 : 151-159.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Statistical significance testing has involved more fantasy than
fact. The emphasis on statistical significance over scientific
significance in educational research represents a corrupt form of the
scientific method. Educational research would be better off if it
stopped testing its results for statistical significance.</p>
<p>– <em>R. P. Carver, The case against statistical testing. Harvard
Educational Review 48: 378-399.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Statistical significance ordinarily depends upon how many
subjects are used in the research. the more subjects the researcher
uses, the more likely the researcher will be to get statistically
significant results.</p>
<p>– <em>R. P. Carver, The case against statistical testing. Harvard
Educational Review 48: 378-399.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>What is the probability of obtaining a dead person (D) given that
the person was hanged (H); that is, in symbol form, what is p(D|H)?
Obviously, it will be very high, perhaps .97 or higher. Now, let us
reverse the question: What is the probability that a person has been
hanged (H) given that the person is dead (D); that is, what is p(H|D)?
This time the probability will undoubtedly be very low, perhaps .01 or
lower. No one would be likely to make the mistake of substituting the
first estimate (.97) for the second (.01); that is, to accept .97 as the
probability that a person has been hanged given that the person is dead.
Even thought this seems to be an unlikely mistake, it is exactly the
kind of mistake that is made with the interpretation of statistical
significance testing—by analogy, calculated estimates of p(H|D) are
interpreted as if they were estimates of p(D|H), when they are clearly
not the same.</p>
<p>– <em>R. P. Carver, The case against statistical testing. Harvard
Educational Review 48: 378-399.</em>
<sub>(<em>#nhst,probability</em>)</sub></p>
</li>
<li>
<p>The author recommends abandoning all statistical significance
testing and suggests other ways of evaluating research results.” …
“Another reason for the popularity of statistical significance testing
is probably the complicated mathematical procedures lend an error of
scientific objectivity to conclusions.” … “Given that statistical
significance testing usually involves a corrupt form of the scientific
method and, at best, is of trivial scientific importance, journal
editors should not require it as a necessary part of a publishable
research article.</p>
<p>– <em>R. P. Carver, The case against statistical testing. Harvard
Educational Review 48: 378-399.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Pencil and paper for construction of distributions, scatter
diagrams, and run-charts to compare small groups and to detect trends,
are more efficient methods of estimation than statistical inference that
depends on variances and standard errors, as the simple techniques
preserve the information in the original data.</p>
<p>– <em>W. Edwards Deming, On probability as a basis for action, 1975.
American Statistician 29: 146-152.</em>
<sub>(<em>#eda</em>)</sub></p>
</li>
<li>
<p>We admit with Sir Winston Churchill that it sometimes pays to
admit the obvious: we do not perform an experiment to find out if two
varieties of wheat or two drugs are equal. We know in advance without
spending a dollar on an experiment that they are not equal. The
difference between two treatments or between two areas or two groups of
people, will show up as ‘significantly different’ if the experiment be
conducted through a sufficient number of trials, even thought the
difference be so small that it is of no scientific or economic
consequence. Likewise tests of whether the data of a survey or an
experiment fit some particular curve is of no scientific or economic
consequence…. With enough data no curve will fit the results of an
experiment. The question that one faces in using any curve or any
relationship is this: how robust are the conclusions? Would some other
curve make safer predictions? Statistical significance of B/A thus
conveys no knowledge, no basis for action.</p>
<p>– <em>W. Edwards Deming, On probability as a basis for action, 1975.
American Statistician 29: 146-152.</em>
<sub>(<em>#significance</em>)</sub></p>
</li>
<li>
<p>Under the usual teaching, the trusting student, to pass the
course must forsake all the scientific sense that he has accumulated so
far, and learn the book, mistakes and all.</p>
<p>– <em>W. Edwards Deming, On probability as a basis for action, 1975.
American Statistician 29: 146-152.</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>While [Edward C. Bryant] was at the University of Wyoming,
someone came in from the Department of Animal Husbandry to announce to
him an astounding scientific discovery—the fibres on the left side of
the sheep and those on the right side are of different diameter.
Dr. Bryant asked him how many fibres he had in the sample: answer,
50,000. This was a number big enough to establish significance. But what
of it? Anyone would know in advance, without spending a dollar, that
there is a difference between fibres of the left side and the right side
of any sheep, or of n sheep combined. The question is whether the
difference is of scientific importance.</p>
<p>– <em>W. Edwards Deming, On probability as a basis for action, 1975.
American Statistician 29: 146-152.</em>
<sub>(<em>#nhst,science</em>)</sub></p>
</li>
<li>
<p>Small wonder that students have trouble [with statistical
hypothesis testing]. They may be trying to think.</p>
<p>– <em>W. Edwards Deming, On probability as a basis for action, 1975.
American Statistician 29: 146-152.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>Data analysis methods in psychology still emphasize statistical
significance testing, despite numerous articles demonstrating its severe
deficiencies. It is now possible to use meta-analysis to show that
reliance on significance testing retards the development of cumulative
knowledge. The reform of teaching and practice will also require that
researchers learn that the benefits that they believe flow from use of
significance testing are illusory. Teachers must re-vamp their courses
to bring students to understand that a) reliance on significance testing
retards the growth of cumulative research knowledge; b) benefits widely
believed to flow from significance testing do not in fact exist; c)
significance testing methods must be replaced with point estimates and
confidence intervals in individual studies and with meta-analyses and
the integration of multiple studies. This reform is essential to the
future progress of cumulative knowledge and psychological research.</p>
<p>– <em>Frank L. Schmidt, Statistical significance testing and
cumulative knowledge in psychology: implications for training of
researchers. Psychological Methods 1(2), Jun 1996, 115-129.</em>
<sub>(<em>#nhst,significance,knowledge</em>)</sub></p>
</li>
<li>
<p>If the null hypothesis is not rejected, Fisher’s position was
that nothing could be concluded. But researchers find it hard to go to
all the trouble of conducting a study only to conclude that nothing can
be concluded.</p>
<p>– <em>Frank L. Schmidt, Statistical significance testing and
cumulative knowledge in psychology: implications for training of
researchers. Psychological Methods 1(2), Jun 1996, 115-129.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>Many researchers believe that statistical significance testing
confers important benefits that are in fact completely imaginary.</p>
<p>– <em>Frank L. Schmidt, Statistical significance testing and
cumulative knowledge in psychology: implications for training of
researchers. Psychological Methods 1(2), Jun 1996, 115-129.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>An important part of the explanation [of continued use of
significance testing] is that researchers hold false beliefs about
significance testing, beliefs that tell them that significance testing
offers important benefits to researchers that it in fact does not. Three
of these beliefs are particularly important. The first is the false
belief that the significance level of a study indicates the probability
of successful replications of the study…. A second false belief widely
held by researchers is that statistical significance level provides an
index of the importance or size of a difference or relation…. The third
false belief held by many researchers is the most devastating of all to
the research enterprise. This is the belief that if a difference or
relation is not statistically significant, then it is zero, or at least
so small that it can safely be considered to be zero. This is the belief
that if the null hypothesis is not rejected then it is to be accepted.
This is the belief that a major benefit from significance tests is that
they tell us whether a difference or affect is real or ‘probably just
occurred by chance’.</p>
<p>– <em>Frank L. Schmidt, Statistical significance testing and
cumulative knowledge in psychology: Implications for training of
researchers. Psychological Methods 1(2), Jun 1996, 115-129.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>We can no longer tolerate a situation in which our upcoming
generation of researchers are being trained to use discredited data
analysis methods while the broader research enterprise of which they are
to become a part has moved toward improved methods.</p>
<p>– <em>Frank L. Schmidt, Statistical significance testing and
cumulative knowledge in psychology: implications for training of
researchers. Psychological Methods 1(2), Jun 1996, 115-129.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>I believe … that hypothesis testing has been greatly
overemphasized in psychology and in the other disciplines that use it.
It has diverted our attention from crucial issues. Mesmerized by a
single all-purpose, mechanized, ‘objective’ ritual in which we convert
numbers into other numbers and get a yes-no answer, we have come to
neglect close scrutiny of where the numbers come from.</p>
<p>– <em>Jacob Cohen, Things I have learned (so far), 1990. American
Psychologist 45: 1304-1312.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>… the primary product of a research inquiry is one or more
measures of effect size, not p values.</p>
<p>– <em>Jacob Cohen, Things I have learned (so far), 1990. American
Psychologist 45: 1304-1312.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>The prevailing yes-no decision at the magic .05 level from a
single research is a far cry from the use of informed judgment. Science
simply doesn’t work that way. A successful piece of research doesn’t
conclusively settle an issue, it just makes some theoretical proposition
to some degree more [or less] likely.</p>
<p>– <em>Jacob Cohen, Things I have learned (so far), 1990. American
Psychologist 45: 1304-1312.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>One of the things I learned early on was that some things you
learn aren’t so.</p>
<p>– <em>Jacob Cohen, Things I have learned (so far), 1990. American
Psychologist 45: 1304-1312.</em></p>
</li>
<li>
<p>When a Fisherian null hypothesis is rejected with an associated
probability of, for example, .026, it is not the case that the
probability that the null hypothesis is true is .026 (or less than .05,
or any other value we can specify). Given our framework of probability
as long-run relative frequency–as much as we might wish it to be
otherwise–this result does not tell us about the truth of the null
hypothesis, given the data. (For this we have to go to Bayesian or
likelihood statistics, in which probability is not relative frequency
but degree of belief.</p>
<p>– <em>Jacob Cohen, Things I have learned (so far), 1990. American
Psychologist 45: 1304-1312.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>Despite widespread misconceptions to the contrary, the rejection
of a given null hypothesis gives us no basis for estimating the
probability that a replication of the research will again result in
rejecting that null hypothesis.</p>
<p>– <em>Jacob Cohen, Things I have learned (so far), 1990. American
Psychologist 45: 1304-1312.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>Of course, everyone knows that failure to reject the Fisherian
null hypothesis does not warrant the conclusion that it is true. Fisher
certainly knew and emphasized it, and our textbooks duly so instruct us.
Yet how often do we read in the discussion and conclusions of articles
now appearing in our most prestigious journals that ‘there is no
difference’ or ‘no relationship’.</p>
<p>– <em>Jacob Cohen, Things I have learned (so far), 1990. American
Psychologist 45: 1304-1312.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>A little thought reveals a fact widely understood among
statisticians: The null hypothesis, taken literally (and that’s the only
way you can take it in formal hypothesis testing), is always false in
the real world…. If it is false, even to a tiny degree, it must be the
case that a large enough sample will produce a significant result and
lead to its rejection. So if the null hypothesis is always false, what’s
the big deal about rejecting it.</p>
<p>– <em>Jacob Cohen, Things I have learned (so far), 1990. American
Psychologist 45: 1304-1312.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>I am, however, appalled by the fact that some publishers of
statistics packages successfully hawk their wares with the pitch that it
isn’t necessary to understand statistics to use them.</p>
<p>– <em>Jacob Cohen, Things I have learned (so far), 1990. American
Psychologist 45: 1304-1312.</em>
<sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>I argue herein that NHST [null hypothesis significance testing]
has not only failed to support the advance of psychology as a science
but also has seriously impeded it.</p>
<p>– <em>Jacob Cohen, The earth is round (p&lt;.05). 1994. American
Psychologist 49: 997-1003.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>they [confidence limits] are rarely to be found in the
literature. I suspect that the main reason they are not reported is that
they are so embarrassingly large!</p>
<p>– <em>Jacob Cohen, The earth is round (p&lt;.05). 1994. American
Psychologist 49: 997-1003.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>After four decades of severe criticism, the ritual of null
hypothesis significance testing—mechanical dichotomous decisions around
a sacred .05 criterion—still persist. This article reviews the problems
with this practice…” … “What’s wrong with [null hypothesis significance
testing]? Well, among many other things, it does not tell us what we
want to know, and we so much want to know what we want to know that, out
of desperation, we nevertheless believe that it does!</p>
<p>– <em>Jacob Cohen, The earth is round (p&lt;.05). 1994. American
Psychologist 49: 997-1003.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Tests of the null hypothesis that there is no difference between
certain treatments are often made in the analysis of agricultural or
industrial experiments in which alternative methods or processes are
compared. Such tests are … totally irrelevant. What are needed are
estimates of magnitudes of effects, with standard errors.</p>
<p>– <em>F. J. Anscombe, Discussion on Dr. David’s and Dr. Johnson’s
Paper. 1956. Journal of the Royal Statistical Society B 18 : 24-27.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>statistical significance is not the same as scientific
significance.</p>
<p>– <em>Norman S. Matloff, Statistical hypothesis testing: problems and
alternatives. 1991. Environmental Entomology 20 : 1246-1250.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>the number of stars by itself is relevant only to the question of
whether H0 is exactly true–a question which is almost always not of
interest to us, especially because we usually know a priori that H0
cannot be exactly true.</p>
<p>– <em>Norman S. Matloff, Statistical hypothesis testing: problems and
alternatives. 1991. Environmental Entomology 20 : 1246-1250.</em>
<sub>(<em>#nhst,anova</em>)</sub></p>
</li>
<li>
<p>no population has an exact normal distribution, nor are variances
exactly homogeneous, and independence assumptions are often violated to
at least some degree.</p>
<p>– <em>Norman S. Matloff, Statistical hypothesis testing: problems and
alternatives. 1991. Environmental Entomology 20 : 1246-1250.</em>
<sub>(<em>#normality</em>)</sub></p>
</li>
<li>
<p>Exact truth of a null hypothesis is very unlikely except in a
genuine uniformity trial.</p>
<p>– <em>David R. Cox, Some problems connected with statistical
inference. 1958. Annals of Mathematical Statistics 29 : 357-372.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Assumptions that we make, such as those concerning the form of
the population sampled, are always untrue.</p>
<p>– <em>David R. Cox, Some problems connected with statistical
inference. 1958. Annals of Mathematical Statistics 29 : 357-372.</em>
<sub>(<em>#sampling</em>)</sub></p>
</li>
<li>
<p>Overemphasis on tests of significance at the expense especially
of interval estimation has long been condemned.</p>
<p>– <em>David R. Cox, The role of significance tests. 1977.
Scandanavian Journal of Statistics 4: 49-70.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>…There are considerable dangers in overemphasizing the role of
significance tests in the interpretation of data.</p>
<p>– <em>David R. Cox, The role of significance tests. 1977.
Scandanavian Journal of Statistics 4: 49-70.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>In any particular application, graphical or other informal
analysis may show that consistency or inconsistency with H0 is so clear
cut that explicit calculation of p is unnecessary.</p>
<p>– <em>David R. Cox, The role of significance tests. 1977.
Scandanavian Journal of Statistics 4: 49-70.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>The central point is that statistical significance is quite
different from scientific significance and that therefore estimation …of
the magnitude of effects is in general essential regardless of whether
statistically significant departure from the null hypothesis is
achieved.</p>
<p>– <em>David R. Cox, The role of significance tests. 1977.
Scandanavian Journal of Statistics 4: 49-70.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>It is very bad practice to summarise an important investigation
solely by a value of P.</p>
<p>– <em>David R. Cox, Statistical significance tests. 1982. British
Journal of Clinical Pharmacology 14 : 325-331.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>The criterion for publication should be the achievement of
reasonable precision and not whether a significant effect has been
found.</p>
<p>– <em>David R. Cox, Statistical significance tests. 1982. British
Journal of Clinical Pharmacology 14 : 325-331.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>The continued very extensive use of significance tests is
alarming.</p>
<p>– <em>David R. Cox, Some general aspects of the theory of statistics.
1986. International Statistical Review 54: 117-126.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>It has been widely felt, probably for thirty years and more, that
significance tests are overemphasized and often misused and that more
emphasis should be put on estimation and prediction. While such a shift
of emphasis does seem to be occurring, for example in medical
statistics, the continued very extensive use of significance tests is on
the one hand alarming and on the other evidence that they are aimed,
even if imperfectly, at some widely felt need.</p>
<p>– <em>David R. Cox, Some general aspects of the theory of statistics.
1986. International Statistical Review 54: 117-126.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>the emphasis given to formal tests of significance … has resulted
in … an undue concentration of effort by mathematical statisticians on
investigations of tests of significance applicable to problems which are
of little or no practical importance … and … it has caused scientific
research workers to pay undue attention to the results of the tests of
significance … and too little to the estimates of the magnitude of the
effects they are investigating.</p>
<p>– <em>Frank Yates, The influence of Statistical Methods for Research
Workers on the development of the science of statistics. 1951. Journal
of the American Statistical Association 46: 19-34.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>…the unfortunate consequence that scientific workers have often
regarded the execution of a test of significance on an experiment as the
ultimate objective.</p>
<p>– <em>Frank Yates, The influence of Statistical Methods for Research
Workers on the development of the science of statistics. 1951. Journal
of the American Statistical Association 46: 19-34.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>[Researchers] pay undue attention to the results of tests of
significance they perform on their data, particularly data derived from
experiments, and too little to the estimates of the magnitude of the
effects which they are investigating…. The emphasis on tests of
significance, and the consideration of the results of each experiment in
isolation, have had the unfortunate consequence that scientific workers
have often regarded the execution of a test of significance on an
experiment as the ultimate objective. Results are significant or not and
that is the end to it.</p>
<p>– <em>Frank Yates, The influence of Statistical Methods for Research
Workers on the development of the science of statistics. 1951. Journal
of the American Statistical Association 46: 19-34.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>The most commonly occurring weakness … is … undue emphasis on
tests of significance, and failure to recognise that in many types of
experimental work estimates of treatment effects, together with
estimates of the errors to which they are subject, are the quantities of
primary interest.</p>
<p>– <em>Frank Yates, Sir Ronald Fisher and the design of experiments.
1964. Biometrics 20: 307-321.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>In many experiments … it is known that the null hypothesis … is
certainly untrue.</p>
<p>– <em>Frank Yates, Sir Ronald Fisher and the design of experiments.
1964. Biometrics 20: 307-321.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>A common misconception is that an effect exists only if it is
statistically significant and that it does not exist if it is not
[statistically significant].</p>
<p>– <em>Jonas Ranstam, A common misconception about p-value and its
consequences. 1996. Acta Orthopaedica Scandinavica 67 : 505-507.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>I contend that the general acceptance of statistical hypothesis
testing is one of the most unfortunate aspects of 20th century applied
science. Tests for the identity of population distributions, for
equality of treatment means, for presence of interactions, for the
nullity of a correlation coefficient, and so on, have been responsible
for much bad science, much lazy science, and much silly science. A good
scientist can manage with, and will not be misled by, parameter
estimates and their associated standard errors or confidence limits.</p>
<p>– <em>Marks Nester, A Myopic View and History of Hypothesis
Testing.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>The scientist must always give due thought to the statistical
analysis, but must never let statistical analysis be a substitute for
thinking!</p>
<p>– <em>Marks Nester, A Myopic View and History of Hypothesis
Testing.</em>
<sub>(<em>#science,significance,statistics</em>)</sub></p>
</li>
<li>
<p>The purpose of this paper is severalfold. First, we attempt to
convince the reader that at its worst, the results of statistical
hypothesis testing can be seriously misleading, and at its best it
offers no informational advantage over its alternatives; in fact it
offers less.</p>
<p>– <em>D. Jones and N. Matloff, Statistical hypothesis testing in
biology: a contradiction in terms. 1986. Journal of Economic Entomology
79: 1156-1160.</em> <sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>In view of our long-term strategy of improving our theories, our
statistical tactics can be greatly improved by shifting emphasis away
from over-all hypothesis testing in the direction of statistical
estimation. This always holds true when we are concerned with the actual
size of one or more differences rather than simply in the existence of
differences.</p>
<p>– <em>David A. Grant, Testing the null hypothesis and the strategy
and tactics of investigating theoretical models. 1962. Psychological
Review 69 : 54-61.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>The null hypothesis of no difference has been judged to be no
longer a sound or fruitful basis for statistical investigation…
Significance tests do not provide the information that scientists need,
and, furthermore, they are not the most effective method for analyzing
and summarizing data.</p>
<p>– <em>Cherry Ann Clark, Hypothesis testing in relation to statistical
methodology. 1963. Review of Educational Research 33: 455-473.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>There is nothing wrong with the t-test; it has merely been used
to give an answer that was never asked for. The Student t-test answers
the question: ‘Is there any real difference between the means of the
measurement by the old and the new method, or could the apparent
difference have arisen from random variation?’ We already know that
there is a real difference, so the question is pointless. The question
we should have answered is: ‘How big is the difference between the two
sets of measurements, and how precisely have we determined it?’</p>
<p>– <em>L. Sayn-Wittgenstein, Statistics - salvation or slavery? 1965.
Forestry Chronicle 41 : 103-105.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>Somehow there has developed a widespread belief that statistical
analysis is legitimate only if it includes significance testing. This
belief leads to, and is fostered by, numerous introductory statistics
texts that are little more than catalogues of techniques for performing
significance tests.</p>
<p>– <em>D. G. Altman, Discussion of Dr Chatfield’s paper. 1985. Journal
of the Royal Statistical Society A 148 : 242.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>Testing the equality of 2 true treatment means is ridiculous.
They will always be different, at least beyond the hundredth decimal
place.</p>
<p>– <em>V. Chew, Statistical hypothesis testing: an academic exercise
in futility. 1977. Proceedings of the Florida State Horticultural
Society 90 : 214-215.</em> <sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>It is surely apparent that anyone who wants to obtain a
significant difference badly enough can obtain one … choose a sample
size large enough.</p>
<p>– <em>A. Binder, Further considerations on testing the null
hypothesis and the strategy and tactics of investigating theoretical
models. 1963. Psychological Review 70 : 107-115.</em>
<sub>(<em>#nhst,significance,sampling</em>)</sub></p>
</li>
<li>
<p>As Confucius might have said, if the difference isn’t different
enough to make a difference, what’s the difference?</p>
<p>– <em>V. Chew, Testing differences among means: correct
interpretation and some alternatives. 1980. HortScience 15(4) :
467-470.</em> <sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>Some hesitation about the unthinking use of significance tests is
a sign of statistical maturity.</p>
<p>– <em>D. S. Moore and G. P. McCabe, Introduction to the Practice of
Statistics. 1989. W. H. Freeman and Company (New York).</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>It is usually wise to give a confidence interval for the
parameter in which you are interested.</p>
<p>– <em>D. S. Moore and G. P. McCabe, Introduction to the Practice of
Statistics. 1989. W. H. Freeman and Company (New York).</em>
<sub>(<em>#significance</em>)</sub></p>
</li>
<li>
<p>Unfortunately, when applied in a cook-book fashion, such
significance tests do not extract the maximum amount of information
available from the data. Worse still, misleading conclusions can be
drawn. There are at least three problems: (1) a conclusion that there is
a significant difference can often be reached merely by collecting
enough samples; (2) a statistically significant result is not
necessarily practically significant; and (3) reports of the presence or
absence of significant differences for multiple tests are not comparable
unless identical sample sizes are used.</p>
<p>– <em>G. B. McBride, J. C. Loftis, &amp; N. C. Adkins, What do
significance tests really tell us about the environment?. 1993.
Environmental Management 17, 423-432 (1993).</em>
<sub>(<em>#nhst,significance,sampling</em>)</sub></p>
</li>
<li>
<p>In many experiments it seems obvious that the different
treatments must have produced some difference, however small, in effect.
Thus the hypothesis that there is no difference is unrealistic: the real
problem is to obtain estimates of the sizes of the differences.</p>
<p>– <em>William G. Cochran, and George M. Cox, Experimental Designs.
2nd ed. 1957. John Wiley &amp; Sons, Inc.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>I suggest to you that Sir Ronald has befuddled us, mesmerized us,
and led us down the primrose path. I believe that the almost universal
reliance on merely refuting the null hypothesis as the standard method
for corroborating substantive theories in the soft areas is a terrible
mistake, is basically unsound, poor scientific strategy, and one of the
worst things that ever happened in the history of psychology.</p>
<p>– <em>P. E. Meehl, Theoretical risks and tabular asterisks: Sir Karl,
Sir Ronald, and the slow progress of soft psychology. 1978. Journal of
Consulting and Clinical Psychology 46 : 806-834.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Probably all theories are false in the eyes of God.</p>
<p>– <em>P. E. Meehl, Theoretical risks and tabular asterisks: Sir Karl,
Sir Ronald, and the slow progress of soft psychology. 1978. Journal of
Consulting and Clinical Psychology 46 : 806-834.</em>
<sub>(<em>#science,significance</em>)</sub></p>
</li>
<li>
<p>The grotesque emphasis on significance tests in statistics
courses of all kinds … is taught to people, who if they come away with
no other notion, will remember that statistics is about tests for
significant differences. … The apparatus on which their statistics
course has been constructed is often worse than irrelevant, it is
misleading about what is important in examining data and making
inferences.</p>
<p>– <em>John A. Nelder, Discussion of Dr Chatfield’s paper. 1985.
Journal of the Royal Statistical Society A 148 : 238.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Statistics is intimately connected with science and technology,
and few mathematicians have experience or understanding of the methods
of either.</p>
<p>– <em>John A. Nelder, Discussion of Dr Chatfield’s paper. 1985.
Journal of the Royal Statistical Society A, 148, p. 238.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>if experimenters realized how little is the chance of their
experiments discovering what they are intended to discover, then a very
substantial proportion of the experiments that are now in progress would
have been abandoned in favour of an increase in size of the remaining
experiments, judged more important.</p>
<p>– <em>Jerzy Neyman, The use of the concept of power in agricultural
experimentation. 1958. Journal of the Indian Society of Agricultural
Statistics 9 : 9-17.</em> <sub>(<em>#power</em>)</sub></p>
</li>
<li>
<p>What was the probability (power) of detecting interactions … in
the experiment performed? … The probability in question is frequently
relatively low … in cases of this kind the fact that the test failed to
detect the existence of interactions does not mean very much. In fact,
they may exist and have gone undetected.</p>
<p>– <em>Jerzy Neyman, The use of the concept of power in agricultural
experimentation. 1958. Journal of the Indian Society of Agricultural
Statistics 9 : 9-17.</em> <sub>(<em>#power</em>)</sub></p>
</li>
<li>
<p>In addition to important technical errors, fundamental errors in
the philosophy of science are frequently involved in this indiscriminate
use of the tests [of significance].</p>
<p>– <em>Denton E. Morrison &amp; Ramon E Henkel, Significance tests
reconsidered. 1969. The American Sociologist 4 : 131-140.</em>
<sub>(<em>#nhst,science,significance</em>)</sub></p>
</li>
<li>
<p>Researchers have long recognized the unfortunate connotations and
consequences of the term ‘significance,’ and we propose it is time for a
change.</p>
<p>– <em>Denton E. Morrison &amp; Ramon E Henkel, Significance tests
reconsidered. 1969. The American Sociologist 4 : 131-140.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>there is evidence that significance tests have been a genuine
block to achieving … knowledge.</p>
<p>– <em>Denton E. Morrison &amp; Ramon E Henkel, Significance tests
reconsidered. 1969. The American Sociologist 4 : 131-140.</em>
<sub>(<em>#nhst,significance,knowledge</em>)</sub></p>
</li>
<li>
<p>scientists care about whether a result is statistically
significant, but they should care much more about whether it is
meaningful.</p>
<p>– <em>Deirdre N. McCloskey, The insignificance of statistical
significance. 1995. Scientific American 272(4) : 104-105.</em>
<sub>(<em>#nhst,science,significance</em>)</sub></p>
</li>
<li>
<p>The statistician should not always remain in his or her own
office: not only is relevant information more likely to be on hand in
the experimenter’s department, but in the longer term the statistician
stands to gain immeasurably in understanding of agricultural problems by
often visiting other departments and their laboratories and fields.</p>
<p>– <em>David J. Finney, Was this in your statistics textbook? I.
Agricultural Scientist and Statistician. 1988. Experimental Agriculture
24 : 153-161.</em> <sub>(<em>#statistician</em>)</sub></p>
</li>
<li>
<p>Rigid dependence upon significance tests in single experiments is
to be deplored.</p>
<p>– <em>David J. Finney, Was this in your statistics textbook? III.
Design and analysis. 1988. Experimental Agriculture 24 : 421-432.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>A null hypothesis that yields under two different treatments have
identical expectations is scarcely very plausible, and its rejection by
a significance test is more dependent upon the size of an experiment
than upon its untruth.</p>
<p>– <em>David J. Finney, Was this in your statistics textbook? III.
Design and analysis. 1988. Experimental Agriculture 24 : 421-432.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>I have failed to find a single instance in which the Duncan test
was helpful, and I doubt whether any of the alternative tests [multiple
range significance tests] would please me better.</p>
<p>– <em>David J. Finney, Was this in your statistics textbook? III.
Design and analysis. 1988. Experimental Agriculture 24 : 421-432.</em>
<sub>(<em>#nhst,significance</em>)</sub></p>
</li>
<li>
<p>Is it ever worth basing analysis and interpretation of an
experiment on the inherently implausible null hypothesis that two (or
more) recognizably distinct cultivars have identical yield
capacities?</p>
<p>– <em>David J. Finney, Was this in your statistics textbook? III.
Design and analysis. 1988. Experimental Agriculture 24 : 421-432.</em>
<sub>(<em>#nhst</em>)</sub></p>
</li>
<li>
<p>Prediction is very difficult, especially of the future.</p>
<p>– <em>Niels Henrick David Bohr</em>
<sub>(<em>#time,science,history</em>)</sub></p>
</li>
<li>
<p>Standard errors of variance components are dumb because the
distribution of a variance component is not symmetric, but Chi-squared
and highly skewed.</p>
<p>– <em>Doug Bates, Presentation at useR 2007</em>
<sub>(<em>#skewness,models</em>)</sub></p>
</li>
<li>
<p>All data are wrong, but some are useful.</p>
<p>– <em>Jim Kloet (after George Box), RStudio::Conf 2022</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>A lot of data science and analytics is just counting things and
labeling them.</p>
<p>– <em>Hamdan Azhar, 2022 New York R Conference</em>
<sub>(<em>#science,counts,data analysis</em>)</sub></p>
</li>
<li>
<p>An observation is judged significant, if it would rarely have
been produced, in the absence of a real cause of the kind we are
seeking. It is a common practice to judge a result significant, if it is
of such a magnitude that it would have been produced by chance not more
frequently than once in twenty trials. This is an arbitrary, but
convenient, level of significance for the practical investigator, but it
does not mean that he allows himself to be deceived once in every twenty
experiments. The test of significance only tells him what to ignore,
namely all experiments in which significant results are not obtained. He
should only claim that a phenomenon is experimentally demonstrable when
he knows how to design an experiment so that it will rarely fail to give
a significant result. Consequently, isolated significant results which
he does not know how to reproduce are left in suspense pending further
investigation.</p>
<p>– <em>Ronald Fisher, The Statistical Method in Psychical Research,
Proceedings of the Society for Psychical Research, 39: 189-192
(1929).</em> <sub>(<em>#significance</em>)</sub></p>
</li>
<li>
<p>The statistician has no magic touch by which he may come in at
the stage of tabulation and make something of nothing. Neither will his
advice, however wise in the early stages of a study, ensure successful
execution and conclusion. Many a study, launched on the ways of elegant
statistical design, later boggled in execution, ends up with results to
which the theory of probability can contribute little.</p>
<p>– <em>W. Edwards Deming, Principles of Professional Statistical
Practice. Annals of Mathematical Statistics, 36(6), 1883. (1965)</em>
<sub>(<em>#statistician</em>)</sub></p>
</li>
<li>
<p>Evaluation of the statistical reliability of a set of results is
not mere calculation of standard errors and confidence limits. The
statistician must go far beyond the statistical methods in textbooks. He
must evaluate uncertainty in terms of possible uses of the data. Some of
this writing is not statistical but draws on assistance from the expert
in the subject-matter.</p>
<p>– <em>W. Edwards Deming, Principles of Professional Statistical
Practice. Annals of Mathematical Statistics, 36(6), 1883. (1965)</em>
<sub>(<em>#data,statistician</em>)</sub></p>
</li>
<li>
<p>An inference, if it is to have scientific value, must constitute
a prediction concerning future data. If the inference is to be made
purely with the help of the distribution theory of statistics, the
experiments that constitute evidence for the inference must arise from a
state of statistical control; until that state is reached, there is no
universe, normal or otherwise, and the statistician’s calculations by
themselves are an illusion if not a delusion.</p>
<p>– <em>W. Edwards Deming, Statistical Method from the Viewpoint of
Quality Control, 1939.</em>
<sub>(<em>#statistics,science</em>)</sub></p>
</li>
<li>
<p>Data visualization is part art and part science. The challenge is
to get the art right without getting the science wrong and vice
versa.</p>
<p>– <em>Claus O. Wilke, Fundamentals of Data Visualization, p.1</em>
<sub>(<em>#data visualization,science</em>)</sub></p>
</li>
<li>
<p>In other words, the model is terrific in all ways other than the
fact that it is totally useless. So why did we create it? In short,
because we could: we have a data set, and a statistical package, and add
the former to the latter, hit a few buttons and voila, we have another
paper.</p>
<p>– <em>Andew J. Vickers &amp; Angel M. Cronin, Everything you always
wanted to know about evaluating prediction models (but were too afraid
to ask). Urology. 2010;76(6):1298-1301.</em>
<sub>(<em>#models</em>)</sub></p>
</li>
<li>
<p>The definition of a medical statistician is one who will not
accept that Columbus discovered America because he said he was looking
for India in the trial plan.</p>
<p>– <em>Stephen J. Senn, Power is indeed irrelevant in interpreting
completed studies. BMJ. 2002;325(7375):1304.</em>
<sub>(<em>#statistician</em>)</sub></p>
</li>
<li>
<p>Inept graphics also flourish because many graphic artists believe
that statistics are boring and tedious. It then follows that decorated
graphics must pep up, animate, and all too often exaggerate what
evidence there is in the data. … If the statistics are boring, then
you’ve got the wrong numbers.</p>
<p>– <em>Edward R Tufte, The Visual Display of Quantitative Information,
1983.</em> <sub>(<em>#data visualization,statistics</em>)</sub></p>
</li>
<li>
<p>Excellence in statistical graphics consists of complex ideas
communicated with clarity, precision, and efficiency. Graphical displays
should show the data, induce the viewer to think about the substance
rather that about the methodology, graphic design, the technology of
graphic production, or something else, avoid distorting what the data
have to say, present many numbers in a small space make large data sets
coherent, encourage the eye to compare different pieces of data, reveal
the data at several levels of detail, from a broad overview to the fine
structure, serve a reasonable clear purpose: description, exploration,
tabulation, or decoration [should] be closely integrated with the
statistical and verbal descriptions of a data set.</p>
<p>– <em>Edward R Tufte, The Visual Display of Quantitative Information,
1983</em> <sub>(<em>#data visualization</em>)</sub></p>
</li>
<li>
<p>If you can’t have an experiment, do the best you can with
whatever data you can gather, but do be very skeptical of historical
data and subject them to all the logical tests you can think of.</p>
<p>– <em>Robert Hooke, Statistics, Sports, and Some Other Things. In:
Statistics: A Guide to the Unknown, Judith M. Tanur</em></p>
</li>
<li>
<p>The purely random sample is the only kind that can be examined
with entire confidence by means of statistical theory, but there is one
thing wrong with it. It is so difficult and expensive to obtain for many
uses that sheer cost eliminates it.</p>
<p>– <em>Darell Huff, How to Lie with Statistics, 1954.</em>
<sub>(<em>#sampling</em>)</sub></p>
</li>
<li>
<p>Probability is the most important concept in modern science,
especially as nobody has the slightest notion what it means.</p>
<p>– <em>Bertrand Russell, 1929 Lecture (cited in Bell 1945, The
Development of Mathematics, p. 587)</em>
<sub>(<em>#probability</em>)</sub></p>
</li>
<li>
<p>It is now proved beyond doubt that smoking is one of the leading
causes of statistics.</p>
<p>– <em>Fletcher Knebel, 1961</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Statistics show that of those who contract the habit of eating,
very few survive.</p>
<p>– <em>William W Irwin</em></p>
</li>
<li>
<p>We are hardwired to make sense of the world around us - to notice
patterns and invent theories to explain these patterns. We underestimate
how easily patterns can be created by inexplicable random events - by
good luck and bad luck.</p>
<p>– <em>Gary Smith, Standard Deviations, 2014</em></p>
</li>
<li>
<p>A very different - and very incorrect - argument is that
successes must be balanced by failures (and failures by successes) so
that things average out. Every coin flip that lands heads makes tails
more likely. Every red at roulette makes black more likely. … These
beliefs are all incorrect. Good luck will certainly not continue
indefinitely, but do not assume that good luck makes bad luck more
likely, or vice versa.</p>
<p>– <em>Gary Smith, Standard Deviations, 2014</em>
<sub>(<em>#probability</em>)</sub></p>
</li>
<li>
<p>Remember that even random coin flips can yield striking, even
stunning, patterns that mean nothing at all. When someone shows you a
pattern, no matter how impressive the person’s credentials, consider the
possibility that the pattern is just a coincidence. Ask why, not what.
No matter what the pattern, the question is: Why should we expect to
find this pattern?</p>
<p>– <em>Gary Smith, Standard Deviations, 2014</em>
<sub>(<em>#probability</em>)</sub></p>
</li>
<li>
<p>We are seduced by patterns and we want explanations for these
patterns. When we see a string of successes, we think that a hot hand
has made success more likely. If we see a string of failures, we think a
cold hand has made failure more likely. It is easy to dismiss such
theories when they involve coin flips, but it is not so easy with
humans. We surely have emotions and ailments that can cause our
abilities to go up and down. The question is whether these fluctuations
are important or trivial.</p>
<p>– <em>Gary Smith, Standard Deviations, 2014.</em></p>
</li>
<li>
<p>[In statistics] you have the fact that the concepts are not very
clean. The idea of probability, of randomness, is not a clean
mathematical idea. You cannot produce random numbers mathematically.
They can only be produced by things like tossing dice or spinning a
roulette wheel. With a formula, any formula, the number you get would be
predictable and therefore not random. So as a statistician you have to
rely on some conception of a world where things happen in some way at
random, a conception which mathematicians don’t have.</p>
<p>– <em>Lucien LeCam, Interview, 1988</em>
<sub>(<em>#probability</em>)</sub></p>
</li>
<li>
<p>Flip a coin 100 times. Assume that 99 heads are obtained. If you
ask a statistician, the response is likely to be: ‘It is a biased coin’.
But if you ask a probabilist, he may say: ‘Wooow, what a rare
event’.</p>
<p>– <em>Chamont Wang, Sense and Nonsense of Statistical Inference,
1993</em> <sub>(<em>#probability</em>)</sub></p>
</li>
<li>
<p>It is seen that continued shuffling may reasonably be expected to
produce perfect ‘randomness’ and to eliminate all traces of the original
order. It should be noted, however, that the number of operations
required for this purpose is extremely large.</p>
<p>– <em>William Feller, An Introduction To Probability Theory And Its
Applications, 1950</em></p>
</li>
<li>
<p>Figures may not lie, but statistics compiled unscientifically and
analyzed incompetently are almost sure to be misleading, and when this
condition is unnecessarily chronic the so-called statisticians may be
called liars.</p>
<p>– <em>Edwin B Wilson, Bulletin of the American Mathematical Society,
Vol 18, 1912</em> <sub>(<em>#statisticians</em>)</sub></p>
</li>
<li>
<p>The statistician’s job is to draw general conclusions from
fragmentary data. Too often the data supplied to him for analysis are
not only fragmentary but positively incoherent, so that he can do next
to nothing with them. Even the most kindly statistician swears heartily
under his breath whenever this happens.</p>
<p>– <em>M J Moroney, Facts from Figures, 1927</em>
<sub>(<em>#statisticians</em>)</sub></p>
</li>
<li>
<p>Just as by ‘literacy’, in this context, we mean much more than
its dictionary sense of the ability to read and write, so by ‘numeracy’
we mean more than mere ability to manipulate the rule of three. When we
say that a scientist is ‘illiterate’, we mean that he is not well enough
read to be able to communicate effectively with those who have had a
literary education. When we say that a historian or a linguist is
‘innumerate’ we mean that he cannot even begin to understand what
scientists and mathematicians are talking about.</p>
<p>– <em>Sir Geoffrey Crowther, A Report of the Central Advisory
Committee for Education, 1959, p. 270.</em>
<sub>(<em>#numeracy</em>)</sub></p>
</li>
<li>
<p>Numeracy has come to be an indispensable tool to the
understanding and mastery of all phenomena, and not only of those in the
relatively close field of the traditional natural sciences.</p>
<p>– <em>Sir Geoffrey Crowther, A Report of the Central Advisory
Committee for Education, 1959, p. 271.</em>
<sub>(<em>#numeracy</em>)</sub></p>
</li>
<li>
<p>Numeracy has two facets–reading and writing, or extracting
numerical information and presenting it. The skills of data presentation
may at first seem ad hoc and judgmental, a matter of style rather than
of technology, but certain aspects can be formalized into explicit
rules, the equivalent of elementary syntax.</p>
<p>– <em>Andrew Ehrenberg, Rudiments of Numeracy, Journal of Royal
Statistical Society, 140, 277-297, 1977.</em>
<sub>(<em>#numeracy</em>)</sub></p>
</li>
<li>
<p>People often feel inept when faced with numerical data. Many of
us think that we lack numeracy, the ability to cope with numbers. … The
fault is not in ourselves, but in our data. Most data are badly
presented and so the cure lies with the producers of the data. To draw
an analogy with literacy, we do not need to learn to read better, but
writers need to be taught to write better.</p>
<p>– <em>Andrew Ehrenberg, The problem of numeracy, American
Statistician 35, 67-71, 1981.</em>
<sub>(<em>#numeracy</em>)</sub></p>
</li>
<li>
<p>To be numerate means to be competent, confident, and comfortable
with one’s judgements on whether to use mathematics in a particular
situation and if so, what mathematics to use, how to do it, what degree
of accuracy is appropriate, and what the answer means in relation to the
context.</p>
<p>– <em>Diana Coben, Numeracy, mathematics and adult learning,
2000</em> <sub>(<em>#numeracy</em>)</sub></p>
</li>
<li>
<p>Numeracy is the ability to process, interpret and communicate
numerical, quantitative, spatial, statistical, even mathematical
information, in ways that are appropriate for a variety of contexts, and
that will enable a typical member of the culture or subculture to
participate effectively in activities that they value.</p>
<p>– <em>Jeff Evans, Adults’ Mathematical Thinking and Emotion,
2000</em> <sub>(<em>#numeracy</em>)</sub></p>
</li>
<li>
<p>Statistics is the art of stating in precise terms that which one
does not know.</p>
<p>– <em>William Kruskal, Statistics, Moliere, and Henry Adams, American
Scientist, 55, 416-428, 1967.</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>If significance tests are required for still larger samples,
graphical accuracy is insufficient, and arithmetical methods are
advised. A word to the wise is in order here, however. Almost never does
it make sense to use exact binomial significance tests on such data -
for the inevitable small deviations from the mathematical model of
independence and constant split have piled up to such an extent that the
binomial variability is deeply buried and unnoticeable. Graphical
treatment of such large samples may still be worthwhile because it
brings the results more vividly to the eye.</p>
<p>– <em>Frederick Mosteller &amp; John W Tukey, The Uses and Usefulness
of Binomial Probability Paper, Journal of the American Statistical
Association 44, 1949.</em> <sub>(<em>#significance,data
visualization</em>)</sub></p>
</li>
<li>
<p>Sequences of random numbers also inevitably display certain
regularities. … The trouble is, just as no real die, coin, or roulette
wheel is ever likely to be perfectly fair, no numerical recipe produces
truly random numbers. The mere existence of a formula suggests some sort
of predictability or pattern.</p>
<p>– <em>Ivars Peterson, The Jungles of Randomness: A Mathematical
Safari, 1998.</em> <sub>(<em>#random numbers</em>)</sub></p>
</li>
<li>
<p>It is very easy to devise different tests which, on the average,
have similar properties, … they behave satisfactorily when the null
hypothesis is true and have approximately the same power of detecting
departures from that hypothesis. Two such tests may, however, give very
different results when applied to a given set of data. The situation
leads to a good deal of contention amongst statisticians and much
discredit of the science of statistics. The appalling position can
easily arise in which one can get any answer one wants if only one goes
around to a large enough number of statisticians.</p>
<p>– <em>Frances Yates, Discussion on the Paper by Dr. Box and
Dr. Andersen, Journal of the Royal Statistical Society B Vol. 17,
1955</em></p>
</li>
<li>
<p>Beware of the problem of testing too many hypotheses; the more
you torture the data, the more likely they are to confess, but
confessions obtained under duress may not be admissible in the court of
scientific opinion.</p>
<p>– <em>Stephen M Stigler, Neutral Models in Biology, 1987,
p. 148.</em> <sub>(<em>#significance</em>)</sub></p>
</li>
<li>
<p>Statistics may be regarded as (i) the study of populations, (ii)
as the study of variation, and (iii) as the study of methods of the
reduction of data.</p>
<p>– <em>Sir Ronald A Fisher, Statistical Methods for Research Workers,
1925</em> <sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>The primes have tantalized mathematicians since the Greeks,
because they appear to be somewhat randomly distributed but not
completely so. … Although the prime numbers are rigidly determined, they
somehow feel like experimental data.</p>
<p>– <em>Timothy Gowers, Mathematics: A Very Short Introduction,
2002</em> <sub>(<em>#random numbers</em>)</sub></p>
</li>
<li>
<p>Frequentist statistics assumes that there is a ‘true’ state of
the world (e.g. the difference between species in predation probability)
which gives rise to a distribution of possible experimental outcomes.
The Bayesian framework says instead that the experimental outcome - what
we actually saw happen - is the truth, while the parameter values or
hypotheses have probability distributions. The Bayesian framework solves
many of the conceptual problems of frequentist statistics: answers
depend on what we actually saw and not on a range of hypothetical
outcomes, and we can legitimately make statements about the probability
of different hypotheses or parameter values.</p>
<p>– <em>Ben Bolker, Ecological Models and Data in R, 2007</em>
<sub>(<em>#bayesian</em>)</sub></p>
</li>
<li>
<p>A statistical estimate may be good or bad, accurate or the
reverse; but in almost all cases it is likely to be more accurate than a
casual observer’s impression, and the nature of things can only be
disproved by statistical methods.</p>
<p>– <em>Sir Arthur L Bowley, Elements of Statistics, 1901</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>An extremely odd demand is often set forth but never met, even by
those who make it; i.e., that empirical data should be presented without
any theoretical context, leaving the reader, the student, to his own
devices in judging it. This demand seems odd because it is useless
simply to look at something. Every act of looking turns into
observation, every act of observation into reflection, every act of
reflection into the making of associations; thus it is evident that we
theorize every time we look carefully at the world.</p>
<p>– <em>Johann Wolfgang von Goethe</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>From the moment we first roll a die in a children’s board game,
or pick a card (any card), we start to learn what probability is. But
even as adults, it is not easy to tell what it is, in the general
way.</p>
<p>– <em>David Stirzaker, Probability and Random Variables: A Beginner’s
Guide, 1999</em> <sub>(<em>#probability</em>)</sub></p>
</li>
<li>
<p>We cannot really have a perfectly shuffled pack of perfect cards;
this ‘collection of equally likely hands’ is actually a fiction. We
create the idea, and then use the rules of arithmetic to calculate the
required chances. This is characteristic of all mathematics, which
concerns itself only with rules defining the behaviour of entities which
are themselves undefined (such as ‘numbers’ or ‘points’).</p>
<p>– <em>David Stirzaker, Probability and Random Variables: A Beginner’s
Guide, 1999</em></p>
</li>
<li>
<p>The whole point of probability is to discuss uncertain
eventualities before they occur. After this event, things are completely
different.</p>
<p>– <em>David Stirzaker, Probability and Random Variables: A Beginner’s
Guide, 1999</em> <sub>(<em>#probability</em>)</sub></p>
</li>
<li>
<p>There is no such thing as randomness. No one who could detect
every force operating on a pair of dice would ever play dice games,
because there would never be any doubt about the outcome. The
randomness, such as it is, applies to our ignorance of the possible
outcomes. It doesn’t apply to the outcomes themselves. They are 100%
determined and are not random in the slightest. Scientists have become
so confused by this that they now imagine that things really do happen
randomly, i.e. for no reason at all.</p>
<p>– <em>Thomas Stark, God Is Mathematics: The Proofs of the Eternal
Existence of Mathematics, 2018</em></p>
</li>
<li>
<p>Why is the human need to be in control relevant to a discussion
of random patterns? Because if events are random, we are not in control,
and if we are in control of events, they are not random. There is
therefore a fundamental clash between our need to feel we are in control
and our ability to recognize randomness. That clash is one of the
principal reasons we misinterpret random events.</p>
<p>– <em>Leonard Mlodinow, The Drunkard’s Walk: How Randomness Rules Our
Lives, 2008</em></p>
</li>
<li>
<p>An experiment is a failure only when it also fails adequately to
test the hypothesis in question, when the data it produces don’t prove
anything one way or the other.</p>
<p>– <em>Robert M Pirsig, Zen and the Art of Motorcycle Maintenance,
1974</em></p>
</li>
<li>
<p>A hypothesis is empirical or scientific only if it can be tested
by experience. […] A hypothesis or theory which cannot be, at least in
principle, falsified by empirical observations and experiments does not
belong to the realm of science.</p>
<p>– <em>Francisco J Ayala, Biological Evolution: Natural Selection or
Random Walk, American Scientist, 1974</em></p>
</li>
<li>
<p>…no one believes an hypothesis except its originator but everyone
believes an experiment except the experimenter.</p>
<p>– <em>William I B Beveridge, The Art of Scientific Investigation,
1950</em></p>
</li>
<li>
<p>The hypothesis is the principal intellectual instrument in
research. Its function is to indicate new experiments and observations
and it therefore sometimes leads to discoveries even when not correct
itself. We must resist the temptation to become too attached to our
hypothesis, and strive to judge it objectively and modify it or discard
it as soon as contrary evidence is brought to light. Vigilance is needed
to prevent our observations and interpretations being biased in favor of
the hypothesis. Suppositions can be used without being believed.</p>
<p>– <em>William I B Beveridge, The Art of Scientific Investigation,
1950</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Experiments are like cross-questioning a witness who will tell
the truth but not the whole truth.</p>
<p>– <em>Alan Gregg, The Furtherance of Medical Research,
1941</em></p>
</li>
<li>
<p>A random sequence is a vague notion embodying the idea of a
sequence in which each term is unpredictable to the uninitiated and
whose digits pass a certain number of tests traditional with
statisticians and depending somewhat on the uses to which the sequence
is to be put.</p>
<p>– <em>Derrick H Lehmer, 1951</em> <sub>(<em>#random
numbers</em>)</sub></p>
</li>
<li>
<p>The moment you forecast you know you’re going to be wrong, you
just don’t know when and in which direction.</p>
<p>– <em>Edgar R Fiedler, “Across the Board”, 1977</em> <sub>(<em>#time
series</em>)</sub></p>
</li>
<li>
<p>Statistics at its best provides methodology for dealing
empirically with complicated and uncertain information, in a way that is
both useful and scientifically valid.</p>
<p>– <em>John M Chambers, 1993</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>…it does not seem helpful just to say that all models are wrong.
The very word model implies simplification and idealization. The idea
that complex physical, biological or sociological systems can be exactly
described by a few formulae is patently absurd. The construction of
idealized representations that capture important stable aspects of such
systems is, however, a vital part of general scientific analysis and
statistical models, especially substantive ones, do not seem essentially
different from other kinds of model.</p>
<p>– <em>Sir David Cox, Comment on ‘Model uncertainty, data mining and
statistical inference’, Journal of the Royal Statistical Society, Series
A 158, 1995.</em> <sub>(<em>#models,uncertainty</em>)</sub></p>
</li>
<li>
<p>The science of statistics may be described as exploring,
analyzing and summarizing data; designing or choosing appropriate ways
of collecting data and extracting information from them; and
communicating that information. Statistics also involves constructing
and testing models for describing chance phenomena. These models can be
used as a basis for making inferences and drawing conclusions and,
finally, perhaps for making decisions.</p>
<p>– <em>Fergus Daly et al, Elements of Statistics, 1995</em>
<sub>(<em>#statistics,knowledge</em>)</sub></p>
</li>
<li>
<p>If a man stands with his left foot on a hot stove and his right
foot in a refrigerator, the statistician would say that, on the average,
he’s comfortable.</p>
<p>– <em>Walter Heller</em> <sub>(<em>#statistician</em>)</sub></p>
</li>
<li>
<p>Things are changing. Statisticians now recognize that computer
scientists are making novel contributions while computer scientists now
recognize the generality of statistical theory and methodology. Clever
data mining algorithms are more scalable than statisticians ever thought
possible. Formal statistical theory is more pervasive than computer
scientists had realized.</p>
<p>– <em>Larry A Wasserman, All of Statistics: A concise course in
statistical inference, 2004</em>
<sub>(<em>#statistician</em>)</sub></p>
</li>
<li>
<p>One feature […] which requires much more justification than is
usually given, is the setting up of unplausible null hypotheses. For
example, a statistician may set out a test to see whether two drugs have
exactly the same effect, or whether a regression line is exactly
straight. These hypotheses can scarcely be taken literally.</p>
<p>– <em>Cedric A B Smith, Book review of Norman T. J. Bailey:
Statistical Methods in Biology, Applied Statistics 9, 1960</em>
<sub>(<em>#statistician</em>)</sub></p>
</li>
<li>
<p>In general, it is necessary to have some data on which to
calculate probabilities. […] Statisticians do not evolve probabilities
out of their inner consciousness, they merely calculate them.</p>
<p>– <em>Leonard C Tippett</em>
<sub>(<em>#data,probability</em>)</sub></p>
</li>
<li>
<p>Even properly done statistics can’t be trusted. The plethora of
available statistical techniques and analyses grants researchers an
enormous amount of freedom when analyzing their data, and it is
trivially easy to ‘torture the data until it confesses’.</p>
<p>– <em>Alex Reinhart, Statistics Done Wrong: The Woefully Complete
Guide, 2015</em></p>
</li>
<li>
<p>Using data from the population as it stands is a dangerous
substitute for testing.</p>
<p>– <em>Frederick Mosteller &amp; Gale Mosteller, “New Statistical
Methods in Public Policy. Part I: Experimentation”, Journal of
Contemporary Business 8, 1979</em></p>
</li>
<li>
<p>The closer that sample-selection procedures approach the gold
standard of random selection - for which the definition is that every
individual in the population has an equal chance of appearing in the
sample - the more we should trust them. If we don’t know whether a
sample is random, any statistical measure we conduct may be biased in
some unknown way.</p>
<p>– <em>Richard E Nisbett, “Mindware: Tools for Smart Thinking”,
2015</em></p>
</li>
<li>
<p>A popular misconception holds that the era of Big Data means the
end of a need for sampling. In fact, the proliferation of data of
varying quality and relevance reinforces the need for sampling as a tool
to work efficiently with a variety of data, and minimize bias. Even in a
Big Data project, predictive models are typically developed and piloted
with samples.</p>
<p>– <em>Peter C Bruce &amp; Andrew G Bruce, “Statistics for Data
Scientists: 50 Essential Concepts”, 2016</em>
<sub>(<em>#sampling,data</em>)</sub></p>
</li>
<li>
<p>All predictions are statistical, but some predictions have such a
high probability that one tends to regard them as certain.</p>
<p>– <em>Marshall J Walker, The Nature of Scientific Thought, 1963</em>
<sub>(<em>#uncertainty</em>)</sub></p>
</li>
<li>
<p>Statistics is a scientific discipline concerned with collection,
analysis, and interpretation of data obtained from observation or
experiment. The subject has a coherent structure based on the theory of
Probability and includes many different procedures which contribute to
research and development throughout the whole of Science and
Technology.</p>
<p>– <em>Egon Pearson, 1936</em>
<sub>(<em>#statistics,probability,science</em>)</sub></p>
</li>
<li>
<p>[Statistics] is both a science and an art. It is a science in
that its methods are basically systematic and have general application;
and an art in that their successful application depends to a
considerable degree on the skill and special experience of the
statistician, and on his knowledge of the field of application,
e.g. economics.</p>
<p>– <em>Leonard H C Tippett, Statistics, 1943</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>The fact must be expressed as data, but there is a problem in
that the correct data is difficult to catch. So that I always say ‘When
you see the data, doubt it!’ ‘When you see the measurement instrument,
doubt it!’ […]For example, if the methods such as sampling, measurement,
testing and chemical analysis methods were incorrect, data. […] to
measure true characteristics and in an unavoidable case, using
statistical sensory test and express them as data.</p>
<p>– <em>Kaoru Ishikawa, Annual Quality Congress Transactions, 1981</em>
<sub>(<em>#data,sampling</em>)</sub></p>
</li>
<li>
<p>There is a tendency to mistake data for wisdom, just as there has
always been a tendency to confuse logic with values, intelligence with
insight. Unobstructed access to facts can produce unlimited good only if
it is matched by the desire and ability to find out what they mean and
where they lead.</p>
<p>– <em>Norman Cousins, “Human Options : An Autobiographical Notebook”,
1981</em> <sub>(<em>#data,knowledge</em>)</sub></p>
</li>
<li>
<p>Data in isolation are meaningless, a collection of numbers. Only
in context of a theory do they assume significance…</p>
<p>– <em>George Greenstein, “Frozen Star”, 1983</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>Intuition becomes increasingly valuable in the new information
society precisely because there is so much data.</p>
<p>– <em>John Naisbitt, “Re-Inventing the Corporation”, 1985</em>
<sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>No matter what the laws of chance might tell us, we search for
patterns among random events wherever they might occur–not only in the
stock market but even in interpreting sporting phenomena.</p>
<p>– <em>Burton G. Malkiel, “A Random Walk Down Wall Street: The
Time-Tested Strategy For Successful Investing”, 2011, p. 149.</em>
<sub>(<em>#random numbers</em>)</sub></p>
</li>
<li>
<p>One can be highly functionally numerate without being a
mathematician or a quantitative analyst. It is not the mathematical
manipulation of numbers (or symbols representing numbers) that is
central to the notion of numeracy. Rather, it is the ability to draw
correct meaning from a logical argument couched in numbers. When such a
logical argument relates to events in our uncertain real world, the
element of uncertainty makes it, in fact, a statistical argument.</p>
<p>– <em>Eric R Sowey, The Getting of Wisdom: Educating Statisticians to
Enhance Their Clients’ Numeracy, The American Statistician 57(2),
2003</em> <sub>(<em>#numeracy,uncertainty</em>)</sub></p>
</li>
<li>
<p>We would wish ‘numerate’ to imply the possession of two
attributes. The first of these is an ‘at-homeness’ with numbers and an
ability to make use of mathematical skills which enable an individual to
cope with the practical mathematical demands of his everyday life. The
second is ability to have some appreciation and understanding of
information which is presented in mathematical terms, for instance in
graphs, charts or tables or by reference to percentage increase or
decrease.</p>
<p>– <em>Cockcroft Committee, Mathematics Counts: A Report into the
Teaching of Mathematics in Schools, 1982</em> <sub>(<em>#numeracy,data
visualization</em>)</sub></p>
</li>
<li>
<p>In all scientific fields, theory is frequently more important
than experimental data. Scientists are generally reluctant to accept the
existence of a phenomenon when they do not know how to explain it. On
the other hand, they will often accept a theory that is especially
plausible before there exists any data to support it.</p>
<p>– <em>Richard Morris, 1983</em>
<sub>(<em>#science,data</em>)</sub></p>
</li>
<li>
<p>To find out what happens to a system when you interfere with it
you have to interfere with it (not just passively observe it).</p>
<p>– <em>George E P Box, “Use and Abuse of Regression”, 1966</em>
<sub>(<em>#Box quotes</em>)</sub></p>
</li>
<li>
<p>Since all models are wrong the scientist cannot obtain a
‘correct’ one by excessive elaboration. On the contrary following
William of Occam he should seek an economical description of natural
phenomena. Just as the ability to devise simple but evocative models is
the signature of the great scientist so overelaboration and
overparameterization is often the mark of mediocrity.</p>
<p>– <em>George E P Box, Science and Statistics, Journal of the American
Statistical Association 71, 1976</em> <sub>(<em>#Box
quotes,models</em>)</sub></p>
</li>
<li>
<p>Since all models are wrong the scientist must be alert to what is
importantly wrong. It is inappropriate to be concerned about mice when
there are tigers abroad.</p>
<p>– <em>George E P Box, Science and Statistics, Journal of the American
Statistical Association 71, 1976</em> <sub>(<em>#models,science,Box
quotes</em>)</sub></p>
</li>
<li>
<p>The fact that [the model] is an approximation does not
necessarily detract from its usefulness because models are
approximations. All models are wrong, but some are useful.</p>
<p>– <em>George E P Box, 1987</em> <sub>(<em>#models,science,Box
quotes</em>)</sub></p>
</li>
<li>
<p>The central limit theorem says that, under conditions almost
always satisfied in the real world of experimentation, the distribution
of such a linear function of errors will tend to normality as the number
of its components becomes large. The tendency to normality occurs almost
regardless of the individual distributions of the component errors. An
important proviso is that several sources of error must make important
contributions to the overall error and that no particular source of
error dominate the rest.</p>
<p>– <em>George E P Box et al, “Statistics for Experimenters: Design,
discovery, and innovation” 2nd Ed., 2005</em>
<sub>(<em>#normality</em>)</sub></p>
</li>
<li>
<p>The postulate of randomness thus resolves itself into the
question, ‘of what population is this a random sample?’ which must
frequently be asked by every practical statistician.</p>
<p>– <em>Ronald Fisher, “On the Mathematical Foundation of Theoretical
Statistics”, Philosophical Transactions of the Royal Society of London
Vol. A222, 1922</em> <sub>(<em>#random
numbers,sampling,statistics</em>)</sub></p>
</li>
<li>
<p>Statistics has been likened to a telescope. The latter enables
one to see further and to make clear objects which were diminished or
obscured by distance. The former enables one to discern structure and
relationships which were distorted by other factors or obscured by
random variation.</p>
<p>– <em>David J Hand, “The Role of Statistics in Psychiatry”,
Psychological Medicine Vol. 15, 1985</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>When looking at the end result of any statistical analysis, one
must be very cautious not to over interpret the data. Care must be taken
to know the size of the sample, and to be certain the method for
gathering information is consistent with other samples gathered. […] No
one should ever base conclusions without knowing the size of the sample
and how random a sample it was. But all too often such data is not
mentioned when the statistics are given - perhaps it is overlooked or
even intentionally omitted.</p>
<p>– <em>Theoni Pappas, “More Joy of Mathematics: Exploring mathematical
insights &amp; concepts”, 1994</em>
<sub>(<em>#sampling</em>)</sub></p>
</li>
<li>
<p>BREAKING: The Supreme Court just ruled 6-3 that according to the
US constitution logistic regression IS machine learning.</p>
<p>– <em>Kareem Carr, <span class="citation">@Kareem_Carr</span>,
Twitter 7/1/21</em> <sub>(<em>#models</em>)</sub></p>
</li>
<li>
<p>In questions of science the authority of a thousand is not worth
the humble reasoning of a single individual</p>
<p>– <em>Galileo Galilei, 1632, Dialog concerning the Two Chief World
Systems</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Science is organized knowledge. Wisdom is organized life.</p>
<p>– <em>Immanuel Kant</em>
<sub>(<em>#science,knowledge</em>)</sub></p>
</li>
<li>
<p>When you steal from one author, it’s plagiarism; if you steal
from many, it’s research.</p>
<p>– <em>Wilson Mizner (American playwright and entrepreneur)</em>
<sub>(<em>#research,ethics</em>)</sub></p>
</li>
<li>
<p>You can’t always get what you want, but if you try, sometimes,
well you just might find, you get what you need.</p>
<p>– <em>Mick Jagger</em> <sub>(<em>#data,science</em>)</sub></p>
</li>
<li>
<p>He who gives up code safety for code speed deserves neither.</p>
<p>– <em>Hadley Wickham</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>Any fool can write code that a computer can understand. Good
programmers write code that humans can understand.</p>
<p>– <em>Martin Fowler, Refactoring: Improving the Design of Existing
Code</em> <sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>Thank you for sending me a copy of your book. I’ll waste no time
reading it.</p>
<p>– <em>Moses Hadas</em> <sub>(<em>#reviews</em>)</sub></p>
</li>
<li>
<p>I will let the data speak for itself when it cleans itself.</p>
<p>– <em>Allison Reichel</em> <sub>(<em>#data</em>)</sub></p>
</li>
<li>
<p>To the untrained eye, randomness appears as regularity or
tendency to cluster.</p>
<p>– <em>W. Feller, An Introduction to Probability Theory and its
Applications (1950)</em> <sub>(<em>#probability,data
visualization</em>)</sub></p>
</li>
<li>
<p>Your assumptions are your windows on the world. Scrub them off
every once in a while, or the light won’t come in.</p>
<p>– <em>Alan Alda, 1936</em>
<sub>(<em>#assumptions</em>)</sub></p>
</li>
<li>
<p>Little experience is sufficient to show that the traditional
machinery of statistical processes is wholly unsuited to the needs of
practical research. Not only does it take a cannon to shoot a sparrow,
but it misses the sparrow! The elaborate mechanism built on the theory
of infinitely large samples is not accurate enough for simple laboratory
data. Only by systematically tackling small sample problems on their
metrics does it seem possible to apply accurate tests to practical
data.</p>
<p>– <em>Ronald Fisher, Statistical Methods for Research Workers
(1925)</em> <sub>(<em>#sample size</em>)</sub></p>
</li>
<li>
<p>All generative models are wrong, but some are useful.</p>
<p>– <em>Jared Lander, Copilot for R, 56:25</em>
<sub>(<em>#models</em>)</sub></p>
</li>
<li>
<p>Everyone who has carried out experiments in the field or farmyard
must be well aware that the result of a single experiment is very often
entirely misleading. Yet it is still common practice to publish single
results and to base practical advice upon them.</p>
<p>– <em>T. B. Wood and F. J. M. Stratton, The interpretation of
experimental results. The Journal of Agricultural Science, 3,
417-440.</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>The preparation of clear and simple plans, and a convenient
system of numbering the [treatments] that are to be applied, will
lighten the work of the man in the field, who is usually operating under
averse conditions, is frequently in a hurry, and is sometimes not very
certain of the points at issue.</p>
<p>– <em>F. Yates, The Design and Analysis of Factorial Experiments
(1937). Harpenden Imperial Bureau of Soil Science.</em>
<sub>(<em>#biometry, expt design</em>)</sub></p>
</li>
<li>
<p>I do not always agree with Sir Ronald Fisher, but it is due to
him that the standard of presentation of results in agriculture is
better than in any of the so-called exact sciencees; and this is a state
of affairs that physicists should cease to tolerate.</p>
<p>– <em>Sir Harald Jeffreys, Half a Century in Geophysics: An original
article from the Report of the British Association for the Advancement
of Science, 1953.</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Data are not just numbers, they are numbers with a context. … In
data analysis, context provides meaning.</p>
<p>– <em>George W. Cobb &amp; David S. Moore, Mathematics, Statistics,
and Teaching. American Mathematical Monthly, 801-23.</em>
<sub>(<em>#numeracy</em>)</sub></p>
</li>
<li>
<p>When [the profession of] statistics becomes clearly embedded in
people’s minds as being concerned with <em>investigation</em> rather
than simply <em>calculation</em> (or worse still, mere cataloguing of
data), there can be no room for doubts about the relevance of the
subject.</p>
<p>– <em>C. J. Wild, Embracing the ‘Wider view’ of Statistics, The
American Statistician, 48, 163-171. p. 165.</em></p>
</li>
<li>
<p>We have to teach non-statisticians to recognize where statistical
expertise is required. No one else will. We teach students how to solve
simple statistical problems, but how often do we make any serious effort
to teach them to recognize situations that call for statistical
expertise that is beyond the technical content of the course.=?</p>
<p>– <em>C. J. Wild, Embracing the ‘Wider view’ of Statistics, The
American Statistician, 48, 163-171. p. 166.</em>
<sub>(<em>#teaching</em>)</sub></p>
</li>
<li>
<p>A careful and sophisticated analysis of the data is often quite
useless if the statistician cannot communicate the essential features of
the data to a client for whom statistics is an entirely foreign
language.</p>
<p>– <em>C. J. Wild, Embracing the ‘Wider view’ of Statistics, The
American Statistician, 48, 163-171. p. 170.</em></p>
</li>
<li>
<p>The dominant feature of our 129 barley trials was the large
differences…overy the five years of the study, even for the same field.
To give sound advice on choice of plot size and shape, statisticians
will require results from many trials conducted over many years, not to
mention crops.</p>
<p>– <em>Dorothy Robinson, Discussion of the paper by Dr Brewer and
Professor Mead, Journal of the Royal Statistical Society. Series A,
1986, 149, pp. 314-348. Quote on p. 341.</em></p>
</li>
<li>
<p>If you torture the data enough, nature will always confess.</p>
<p>– <em>Ronald Coase, quoted from Coase, R. H. (1982). How should
economists chose? American Enterprise Institute, Washington, D. C.</em>
<sub>(<em>#data analysis</em>)</sub></p>
</li>
<li>
<p>A big computer, a complex algorithm and a long time does not
equal science.</p>
<p>– <em>Robert Gentleman</em>
<sub>(<em>#science,computing</em>)</sub></p>
</li>
<li>
<p>Absence of evidence is not evidence of absence.</p>
<p>– <em>Martin Rees, Project Cyclops: A Design Study of a System for
Detecting Extraterrestrial Intelligent Life (1971) by Bernard M. Oliver,
and John Billingham</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Statistics - A subject which most statisticians find difficult
but which many physicians are experts on.</p>
<p>– <em>Stephen Senn, Senn, S. (2007). Statistical Issues in Drug
Development (2nd Edition). Chichester: John Wiley &amp; Sons</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>The statistician cannot evade the responsibility for
understanding the process he applies or recommends.</p>
<p>– <em>Ronald A Fisher, Fisher, R. A. (1971 [1935]). The Design of
Experiments (9th ed.). Macmillan.</em>
<sub>(<em>#statistics,science</em>)</sub></p>
</li>
<li>
<p>Taking a model too seriously is really just another way of not
taking it seriously at all.</p>
<p>– <em>Andrew Gelman</em> <sub>(<em>#models</em>)</sub></p>
</li>
<li>
<p>What the use of a p-value implies, therefore, is that a
hypothesis that may be true may be rejected because it has not predicted
observable results that have not occurred.</p>
<p>– <em>Harold Jeffreys, Jeffreys, H. (1939). Theory of Probability.
Oxford, England: Clarendon Press.</em>
<sub>(<em>#p-values</em>)</sub></p>
</li>
<li>
<p>Statistics are no substitution for judgment.</p>
<p>– <em>Henry Clay, Evening Sentinel (Staffordshire Sentinel), 1930
October 13, Production Prices and Depression: Professor Clay on the
Trade Outlook, Quote Page 5, Column 5, Staffordshire, England. (British
Newspaper Archive)</em></p>
</li>
<li>
<p>The most effective debugging tool is still careful thought,
coupled with judiciously placed print statements.</p>
<p>– <em>Brian Kernighan, Unix for Beginners (1979)</em>
<sub>(<em>#computing</em>)</sub></p>
</li>
<li>
<p>We live on an island of knowledge surrounded by a sea of
ignorance. As our island of knowledge grows, so does the shore of our
ignorance.</p>
<p>– <em>John A. Wheeler, Scientific American, 1992</em>
<sub>(<em>#science,knowledge</em>)</sub></p>
</li>
<li>
<p>It’s not just about visualizing information, but making
information visible.</p>
<p>– <em>Jose Duarte</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>I don’t know what the definition of “philosopher” is. But I know
for a fact that a “statistician” is someone who has written an R
package.</p>
<p>– <em>Richard McElrath, Twitter, 4-29-2023</em>
<sub>(<em>#computing,statistics</em>)</sub></p>
</li>
<li>
<p>In every project you have at least one other collaborator;
future-you. You don’t want future-you to curse past-you.</p>
<p>– <em>Solomon Kurz</em> <sub>(<em>#time</em>)</sub></p>
</li>
<li>
<p>The whole point of science is that most of it is uncertain.
That’s why science is exciting–because we don’t know. Science is all
about things we don’t understand. The public, of course, imagines
science is just a set of facts. But it’s not. Science is a process of
exploring, which is always partial. We explore, and we find out things
that we understand. We find out things we thought we understood were
wrong. That’s how it makes progress.</p>
<p>– <em>Freeman Dyson, mentioned in a 2014 interview</em>
<sub>(<em>#science,knowledge,uncertainty</em>)</sub></p>
</li>
<li>
<p>Mathematical Statistics is a rigorous way to solve the wrong
problem. Applied statistics is a subjective way to solve the right
problem.</p>
<p>– <em>Frank Harrell, Twitter, 10-10-2023</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>It is as scientific and intelligible to represent time by space,
as it is to represent space by space.</p>
<p>– <em>Emma Willard, Guide to the Temple of Time, 1850, p. 21</em>
<sub>(<em>#time</em>)</sub></p>
</li>
<li>
<p>Can’t duration be imitated and represented as significantly, as
distinctly as space?</p>
<p>– <em>Jacques Barbeu Dubourg, Carte chronographique, 1753</em>
<sub>(<em>#time</em>)</sub></p>
</li>
<li>
<p>Science has before it two obstacles which hinder its progress:
the deficiency of our senses to discover truths, and the insufficiencies
of language to express and transmit what truths we have acquired. The
Graphic Method this double goal better than any other.</p>
<p>– <em>Etienne-Jules Marey, La Methode Graphique, 1878</em>
<sub>(<em>#science,data visualization</em>)</sub></p>
</li>
<li>
<p>Let’s solve the problem but let’s not make it worse by
guessing.</p>
<p>– <em>Gene Kranz, Apollo 13 Lead Flight Director</em>
<sub>(<em>#science,prediction</em>)</sub></p>
</li>
<li>
<p>We should forget about small efficiencies, say about 97% of the
time: premature optimization is the root of all evil.</p>
<p>– <em>Donald Knuth</em> <sub>(<em>#programming</em>)</sub></p>
</li>
<li>
<p>There are only two kinds of languages: the ones people complain
about and the ones nobody uses.</p>
<p>– <em>Bjarne Stroustrup</em>
<sub>(<em>#programming</em>)</sub></p>
</li>
<li>
<p>Sometimes a simple model will out perform a more complex model …
Nevertheless, I believe that deliberately limiting the complexity of the
model is not fruitful when the problem is evidently complex. Instead, if
a simple model is found that out performs some particular complex model,
the appropriate response is to define a different complex model that
captures whatever aspect of the problem led to the simple model
performing well.</p>
<p>– <em>Radford M. Neal, Neal, R. M. (1996). Bayesian Learning for
Neural Networks. Springer, New York.</em> <sub>(<em>#modeling,data
analysis,Ockham’s razor</em>)</sub></p>
</li>
<li>
<p>What makes a statistician look like a hero? You might think that
the answer would be, Extracting a small faint signal from noise. But I
don’t think so. I think that a statistician looks like a hero by
studying large effects.</p>
<p>– <em>Andrew Gelman, Numbersense Pros: Interview with Andrew Gelman,
2013-09-12</em> <sub>(<em>#data analysis</em>)</sub></p>
</li>
<li>
<p>Data are not taken for museum purposes; they are taken as a basis
for doing something. If nothing is to be done with the data, then there
is no use in collecting any. The ultimate purpose of taking data is to
provide a basis for action or a recommendation for action. The step
intermediate between the collection of data and the action is
prediction.</p>
<p>– <em>W. Edwards Deming, On a Classification of the Problems of
Statistical Inference, June 1942, Journal of the American Statistical
Association, page 173.</em>
<sub>(<em>#data,prediction</em>)</sub></p>
</li>
<li>
<p>In real-life situations the interpretation of the results of
goodness-of-fit tests must rely on judgment of content rather than on
P-values.</p>
<p>– <em>Peter Huber, Data Analysis: What Can Be Learned from the Past
50 Years.</em> <sub>(<em>#nhst,normality</em>)</sub></p>
</li>
<li>
<p>Too much emphasis is put on futile attempts to automate
non-routine tasks, and not enough effort is spent on facilitating
routine work.</p>
<p>– <em>Peter Huber, Data Analysis: What Can Be Learned from the Past
50 Years.</em> <sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>The best thing to do with missing data is to not have any.</p>
<p>– <em>Gertrude Cox</em></p>
</li>
<li>
<p>The modern student, and too often his teacher, overlook the fact
that such a simple thing as a scatter diagram is a more important tool
of prediction than the correlation coefficient, especially if the points
are labeled so as to distinguish the different sources of the data.</p>
<p>– <em>W. Edwards Deming, 1940</em></p>
</li>
<li>
<p>Above all, a statistician must be a scientist.</p>
<p>– <em>W. Edwards Deming, 1940</em></p>
</li>
<li>
<p>The chaste beauty and intellectual delights of the theory of
statistical inference, regarded as the intellectual offspring of
mathematics and inductive logic, are known at present only to a few
devotees; but this theory is bound in time to receive a wider
appreciation and a higher valuation even apart from its practical
usefulness in the form of application</p>
<p>– <em>Harold Hotelling, Presidential address to Indian Statstical
Congress at Madras, 1941, Sankhya, 5, 127-129</em></p>
</li>
<li>
<p>Unfortunately, too many people like to do their statistical work
just as they say their prayers-merely substitute in a formula found in a
highly respected book written a long time ago.</p>
<p>– <em>Harold Hotelling, The Place of Statistics in the University,
Proceedings of the Berkley Symposium on Mathematical Statistics and
Probability, University of California Press, 1949, 21-40.</em></p>
</li>
<li>
<p>We have overwhelming evidence that available information plus
analysis does not lead to knowledge. The management science team can
properly analyse a situation and present recommendations to the manager,
but no change occurs. The situation is so familiar to those of us who
try to practice management science that I hardly need to describe the
cases.</p>
<p>– <em>C. West Churchman, Managerial acceptance of scientific
recommendations, California Management Review Vol 7, 1964.</em></p>
</li>
<li>
<p>Statistics is a body of methods and theory applied to numerical
evidence in making decisions in the face of uncertainty.</p>
<p>– <em>Lawrence Lapin, Statistics for Modern Business Decisions,
1973.</em></p>
</li>
<li>
<p>Like a detective, a data analyst will experience many dead ends,
retrace his steps, and explore many alternatives before settling on a
single description of the evidence in front of him.</p>
<p>– <em>David Lubinsky &amp; Daryl Pregibon, Data analysis as search,
Journal of Econometrics Vol. 38 (1–2), 1988</em></p>
</li>
<li>
<p>…the simplest hypothesis proposed as an explanation of phenomena
is more likely to be the true one than is any other available
hypothesis, that its predictions are more likely to be true than those
of any other available hypothesis, and that it is an ultimate a priori
epistemic principle that simplicity is evidence for truth.</p>
<p>– <em>Richard Swinburne, Simplicity as Evidence for Truth,
1997.</em></p>
</li>
<li>
<p>When significance tests are used and a null hypothesis is not
rejected, a major problem often arises - namely, the result may be
interpreted, without a logical basis, as providing evidence for the null
hypothesis.</p>
<p>– <em>David F Parkhurst, Statistical Significance Tests: Equivalence
and Reverse Tests Should Reduce Misinterpretation, BioScience Vol. 51
(12), 2001.</em></p>
</li>
<li>
<p>Data analysis is careful thinking about evidence.</p>
<p>– <em>Michael Milton, Head First Data Analysis, 2009.</em></p>
</li>
<li>
<p>Data clusters are everywhere, even in random data. Someone who
looks for an explanation will inevitably find one, but a theory that
fits a data cluster is not persuasive evidence. The found explanation
needs to make sense and it needs to be tested with uncontaminated
data.</p>
<p>– <em>Gary Smith, Standard Deviations, 2014.</em>
<sub>(<em>#clustering</em>)</sub></p>
</li>
<li>
<p>In general, when building statistical models, we must not forget
that the aim is to understand something about the real world. Or
predict, choose an action, make a decision, summarize evidence, and so
on, but always about the real world, not an abstract mathematical world:
our models are not the reality - a point well made by George Box in his
oft-cited remark that “all models are wrong, but some are useful.</p>
<p>– <em>David Hand, Wonderful examples, but let’s not close our eyes,
Statistical Science 29, 2014.</em></p>
</li>
<li>
<p>In terms of characteristics, a data scientist has an inquisitive
mind and is prepared to explore and ask questions, examine assumptions
and analyse processes, test hypotheses and try out solutions and, based
on evidence, communicate informed conclusions, recommendations and
caveats to stakeholders and decision makers.</p>
<p>– <em>Jesús Rogel-Salazar, Data Science and Analytics with Python,
2017.</em></p>
</li>
<li>
<p>With the growing availability of massive data sets and
user-friendly analysis software, it might be thought that there is less
need for training in statistical methods. This would be naïve in the
extreme. Far from freeing us from the need for statistical skills,
bigger data and the rise in the number and complexity of scientific
studies makes it even more difficult to draw appropriate conclusions.
More data means that we need to be even more aware of what the evidence
is actually worth.</p>
<p>– <em>David Spiegelhalter, The Art of Statistics: Learning from Data,
2019.</em></p>
</li>
<li>
<p>The general principles of starting with a well-defined question,
engaging in careful observation, and then formulating hypotheses and
assessing the strength of evidence for and against them became known as
the scientific method.</p>
<p>– <em>Michael Friendly &amp; Howard Wainer, A History of Data
Visualization and Graphic Communication, 2021.</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>Is statistics a science?…If statistics is a science, then what is
its subject matter? Physicists study “stuff”, biologists study life,
astronomers study the universe—what do statisticians study? doi:
10.1198/004017006000000219</p>
<p>– <em>Colin Mallows, Tukey’s Paper After 40 Years, Technometrics,
2006, V 48, No 3. Page 322.</em></p>
</li>
<li>
<p>David Cox was asked to identify “what is statistics.” His answer
was that statistics is the discipline concerned with the study of
variability, the study of uncertainty, and the study of decision making
in the face of uncertainty. … This seems to say that what statisticians
study is their methodology, divorced from applications. doi:
10.1198/004017006000000219</p>
<p>– <em>Colin Mallows, Tukey’s Paper After 40 Years, Technometrics,
2006, V 48, No 3. Page 322.</em></p>
</li>
<li>
<p>The definition [of statistics] I prefer says that: Statistics
concerns the relation of quantitative data to a real-world problem,
often in the presence of variability and uncertainty. It attempts to
make precise and explicit what the data has to say about the problem of
interest. doi: 10.1198/004017006000000219</p>
<p>– <em>Colin Mallows, Tukey’s Paper After 40 Years, Technometrics,
2006, V 48, No 3. Page 322.</em></p>
</li>
<li>
<p>In 1995 Daryl Pregibon and I gave a definition of a massive-data
problem. This is not just a problem for which we have massive data; that
could be simply a problem where classical asymptotic theory is all that
is needed. I have not seen many of those. No, a massive data problem is
a massive problem for which we have massive data. doi:
10.1198/004017006000000219</p>
<p>– <em>Colin Mallows, Tukey’s Paper After 40 Years, Technometrics,
2006, V 48, No 3. Page 323.</em></p>
</li>
<li>
<p>I can say that everything important about statistics that I ever
learned, I learned at lunch at Murray Hill [at Bell Labs 1960-1961]. The
rest of my career has been applying what I learned. doi:
10.1198/004017006000000200</p>
<p>– <em>David Brillinger, Discussion, Technometrics, 2006, V 48 No 3.
Page 325.</em></p>
</li>
<li>
<p>We’re making it less random to make it feel more random.</p>
<p>– <em>Steve Jobs, Steven Levy, 2006, The perfect thing: how the iPod
shuffles commerce, culture, and coolness.</em></p>
</li>
<li>
<p>The only statistics you can trust are those you falsified
yourself</p>
<p>– <em>Winston Churchill</em>
<sub>(<em>#statistics</em>)</sub></p>
</li>
<li>
<p>Statistics must have a clearly defined purpose, one aspect of
which is scientific advance and the other, human welfare and national
development</p>
<p>– <em>P.C. Mahalanobis, quoted by C.R. Rao</em>
<sub>(<em>#statistics,science</em>)</sub></p>
</li>
<li>
<p>It is as scientific and intelligible to represent time by space,
as it is to represent space by space.</p>
<p>– <em>Emma Willard, Guide to the Temple of Time (1850), p. 21</em>
<sub>(<em>#time,space</em>)</sub></p>
</li>
<li>
<p>Can’t duration be imitated and represented as significantly, as
distinctly as space?</p>
<p>– <em>Jacques Barbeu Dubourg, 1753</em>
<sub>(<em>#time,space</em>)</sub></p>
</li>
<li>
<p>As knowledge increases amongst mankind, and transactions
multiply, it becomes more and more desireable to abbreviate and
facilitate the modes of conveying information</p>
<p>– <em>William Playfair, 1786</em> <sub>(<em>#data
visualization</em>)</sub></p>
</li>
<li>
<p>Study the science of art and the art of science. Learn how to
see. Realize that everything connects to everything else.</p>
<p>– <em>Leonardo da Vinci (1452 - 1519)</em>
<sub>(<em>#science</em>)</sub></p>
</li>
<li>
<p>All thinking begins with seeing</p>
<p>– <em>Susanne K Langer, Philosophy in a New Key, 1942</em>
<sub>(<em>#data visualization</em>)</sub></p>
</li>
</ol></main>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Michael Friendly, Kevin Wright, Phil Chalmers.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
