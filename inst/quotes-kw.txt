% Collected by Kevin Wright
% These are quotations related to statistics in some way.
% Many have been extracted from source documents by me.
% Reasonable care has been taken, but errors may exist.


## Computing


Stepwise regression is probably the most abused computerized statistical technique ever devised.  If you think you need stepwise regression to solve a particular problem you have, it is almost certain that you do not. Professional statisticians rarely use automated stepwise regression.
--- Leland Wilkinson, SYSTAT (1984).


It would help if the standard statistical programs did not generate t statistics in such profusion.  The programs might be written to ask, "Do you really have a probability sample?", "By what standard would you judge a fitted coefficient large or small?"  Or perhaps they could merely say, printed in bold capitals beside each equation, "So What Else Is New?"
--- Donald M. McCloskey, "The Loss Function Has Been Mislaid: The Rhetoric of Significance Tests", American Economic Review, Vol 75, #2.


The documentation level of R is already much higher than average for open source software and even than some commercial packages (esp. SPSS is notorious for its attitude of "You want to do one of these things. If you don't understand what the output means, click help and we'll pop up five lines of mumbo-jumbo that you're not going to understand either.")
--- Peter Dalgaard, R-help mailing list 4.2.2002


S has forever altered the way people analyze, visualize, and manipulate data .... S is an elegant, widely accepted, and enduring software system, with conceptual integrity, thanks to the insight, taste, and effort of John Chambers.
--- Association for Computing Machinery Software System Award


Tradition among experienced S programmers has always been that loops (typically 'for' loops) are intrinsically inefficient: expressing computations without loops has provided a measure of entry into the inner circle of S programming.
--- John Chambers, "Programming With Data", p. 173.


While the distribution and publication of Version 2 [of S] was still evolving, parallel research work was starting to shape the next major version. At first this seemed to be a move away from S altogether: something called the "Quantitative Programming Environment" was initially a separate research project, aimed more explicitly at programming and trying to emphasize that users need not be statistically sophisticated. By 1986, however, the decision was made to merge this work with the facilities (especially the graphics) underlying S, to produce a new version of S. (This explains, by the way, why the main program for S is called Sqpe, another one of those little puzzles for users.)
--- Unknown
% Original source (now broken) http://cm.bell-labs.com/cm/ms/departments/sia/S/history.html


This computationally intensive operation [bootstrapping] is not one calculated to endear a user to a database administrator.
--- Leland Wilkinson, "The Grammar of Graphics", p. 49.


Most computer software is not yet intelligent enough to stop the user doing something stupid. The old adage 'Garbage In -> Garbage Out' still hold good, and it must be realized that careful thought and close inspection of the data are vital preliminaries to complicated computer analysis.
--- Christopher Chatfield, 1988, "Problem solving : a statistician's guide", p. 59.


Today...we have high-speed computers and prepackaged statistical routines to perform the necessary calculations...statistical software will no more make one a statistician than would a scalpel turn one into a neurosurgeon. Allowing these tools to do our thinking for us is a sure recipe for disaster.
--- Good & Hardin, "Common Errors in Statistics and How to Avoid Them", p. ix


The generation of random numbers is too important to be left to chance.
--- Robert R. Coveyou, Oak Ridge National Laboratory, 1969


Landon Noll...has been tinkering with random number generators for nearly a decade--an exercise in bringing order to chaos. "There's a lot of beauty in chaos," Noll says. "The Grand Canyon wouldn't be so popular if it was just a uniform trench. The trick is controlling and managing chaos and turning it into something useful."
--- Tom McNichol, "Wired", August, 2003, page 088.


Someone has characterized the user of stepwise regression as a person who checks his or her brain at the entrance of the computer center.
--- D. R. Wittink, "The application of regression analysis". Needham Heights, MA: Allyn and Bacon. p. 259.


The idea of optimization transfer is very appealing to me, especially since I have never succeeded in fully understanding the EM algorithm.
--- Andrew Gellman, "Discussion", Journal of Computational and Graphical Statistics, vol 9, p 49.


This reminds me of the duality in statistics between computation and model fit: better-fitting models tend to be easier to compute, and computational problems often signal modeling problems.
--- Andrew Gelman, "Fooled by randomness"
% http://www.stat.columbia.edu/~cook/movabletype/archives/2006/02/fooled_by_rando.html


It is a nontrivial exercise to correctly program even the simplest split-plot model using PROC MIXED. 
--- Jeremy Aldworth & Wherly P Hoffman, "Split-Plot Model With Covariate: A Cautionary Tale", The American Statistician, 56, 284--289.


Sometimes the most important fit statistic you can get is 'convergence not met'--it can tell you something is wrong with your model.
--- Oliver Schabenberger, 2006 Applied Statistics in Agriculture Conference.


It is obviously pointless to report or quote results to more digits than is warranted. In fact, it is misleading or at the very least unhelpful, because it fails to communicate to the reader another important aspect of the result--namely its reliability! A good rule (sometimes known as Ehrenberg's rule) is to quote all digits up to and including the first two variable digits. 
--- Philipp K. Janert, "Data Analysis with Open Source Tools", O'Reilly, 2010.


## Probability


Doubt is not a pleasant mental state, but certainty is a ridiculous one.
--- Voltaire (1694-1778)


It is a part of probability that many improbable things will happen.
--- Agathon (445 - 400 BC), [Chance News 7.02]


Statistics make it clear the fact that one's chances of being hurt by a bear are far, far fewer than being struck by an auto almost anywhere, or being mugged on a city street, for that matter. We pursue our automotive, urban lives undaunted, often indifferent amid the police and ambulance sirens, but in the Alaskan wilderness we lie awake worrying about bears.
--- John Kauffmann, "Alaska's Brooks Range"


Cougars can be dangerous, especially to unsupervised children, but the chances of becoming a cougar victim are far less than becoming a victim of lightning, honeybees, moose, deer, pit bulls, football, snow-shoveling, or crossing the street in front of your house. For some reason, we fear the true risks of being killed far less than the remote risk of becoming prey.
--- Dennis L Olsen, "Cougars", page 46.


## Data
### Data models


Things which ought to be expected can seem quite extraordinary if you've got the wrong model.
--- David Hand, Significance, 2014, 11, 36-39.


In many applications, the data analyst has a dilemma: Should an effect be classified as [fixed] and a BLUE obtained, or as [random] and a BLUP obtained?  The traditional distinction between fixed and random effects is not helpful; it may, in fact, lead the data analyst to choose the less efficient route.
--- Walter Stroup and D K Mulitze, 1991. "Nearest Neighbor Adjusted Best Linear Unbiased Prediction", The American Statistician, 45, 194--200.


What should be the distribution of random effects in a mixed model?  I think Gaussian is just fine, unless you are trying to write a journal paper.
--- Terry Therneau, Speaking at useR 2007.


Competent scientists do not believe their own models or theories, but rather treat them as convenient fictions. ...The issue to a scientist is not whether a model is true, but rather whether there is another whose predictive power is enough better to justify movement from today's fiction to a new one.
--- Steve Vardeman, 1987. "Comment", Journal of the American Statistical Association, 82 : 130-131.


If you just rely on one model, you tend to amputate reality to make it fit your model.
--- David Brooks
% Found at http://www.johndcook.com/blog/2011/12/01/amputating-reality/


Statistical models are sometimes misunderstood in epidemiology. Statistical models for data are never true. The question whether a model is true is irrelevant. A more appropriate question is whether we obtain the correct scientific conclusion if we pretend that the process under study behaves according to a particular statistical model.
--- Scott Zeger, "Statistical reasoning in epidemiology", American Journal of Epidemiology, 1991


It is not always convenient to remember that the right model for a population can fit a sample of data worse than a wrong model -- even a wrong model with fewer parameters. We cannot rely on statistical diagnostics to save us, especially with small samples. We must think about what our models mean, regardless of fit, or we will promulgate nonsense.
--- Leland Wilkinson, "The Grammar of Graphics", p. 335.


Fitting models to data is a bit like designing shirts to fit people. If you fit a shirt too closely to one particular person, it will fit other people poorly. Likewise, a model that fits a particular data set too well might not fit other data sets well.
--- Rahul Parsa, speaking to the Iowa SAS User's Group


You might say that there's no reason to bother with model checking since all models are false anyway. I do believe that all models are false, but for me the purpose of model checking is not to accept or reject a model, but to reveal aspects of the data that are not captured by the fitted model.
--- Andrew Gelman, "Some thoughts on the sociology of statistics", 2007.
% http://www.stat.columbia.edu/~cook/movabletype/archives/2007/04/some_thoughts_o_3.html


When evaluating a model, at least two broad standards are relevant. One is whether the model is consistent with the data. The other is whether the model is consistent with the 'real world.'
--- Kenneth Bollen, "Structural Equations with Latent Variable"


The point of a model is to get useful information about the relation between the response and predictor variables. Interpretability is a way of getting information. But a model does not have to be simple to provide reliable information about the relation between predictor and response variables; neither does it have to be a data model. The goal is not interpretability, but accurate information.
--- Leo Breiman, "Statistical Modeling: The Two Cultures", Statistical Science, Vol 16, p. 210.


The goals in statistics are to use data to predict and to get information about the underlying data mechanism. Nowhere is it written on a stone tablet what kind of model should be used to solve problems involving data. To make my position clear, I am not against data models per se. In some situations they are the most appropriate way to solve the problem. But the emphasis needs to be on the problem and on the data. Unfortunately, our field has a vested interest in data models, come hell or high water. For instance, see Dempster's (1998) paper on modelling. His position on the 1990 Census adjustment controversy is particularly interesting. He admits that he doesn't know much about the data or the details, but argues that the problem can be solved by a strong dose of modelling. That more modeling can make error-ridden data accurate seems highly unlikely to me.
--- Leo Breiman, "Statistical Modeling: The Two Cultures", Statistical Science, Vol 16, p. 214.


## Data 
### Data analysis


Exploratory data analysis can never be the whole story, but nothing else can serve as the foundation stone--as the first step.
--- John Tukey, "Exploratory Data Analysis", p. 3.


Unless exploratory data analysis uncovers indications, usually quantitative ones, there is likely to nothing for confirmatory data analysis to consider.
--- John Tukey, "Exploratory Data Analysis", p. 3.


One thing the data analyst has to learn is how to expose himself to what his data are willing--or even anxious--to tell him. Finding clues requires looking in the right places and with the right magnifying glass.
--- John Tukey, "Exploratory Data Analysis", p. 21.


In data analysis, a plot of y against x may help us when we know nothing about the logical connection from x to y--even when we do not know whether or not there is one--even when we know that such a connection is impossible.
--- John Tukey, "Exploratory Data Analysis", p. 131.


Whatever the data ,we can try to gain understanding by straightening or by flattening. When we succeed in doing one or both, we almost always see more clearly what is going on.
--- John Tukey, "Exploratory Data Analysis", p. 148.


When nearest neighbor effects exist, the randomized complete block analysis [can be] so poor as to deserver to be called catastrophic.  It [can not] even be considered a serious form of analysis. It is extremely important to make this clear to the vast number of researchers who have near religious faith in the randomized complete block design.
--- Walt Stroup & D Mulitze, "Nearest Neighbor Adjusted Best Linear Unbiased Prediction", The American Statistician, 45, 194--200.


There are two books devoted solely to principal components, Jackson (1991) and Jolliffe (1986), which we think overstates its value as a technique.
--- Venables & Ripley, "Modern Applied Statistics with S", 4th ed., page 305.


...the orthogonal least squares technique does *not* build in a hidden agenda about "predicting" y from x or x from y. If I choose to think that x causes y, or y causes x, that's my business -- and I do, in fact, choose to think this way, at times. But the cause is not in the data, so all I want from the line is a summary description of the data. The rest is up to me. That is why I use the orthogonal least squares line. Compared to the regression line, the line of means, it uses fewer hidden assumptions and is closer to a purely descriptive statistic: *closer* to the facts and nothing but the facts.
--- Joel Levine, "xceptions Are The Rule" p. 31.


Understanding the split-plot isn't everything.  It's the only thing.
--- Oliver Schabenberger, Speaking at JSM 2008.


Residual analysis is similarly unreliable. In a discussion after a presentation of residual analysis in a seminar at Berkeley in 1993, William Cleveland, one of the fathers of residual analysis, admitted that it could not uncover lack of fit in more than four to five dimensions. The papers I have read on using residual analysis to check lack of fit are confined to data sets with two or three variables. With higher dimensions, the interactions between the variables can produce passable residual plots for a variety of models. A residual plot is a goodness-of-fit test, and lacks power in more than a few dimensions. An acceptable residual plot does not imply that the model is a good fit to the data.
--- Leo Breiman, "Statistical Modeling: The Two Cultures", Statistical Science, Vol 16, p. 203.


I was profoundly disappointed when I saw that S-PLUS 4.5 now provides Type III sums of squares as a routine option for the summary method for aov objects. I note that it is not yet available for multistratum models, although this has all the hallmarks of an oversight (that is, a bug) rather than common sense seeing the light of day. When the decision was being taken of whether to include this feature, because the FDA requires it a few of my colleagues and I were consulted and our reply was unhesitatingly a clear and unequivocal No, but it seems the FDA and SAS speak louder and we were clearly outvoted.
--- Bill Venables, "Exegeses on Linear Models"


Some of us feel that type III sum of squares and so-called LS-means are statistical nonsense which should have been left in SAS.
--- Brian Ripley, discussing features of S-Plus, S-news 5.29.1999


I think it would be interesting to ask people who use the results from LSMEANS to explain what the results represent. My guess is that less than 1% of the people who use LSMEANS know what they in fact are.
--- Doug Bates, R-help mailing list, 16 Oct 2003


Doing applied statistics is never easy, especially if you want to get it right.
--- Xiao-Li Meng, 2005 Joint Statistical Meetings


...it must still be said that S-PLUS offers the best environment and suite of tools for actually doing linear modelling on the sorts of consulting jobs that arise in practice. The area covered by just the four fitting functions lm, aov, glm and nls is handled in SAS by an unbelievable array of PROCs each with some special features and other special limitations. Even the notion of "General linear model" in SAS simply means a linear model that is allowed to have *both* factors *and* quantitative explanatory variables. Just how general can you get?
--- Bill Venables, "Exegeses on Linear Models"


I agree with the general message: "The right variables make a big difference for accuracy. Complex statistical methods, not so much." This is similar to something Hal Stern told me once: the most important aspect of a statistical analysis is not what you do with the data, it's what data you use. 
--- Andrew Gelman, "The most important aspect of a statistical analysis is not what you do with the data, it's what data you use", 2018.
% https://statmodeling.stat.columbia.edu/2018/08/07/important-aspect-statistical-analysis-not-data-data-use-survey-adjustment-edition/


Once upon a time, the phrase 'statistical reduction of data' was used as a synonym for statistical analysis; it implied refining and concentrating the data so as eventually to express the main features in a much smaller number of means, indices, coefficients. ... Today some statisticians and some computer programs seem more disposed to undertake 'statistical expansion of data', perhaps with an original 96 observations leading to 25 pages of output.
--- D. J. Finney, "Was This In Your Statistics Textbook: 1. Agricultural Scientist And Statistician", Experimental Agriculture, 24, 153-161.


Data analysis is a tricky business -- a trickier business than even tricky data analysts sometimes think.
--- Bert Gunter, S-news mailing list, 6 Jun 2002


A first analysis of experimental results should, I believe, invariably be conducted using flexible data analytical techniques--looking at graphs and simple statistics--that so far as possible allow the data to 'speak for themselves'. The unexpected phenomena that such a approach often uncovers can be of the greatest importance in shaping and sometimes redirecting the course of an ongoing investigation.
--- George Box, "Signal to Noise Ratios, Performance Criteria, and Transformations", Technometrics, 30, 1--17


When I was in graduate school, a fellow student who was writing his dissertation with the late William G. Cochran passed along some of Cochran's advice: You make a bigger contribution to statistics if you find a workable solution to an important unsolved problem than if you find an optimal solution where a good one already exists.
--- Fred L. Ramsey and Daniel W. Schafer, 2000, The American Statistician, 54, 78.


The six degrees of freedom for error provided by the 4x4 Latin square have long been recognized as inadequate, at least by Fisher.  Something of the order of 12 error degrees of freedom would appear desirable...unless the effects under investigation are large in comparison with their experimental errors.
--- Frank Yates, "Complex Experiments", Supplement to the Journal of the Royal Statistical Society, 1935, Vol 2, No. 1.


The old rule of trusting the Central Limit Theorem if the sample size is larger than 30 is just that--old. Bootstrap and permutation testing let us more easily do inferences for a wider variety of statistics.
--- Tim Hesterberg


A competent data analysis of an even moderately complex set of data is a thing of trials and retreats, of dead ends and branches.
--- John Tukey, "Computer Science and Statistics: Proceedings of the 14th Symposium on the Interface", p. 4.


Scrutiny [of data] should take in the *names* of variates. Analysis of variates y1 to y5 is not statistics; analysis of plant height in centimeters, root weight in grams, etc., may be.
--- D. A. Preece, In discussion of C. Chatfield, "The initial examination of data", Journal of the Royal Statistical Society. Series A (1985), p. 234.


To call in the statistician after the experiment is done may be no more than asking him to perform a postmortem examination: he may be able to say what the experiment died of.
--- R.A. Fisher, Sankya, Indian Statistical Congress, Vol 4, p. 17.


It is clear that a statistician who is involved at the start of an investigation, advises on data collection, and who knows the background and objectives, will generally make a better job of the analysis than a statistician who was called in later on. 
--- Christopher Chatfield, 1988, "Problem solving : a statistician's guide", p. 12.


[I ask consulting clients] to explain the project to me as though he or she was explaining it to his or her parents. (A former colleague with considerable consulting experience substitutes "grandparents" for "parents.")
--- Michael W. Tosset, "Statistical Science", Feb 98, p. 23.


## Data


We really haven't got any great amount of data on the subject, and without data how can we reach any definite conclusions?
-- Thomas Alva Edison (1847–1931)


Small data...fits in memory on a laptop: <10 GB. Medium data...fits in memory on a server: 10 GB-1 TB. Big data...can't fit in memory on one computer: >1 TB.
--- Hadley Wickham, "Big Data Pipelines", 2015.


A *massive* data set is one for which the size, heterogeneity, and general complexity cause serious pain for the analyst(s).
--- J. Kettenring, "Massive data sets...reflections on a workshop", Computing Science and Statistics, Proceedings of the 33rd Symposium on the Interface, Vol 33, 2001.


The Dirty Data Theorem states that "real world" data tends to come from bizarre and unspecifiable distributions of highly correlated variables and have unequal sample sizes, missing data points, non-independent observations, and an indeterminate number of inaccurately recorded values.
--- Unknown, "Statistically Speaking", p. 282.


The Titanic survival data seem to become to categorical data analysis what Fisher's Iris data are to discriminant analysis.
--- Andreas Buja, "A Word from the Editor of JCGS", Statistical Computing & Graphics Newsletter, V10, N1, p 32.


Consideration needs to be given to the most appropriate data to be collected. Often the temptation is to collect too much data and not give appropriate attention to the most important. Filing cabinets and computer files world-wide are filled with data that have been collected because they may be of interest to someone in future. Most is never of interest to anyone and if it is, its existence is unknown to those seeking the information, who will set out to collect the data again, probably in a trial better designed for the purpose. In general, it is best to collect only the data required to answer the questions posed, when setting up the trial, and plan another trial for other data in the future, if necessary.
--- P. Portmann & H. Ketata, "Statistical Methods for Plant Variety Evaluation", p. 15.


We have found that some of the hardest errors to detect by traditional methods are unsuspected gaps in the data collection (we usually discovered them serendipitously in the course of graphical checking).
--- Peter Huber, "Huge data sets", Compstat '94: Proceedings, 1994.


Every messy data is messy in its own way - it's easy to define the characteristics of a clean dataset (rows are observations, columns are variables, columns contain values of consistent types).  If you start to look at real life data you'll see every way you can imagine data being messy (and many that you can't)!
--- Hadley Wickham, R-help mailing list, 17 Jan 2008


What all practicing data analysts agree on is that the proportion of project time spent on data cleaning is huge.  Estimates of 75-90 percent have been suggested.
--- Unknown, "Graphics of Large Datasets", p. 20.


That the ten digits do not occur with equal frequency must be evident to any one making much use of logarithmic tables, and noticing how much faster the first pages wear out than the last ones.
--- Simon Newcomb, "Note on the frequencies of the different digits in natural numbers", Amer. J. Math, 4, 39-40, 1881.


For a hundred years or so, mathematical statisticians have been in love with the fact that the probability distribution of the sum of a very large number of very small random deviations always converges to a normal distribution. This infatuation tended to focus interest away from the fact that, for real data, the normal distribution is often rather poorly realized, if it is realized at all.
--- "Numerical Recipes in C", p 520.


## Data
### Averages


I abhor averages. I like the individual case. A man may have six meals one day and none the next, making an average of three meals per day, but that is not a good way to live.
--- Louis Brandeis


The per capita gross national product of a nation...as a measure of the comfort of individual lives is about as apt, say, as deciding how to dress in the morning according to the mean annual temperature of the region in which one lives. If one lives in the tropics this would work well. But if one lives in Minnesota, where the temperature might be thirty degrees below zero one morning and one hundred degrees above zero another morning, one would be in danger of dying of exposure or of prostration most of the time. The problem with aggregate statistics is that they obscure both the extremes and patterns of distribution.
--- Paul Gruchow, "Grass Roots", p. 44.


In former times, when the hazards of sea voyages were much more serious than they are today, when ships buffeted by storms threw a portion of their cargo overboard, it was recognized that those whose goods were sacrificed had a claim in equity to indemnification at the expense of those whose goods were safely delivered. The value of the lost goods was paid for by agreement between all of those whose merchandise had been in the same ship. This sea damage to cargo in transit was known as 'havaria' and the word came naturally to be applied to the compensation money which each individual was called upon to pay. From this Latin word derives our modern word 'average'.
--- M. J. Moroney, "Facts from Figures", p. 34.


It is difficult to understand why statisticians commonly limit their inquiries to Averages, and do not revel in more comprehensive views.  Their souls seem as dull to the charm of variety as that of the native of one of our flat English counties, whose retrospect of Switzerland was that, if its mountains could be thrown into its lakes, two nuisances would be got rid of at once. An Average is but a solitary fact, whereas if a single other fact be added to it, an entire Normal Scheme, which nearly corresponds to the observed one, starts potentially into existence. Some people hate the very name of statistics, but I find them full of beauty and interest. Whenever they are not brutalised, but delicately handled by the higher methods, and are warily interpreted, their power of dealing with complicated phenomena is extraordinary. They are the only tools by which an opening can be cut through the formidable thicket of difficulties that bars the path of those who pursue the Science of man.
--- Frances Galton, "Natural Inheritance".
% Found in "Francis Galton: Pioneer of Heredity and Biometry" by Michael Bulmer.


## Data
### Outliers


The central limit theorem is often used to justify the assumption of normality when using the sample mean and the the sample standard deviation.  But it is inevitable that real data contain gross errors. Five to ten percent unusual values in a dataset seem to be the rule rather than the exception (Hampel 1973). The distribution of such data is no longer Normal.
--- A. S. Hedayat and Guoqin Su, "Robustness of the Simultaneous Estimators of Location and Scale From Approximating a Histogram by a Normal Density Curve", The American Statistician, 2012, 66, p. 25.


Why is a particular record or measurement classed as an outlier? Among all who handle and interpret statistical data, the word has long been in common use as an epithet for any item among a dataset of N that departs markedly from the broad pattern of the set.
--- David Finney, "Calibration Guidelines Challenge Outlier Practices", The American Statistician, 2006, Vol 60, No 4, p. 310.


Dodge (2003) provided a definition of 'outlier' that is helpful but far from complete: In a sample of N observations, it is possible for a limited number to be so far separated in value from the remainder that they give rise to the question whether they are not from a different population, or that the sampling technique is at fault. Such values are called outliers.
--- David Finney, "Calibration Guidelines Challenge Outlier Practices", The American Statistician, 2006, Vol 60, No 4, p. 310.


The finding of an outlier is not necessarily a discovery of a bad or misleading datum that may contaminate the data, but it may amount to a comment on the validity of distributional assumptions inherent in the form of analysis that is contemplated.
--- David Finney, "Calibration Guidelines Challenge Outlier Practices", The American Statistician, 2006, Vol 60, No 4, p. 312.


If any observation has been classed as an outlier, the next step should be if possible to infer the cause...attention should be given to the possibility that laboratory and data management techniques have been imperfect: improvements and safeguards for the future should be considered.
--- David Finney, "Calibration Guidelines Challenge Outlier Practices", The American Statistician, 2006, Vol 60, No 4, p. 312.


The motivation for any action on outliers must be to improve interpretation of data without ignoring unwelcome truth. To remove bad and untrustworthy data is a laudable ambition, but naive and untested rules may bring harm rather than benefit.
--- David Finney, "Calibration Guidelines Challenge Outlier Practices", The American Statistician, 2006, Vol 60, No 4, p. 312.


One cautious approach is represented by Bernoulli's more conservative outlook. If there are very strong reasons for believing that an observation has suffered an accident that made the value in the data-file thoroughly untrustworthy, then reject it; in the absence of clear evidence that an observation, identified by formal rule as an outlier, is unacceptable then retain it unless there is lack of trust that the laboratory obtaining it is conscientiously operated by able persons who have "...taken every care."
--- David Finney, "Calibration Guidelines Challenge Outlier Practices", The American Statistician, 2006, Vol 60, No 4, p. 313.


Treat outliers like children.  Correct them when necessary, but never throw them out.
--- Top 12 Tip #2.  Practical Stats' Applied Environmental Statistics course


There are a lot of statistical methods looking at whether an outlier should be deleted...I don't endorse any of them.
--- Barry Nussbaum, Significance, Apr 2017.


All this discussion of deleting the outliers is completely backwards.  In my work, I usually throw away all the good data, and just analyze the outliers.
--- Unknown pharmaceutical statistician, The American Statistician, Vol 61, No 3, page 193.


I have often thought that outliers contain more information than the model.
--- Arnold Goodman, 2005 Joint Statistical Meetings


Whatever actually happened, outliers need to be investigated not omitted. Try to understand what caused some observations to be different from the bulk of the observations. If you understand the reasons, you are then in a better position to judge whether the points can legitimately removed from the data set, or whether you’ve just discovered something new and interesting. Never remove a point just because it is weird.
--- Rob J. Hyndman, "Omitting outliers", 2016
% https://robjhyndman.com/hyndsight/omitting-outliers/


## Data
### Tables


Scholars feel the need to present tables of model parameters in academic articles (perhaps just as evidence that they ran the analysis they claimed to have run), but these tables are rarely interpreted other than for their sign and statistical significance.  Most of the numbers in these tables are never even discussed in the text.  From the perspective of the applied data analyst, R packages without procedures to compute quantities of scientific interest are woefully incomplete.  A better approach focuses on quantities of direct scientific interest rather than uninterpretable model parameters. ... For each quantity of interest, the user needs some summary that includes a point estimate and a measure of uncertainty such as a standard error, confidence interval, or a distribution.  The methods of calculating these differ greatly across theories of inference and methods of analysis.  However, from the user's perspective, the result is almost always the same: the point estimate and uncertainty of some quantity of interest.
--- Kousuke Imai, Gary King, Oliva Lau, "Toward a Common Framework for Statistical Analysis and Developmen", Journal of Computational and Graphical Statistics, 2008, v 17.


## Data visualization


The purpose of plotting is to convey phenomena to the viewer's cortex, not to provide a place to lookup observed numbers.
--- Kaye Basford, John Tukey, "Graphical Analysis of Multi-Response Data", p. 373.


Had we started with this [quantile] plot, noticed that it looks straight and not looked further, we would have missed the important features of the data.  The general lesson is important. Theoretical quantile-quantile plots are not a panacea and must be used in conjunction with other displays and analyses to get a full picture of the behavior of the data.
--- John M. Chambers, William S. Cleveland, Beat Kleiner, Paul A. Tukey, "Graphical Methods for Data Analysis", p. 212.


Visualization for large data is an oxymoron--the art is to reduce size before one visualizes.  The contradiction (and challenge) is that we may need to visualize first in order to find out how to reduce size.
--- Peter Huber, "Massive datasets workshop: Four years after", Journal of Computational and Graphical Statistics, Vol 8, 635--652.


Pie charts have severe perceptual problems... If you want to display some data, and perceiving the information is not so important, then a pie chart is fine.
--- "S-Plus 2000 Programmer's Guide", p. 349.


Merely drawing a plot does not constitute visualization. Visualization is about conveying important information to the reader accurately. It should reveal information that is in the data and should not impose structure on the data.
--- W. Huber, X. Li, and R. Gentleman, "Bioinformatics and Computational Biology Solutions using R and Bioconductor", p. 162.


While the dendrogram has been widely used to represent distances between objects, it cannot really be considered to be a visualization method.  Dendrograms do not necessarily expose structure that exists in the data. In many cases they impose structure on the data, and when that is the case it is dangerous to interpret the observed structure.
--- W. Huber, X. Li, and R. Gentleman, "Bioinformatics and Computational Biology Solutions using R and Bioconductor", p. 170.


Chartjunk does not achieve the goals of its propagators. The overwhelming fact of data graphics is that they stand or fall on their content, gracefully displayed. Graphics do not become attractive and interesting through the addition of ornamental hatching and false perspective to a few bars.
--- Edward Tufte, "The Visual Display of Quantitative Information", p. 121.


A table is nearly always better than a dumb pie chart; the only worse design than a pie chart is several of them, for then the viewer is asked to compare quantities located in spatial disarray both within and between pies...Given their low data-density and failure to order numbers along a visual dimension, pie charts should never be used.
--- Edward Tufte, "The Visual Display of Quantitative Information", p. 178.


The preliminary examination of most data is facilitated by the use of diagrams. Diagrams prove nothing, but bring outstanding features readily to the eye; they are therefore no substitutes for such critical tests as may be applied to the data, but are valuable in suggesting such tests, and in explaining the conclusions founded upon them.
--- Ronald A Fisher, "Statistical Methods for Research Workers", p. 27.


Our statistical puritanism may incline us not to use shadows, but we confess that a little bit of shadow is fun.
--- Dan Carr, "Using Layering and Perceptual Grouping in Statistical Graphics", Statistical Computing & Graphics Newsletter, V. 10, N. 1, p. 25.


We are not saying that the primary purpose of a graph is to convey numbers with as many decimal places as possible. We agree with Ehrenberg (1975) that if this were the only goal, tables would be better. The power of a graph is its ability to enable one to take in the quantitative information, organize it, and see patterns and structure not readily revealed by other means of studying the data.
--- William Cleveland & Robert McGill, "Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Models", Journal of the American Statistical Association, 79, 531-554, 1984.


There was a controversy [in the 1920s]...about whether the divided bar chart or the pie chart was superior for portraying the parts of a whole. The contest appears to have ended in a draw. We conclude that neither graphical form should be used because other methods are demonstrably better.
--- William Cleveland & Robert McGill, "Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Models", Journal of the American Statistical Association, 79, 531-554, 1984.


Our conclusion about [choropleth] patch maps agrees with Tukey's (1979), who left little doubt about his opinions by stating, 'I am coming to be less and less satisfied with the set of maps that some dignify by the name *statistical map* and that I would gladly revile with the name *patch map*'.
--- William Cleveland & Robert McGill, "Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Models", Journal of the American Statistical Association, 79, 531--554, 1984.


There is no more reason to expect one graph to 'tell all' than to expect one number to do the same.
--- John Tukey, "Exploratory Data Analysis"


There is no excuse for failing to plot and look.
--- John Tukey, "Exploratory Data Analysis"


It's generally considered bad practice to use more than six colors in a single display.
--- Ross Ihaka, R-help mailing list, 2004
% Original source (now broken) http://tolstoy.newcastle.edu.au/R/help/04/12/9594.html


The mere multiplicity of the attempts to deal with more than three continuous dimensions by encoding additional variables into glyphs, Chernoff faces, stars, Kleiner-Hartigan trees, and so on indicates that each of them has met only with rather limited success.
--- Peter Huber, "Statistical graphics: history and overview", Proceedings of the Fourth Annual Conference and Exposition, p. 674.


Spatial patterns may be due to many sources of variation. In the context of seeking explanations, John Tukey said that, "the unadjusted plot should not be made." In other words, our perceptual/cognitive abilities are poor in terms of adjusting for known source of variations and envisioning the resulting map. A better strategy is to control for known sources of variation and/or adjust the estimates before making the map.
--- Dan Carr, Survey Research Methods Section newsletter, July 2002.


Graphical excellence is that which gives to the viewer the greatest number of ideas in the shortest time with the least ink in the smallest space.
--- Edward R. Tufte, "The Visual Display of Quantitative Information", 1983.


It's not easy to select more than a few clearly distinct colors. Also, "distinct" is context-dependent, because: What will be the spatial relationships of the different colors in your output? You can successfully have fairly similar colors adjacent to each other, since the contrast is more obvious when they're adjacent. However, if you want to use colors to track identity and difference across scattered points or patches, then you need bigger separations between colors, since you want to be able to see easily that patch "A" here is of the same kind as patch "A" there and different from patch "B" somewhere else, when mingled with patches of other kinds. And size matters. Big patches of similar color (as on a map) can look quite distinct, while the same colors used to plot filled circular blobs on a graph might be barely distinguishable, and totally indistinguishable if used to plot colored "."s or "+"s. It depends too on what you will be using to render the colors. Monitor screens vary in their ability to render different colors distinctly, and so do color printers. It's all very psycho-visual and success usually requires experimentation!
--- Ted Harding, R-help mailing list, 2004
% Original source (now broken) http://tolstoy.newcastle.edu.au/R/help/04/12/9520.html


## History


The concept of randomness arises partly from games of chance. The word 'chance' derives from the Latin *cadentia* signifying the fall of a die. The word 'random' itself comes from the French *randir* meaning to run fast or gallop.
--- G. Spencer Brown, "Probability and Scientific Inference", Chapter VII, p. 35.


Statistics derives from a German term, 'Statistik', first used as a substantive by the Gottingen professor Gottfried Achenwall in 1749.
--- Theodore M. Porter, "The Rise of Statistical Thinking 1820-1900".


Strangely, the motto chosen by the founders of the Statistical Society in 1834 was 'Aliis exterendum', which means 'Let other thrash it out.' William Cochran confessed that 'it is a little embarrassing that statisticians started out by proclaiming what they will not do'.
--- Edmund A. Gehan and Noreen A. Lemak, "Statistics in Medical Research: Developments in Clinical Trials"


[Statistics] is not a subject because there is nothing revolutionary about it, but to those leaders of industry who realize that in these days of keen competition and cut prices, any weapon which increases the brain competition and cut prices, any weapon which increases the brain value of judgment of members of their staffs must be an advantage, the subject should have a definite appeal.
--- E. S. Grumell, (1935) "Statistical Methods In Industry, With Special Reference to the Sampling of Coal and Other Materials", Supplement to the Journal of the Royal Statistical Society, Vol 2, No. 1.


What accounts for the success of the [Iowa State] Stat Lab? I believe that it is because it was not driven by the mathematics, but by actual problems in biology, genetics, demography, economics, psychology, and so on.  To be sure, a real problems give rise to abstract problems in statistical inference which have a fascination of their own.  However, for statistics to remain viable, statistical problems should have their genesis in real, data-related problems.
--- Oscar Kempthorne, "A conversation with Oscar Kempthorne", Statistical Science, 1995, V 10, p. 335.


You prepare yourself to win. You prepare yourself for the possibility that you won't win. You don't really prepare yourself for the possibility that you flip the coin in the air and it lands on its edge and you get neither outcome.
--- Al Gore, On the 2004 presidential election, quoted in "Chance News" 10.01.


## Statistics


The invalid assumption that correlation implies cause is probably among the two or three most serious and common errors of human reasoning.
--- Stephen Jay Gould,  "The Mismeasure of Man"


When noise is correlated it becomes music.
--- Anindya Roy, personal communication


As I left consulting to go back to the university, these were the perceptions I had about working with data to find answers to problems: (a) Focus on finding a good solution--that's what consultants get paid for. (b) Live with the data before you plunge into modelling. (c) Search for a model that gives a good solution, either algorithmic or data. (d) Predictive accuracy on test sets is the criterion for how good the model is. (e) Computers are an indispensable partner.
--- Leo Breiman, "Statistical Modeling: The Two Cultures", Statistical Science, Vol. 16, p. 201.


I have always thought that statistical design and sampling from populations should be the first courses taught, but all elementary courses I know of start with statistical methods or probability. To me, this is putting the cart before the horse!
--- Walter Federer, "A Conversation with Walter T Federer", Statistical Science, 2005, Vol 20, p. 312.


[John Kennedy] read every fiftieth letter of the thirty thousand coming weekly to the White House, as well as a statistical summary of the entire batch, but he knew that these were often as organized and unrepresentative as the pickets on Pennsylvania Avenue.
--- Theodore Sorensen, "Kennedy"
% Found in "Sampling: Design and Analysis" by Sharon Lohr, page 23.


Bill Hunter told me that their editor wanted a title for their book with sex appeal. Thus, "Statistics for Experimenters", which is pretty subliminal but it's there.
--- Robert Easterling, The American Statistician, v 58, p 248.


[Canada geese] fly across continents from the Arctic to the tropic with a message from the north and bringing happiness to every one...So also the Statistical Science Association of Canada and the Canadian Journal of Statistics shall spread a message across continents and shall bring home happiness....Like the gushing water at the Horseshoe Falls, Statistical Science Association, through its journal, shall gush out the vast reservoir of knowledge, radiating a beautiful rainbow across the horizon of scientific activities.
--- Unknown
% Found in Statistical Science, Feb 1999, p. 87:


The only useful function of a statistician is to make predictions, and thus to provide a basis for action.
--- William Edwards Deming, Journal of the American Statistical Association
% Quoted in W.A. Wallis' The Statical Research Group, 1942-1945 Volume 75, Number 370, June 1980 (p. 321)


We must watch our own language.  For example, "Type I error" and "Type II error" are meaningless and misleading terms.  Instead, try "chance of a false alarm" and "a missed opportunity".
--- Deborah J. Rumsey, "Assessing Student Retention of Essential Statistical Ideas: Perspectives, Priorities, and Possibilities", American Statistician, Vol 62, No 1, p. 58.


Statistics prove near & far that folks who drive like crazy...are.
--- Burma Shave sign in the Advertising Museum in Portland


It's a random pattern. That's the pattern.
--- Ed Chigliak on the TV series "Northern Exposure"


Randomness is NOT the absence of a pattern.
--- Bill Venables, 1999 S-Plus User's Conference


The statistics on sanity are that one out of every four Americans is suffering from some form of mental illness. Think of your three best friends. If they are okay, then it's you.
--- Rita Mae Brown


It is proven that the celebration of birthdays is healthy. Statistics show that those people who celebrate the most birthdays become the oldest.
--- S. den Hartog, Ph D. Thesis University of Groningen


It has now been proved beyond doubt that smoking is one of the leading causes of statistics.
--- Fletcher Krebel


Because no one becomes statistically self-sufficient after one semester of study, I try to prepare students to become intelligent consumers of the assistance that they will inevitably seek. Service courses train future clients, not future statisticians.
--- Michael W. Tosset, "Statistical Science", Feb 98, p. 24.


If there was ever an idea in statistics which evokes the reaction, "Why the hell didn't I think of that," it has to be the bootstrap.
--- James R. Thompson, 1997 Interface Proceedings


The sensible statistician should be wary of other people's statistics. In particular, it is unwise to believe all official statistics, such as government statements that the probability of a nuclear meltdown is only one in 10,000 years (remember Chernobyl!).
--- Christopher Chatfield, 1988, "Problem solving: a statistician's guide", p 73.


The government are very keen on amassing statistics. They collect them, add them, raise them to the n-th power, take the cube root and prepare wonderful diagrams. But you must never forget that every one of these figures comes in the first instance from the village watchman, who just puts down what he damn pleases.
--- English judge on the subject of Indian statistics; Quoted in Sir Josiah Stamp in Some Economic Matters in Modern Life, London: King and Sons, 1929, pp. 258-259.


It was always important for the biometrician to take part in the field-work for most kinds of trials. A willingness to get our hands dirty did much to dispel the distrust of the theoretician from Head Office, as well as giving us an appreciation of the practical problems. Trials were always carried out to simulate farming conditions as much as possible. We had once advocated a change in wheat plot lengths from 2 chains to 3, based on results from uniformity trials. It seemed a very good idea until a biometrician went to help harvest a very good crop and found he had to lift and carry bags of over 100 lb, this being the yield from each plot. But the local agriculturist in charge of the trial would never have reported this; he would only have gone on grumbling about those 'theory guys' in Head Office forever.
--- Jean Heywood, in "A History of Statistics in New Zealand", edited by H.S.Roberts. p. 23-24.


Econometrics has successfully predicted 14 of the last 3 economic depressions.
--- David Hand, speaking at Interface 2000.


We feel that nothing can replace the value to a [corn] breeder of careful study and understanding of his plants...More and more, we feel that grave danger exists of statistics being used as a substitute for critical observation and thought...Statistics have their place, a very important one, but they can never serve as a substitute for close association with plants. Their real value, it seems to us, is in measuring precisely what we already know in a general way. Statistics tends to be an office art based on machines and figures rather than a field art based on living things.
--- Henry A. Wallace and William L. Brown, "Corn and Its Early Fathers", 1956, p. 123.


The great scientific weakness of America today is that she tends to emphasize quantity at the expense of quality--statistics instead of genuine insight--immediate utilitarian application instead of genuine thought about fundamentals.
--- Henry A. Wallace and William L. Brown, "Corn and Its Early Fathers", 1956, p. 124.


It is easy to lie with statistics, but it is easier to lie without them.
--- Frederick Mosteller


There are aspects of statistics other than it being intellectually difficult that are barriers to learning. For one thing, statistics does not benefit from a glamorous image that motivates students to persist through tedious and frustrating lessons...there are no TV dramas with a good-looking statistician playing the lead, and few mothers' chests swell with pride as they introduce their son or daughter as "the statistician."
--- Chap T. Le and James R. Boen, in "Health and Numbers: Basic Statistical Methods"


[I was 17 when] there was an item in "Barron's" saying if we would send along a description of how we used their statistical material they would publish some of them and pay $5. I wrote up something about how I used odd-lot figures. That $5 was the only money I ever made using statistics.
--- Warren Buffett, "Warren Buffett Speaks", by Janet Lowe, p. 129.


At its core statistics is not about cleverness and technique, but rather about *honesty*. Its real contribution to society is primarily *moral*, not technical. It is about doing *the right thing* when interpreting empirical information. Statisticians are *not* the world's best computer scientists, mathematicians, or scientific subject matter specialists. We *are* (potentially, at least) the best at the *principled* collection, summarization, and analysis of data.
--- Stephen B. Vardeman and Max D. Morris, "Statistics and Ethics: Some Advice for Young Statisticians", The American Statistician, vol 57, p. 21.


Statistical analysis of data can only be performed within the context of selected assumptions, models, and/or prior distributions. A statistical analysis is actually the extraction of substantive information from data and assumptions. And herein lies the rub, understood well by Disraeli and others skeptical of our work: For given data, an analysis can usually be selected which will result in "information" more favorable to the owner of the analysis then is objectively warranted.
--- Stephen B. Vardeman and Max D. Morris, "Statistics and Ethics: Some Advice for Young Statisticians", The American Statistician, vol 57, p. 25.


Too much of what all statisticians do ... is blatantly subjective for any of us to kid ourselves or the users of our technology into believing that we have operated 'impartially' in any true sense. ... We can do what seems to us most appropriate, but we can not be objective and would do well to avoid language that hints to the contrary.
--- Steve Vardeman, 1987, "Comment", Journal of the American Statistical Association, 82, 130-131.


The standard error of most statistics is proportional to 1 over the square root of the sample size.  God did this, and there is nothing we can do to change it.
--- Howard Wainer, "Improving Tabular Displays, With NAEP Tables as Examples and Inspirations", Journal of Educational and Behavioral Statistics, Vol 22, No. 1, pp. 1-30.


Suppose that Sir R. A. Fisher--a master of public relations--had not taken over from ordinary English such evocative words as "sufficient", "efficient", and "consistent" and made them into precisely defines terms of statistical theory.  He might, after all, have used utterly dull terms for those properties of estimators, calling them characteristics A, B, and C. ... Would his work have had the same smashing influence that it did?  I think not, or at least not as rapidly.
--- William H. Kruskal, "Formulas, Numbers, Words: Statistics in Prose", The American Scholar, 1978
% Reprinted in D. Fiske, ed., New Directions for Methodology in Social and Behavioral Sciences, San Francisco: Jossey-Bass, 1981.


Statistics state the status of the state.
--- Leland Wilkinson, "The Grammar of Graphics", p. 165.


My philosophy on lotteries is that while you actually have to buy a ticket in order to win the lottery, buying a ticket does not significantly increase your odds of winning.
--- Howie Smith, personal communication


If you show your friends your confidence interval for the standard error of the estimated length of the confidence interval of your confidence about yourself, I guess one nice thing to ask to freak them out is: "Can you construct a confidence interval for the confidence level of my confidence?
--- Tony Baiching, personal communication


## Statistics 
### Box quotes


To make the preliminary test on variances is rather like putting to sea in a rowing boat to find out whether conditions are sufficiently calm for an ocean liner to leave port.
--- G.E.P. Box, "Non-normality and Tests on Variances", Biometrika, 40, 318-335.


Statistics is, or should be, about scientific investigation and how to do it better, but many statisticians believe it is a branch of mathematics.
--- George Box, Found in AmStat News, Oct 2000, page 11.


These days the statistician is often asked such questions as "Are you a Bayesian?" "Are you a frequentist?" "Are you a data analyst?" "Are you a designer of experiments?". I will argue that the appropriate answer to ALL of these questions can be (and preferably should be) "yes", and that we can see why this is so if we consider the scientific context for what statisticians do.
--- G.E.P. Box


## Statistics
### Tukey quotes


One is so much less than two. [John Tukey's eulogy of his wife.]
--- John Tukey,"The life and professional contributions of John W. Tukey", The Annals of Statistics, 2001, Vol 30, p. 46.


Statisticians classically asked the wrong question--and were willing to answer with a lie, one that was often a downright lie. They asked "Are the effects of A and B different?" and they were willing to answer "no". All we know about the world teaches us that the effects of A and B are always different--in some decimal place--for every A and B. Thus asking "Are the effects different?" is foolish. What we should be answering first is "Can we tell the direction in which the effects of A differ from the effects of B?" In other words, can we be confident about the direction from A to B? Is it "up", "down" or "uncertain"?
--- John Tukey, "The Philosophy of Multiple Comparisons", Statistical Science, 6, 100-116.


No one has ever shown that he or she had a free lunch. Here, of course, "free lunch" means "usefulness of a model that is locally easy to make inferences from".
--- John Tukey, "Issues relevant to an honest account of data-based inference, partially in the light of Laurie Davies' paper".


If asymptotics are of any real value, it must be because they teach us something useful in finite samples. I wish I knew how to be sure when this happens.
--- John Tukey, "Issues relevant to an honest account of data-based inference, partially in the light of Laurie Davies' paper".


George Box: We don't need robust methods. A good statistician (particularly a Bayesian one) will model the data well and find the outliers. John Tukey: They ran over 2000 statistical analyses at Rothamsted last week and nobody noticed anything. A red light warning would be most helpful.
--- George Box vs. John Tukey, argument circa 1989. Cited by R. Douglas Martin, 1999 S-Plus Conference Proceedings.


Statistics is a science in my opinion, and it is no more a branch of mathematics than are physics, chemistry, and economics; for if its methods fail the test of experience--not the test of logic--they will be discarded.
--- John Tukey, quoted in "The life and professional contributions of John W. Tukey" by David Brillinger, The Annals of Statistics, 2001, Vol 30.


One Christmas Tukey gave his students books of crossword puzzles as presents. Upon examining the books the students found that Tukey had removed the puzzle answers and had replaced them with words of the sense: "Doing statistics is like doing crosswords except that one cannot know for sure whether one has found the solution."
--- John Tukey, quoted in "The life and professional contributions of John W. Tukey" by David Brillinger, The Annals of Statistics, 2001, Vol 30, p. 22.


A sort of question that is inevitable is: "Someone taught my students exploratory, and now (boo hoo) they want me to tell them how to assess significance or confidence for all these unusual functions of the data. Oh, what can we do?" To this there is an easy answer: TEACH them the JACKKNIFE.
--- John Tukey, "We Need Both Exploratory and Confirmatory", The American Statistician, Vol 34, No 1, p. 25.


John Tukey's eye for detail was amazing. When we were preparing some of the material for our book (which was published last year), it was most disconcerting to have him glance at the data and question one value out of several thousand points. Of course, he was correct and I had missed identifying this anomaly.
--- Kaye Basford
% Original source (now broken) http://stat.bell-labs.com/who/tukey/tributes.html


Many students are curious about the '1.5 x IQR Rule';, i.e. why do we use Q1 - 1.5 x IQR (or Q3 + 1.5 x IQR) as the value for deciding if a data value is classified as an outlier? Paul Velleman, a statistician at Cornell University, was a student of John Tukey, who invented the boxplot and the 1.5 x IQR Rule. When he asked Tukey, 'Why 1.5?', Tukey answered, 'Because 1 is too small and 2 is too large.' [Assuming a Gaussian distribution, about 1 value in 100 would be an outlier. Using 2 x IQR would lead to 1 value in 1000 being an outlier.]
--- Unknown


It is a rare thing that a specific body of data tells us as clearly as we would wish how it itself should be analyzed.
--- John Tukey, "Exploratory Data Analysis", p. 397.


Just which robust/resistant methods you use is not important--what is important is that you use some. It is perfectly proper to use both classical and robust/resistant methods routinely, and only worry when they differ enough to matter. But, when they differ, you should think hard.
--- John Tukey, 1979, quoted by Doug Martin


## Statistics
### Bayesian

We thus echo the classical Bayesian literature in concluding that “noninformative prior information” is a contradiction in terms. The flat prior carries information just like any other; it represents the assumption that the effect is likely to be large. This is often not true. Indeed, the signal-to-noise ratio β/s is often very low and then it is necessary to shrink the unbiased estimate. Failure to do so by inappropriately using the flat prior causes overestimation of effects and subsequent failure to replicate them.
--- Erik van Zwet & Andrew Gelman, A proposal for informative default priors scaled by the standard error of estimates, The American Statistician, 76, p. 7.


Another reason for the applied statistician to care about Bayesian inference is that consumers of statistical answers, at least interval estimates, commonly interpret them as probability statements about the possible values of parameters. Consequently, the answers statisticians provide to consumers should be capable of being interpreted as approximate Bayesian statements.
--- Donald B. Rubin. Bayesianly justifiable and relevant frequency calculations for the applied statistician. Annals of Statistics, 12(4):1151–1172, 1984.


In most cases the frequentist adopts numerical values because they are convenient in that the calculations can be easily performed. For instance, a reliability engineer will use an exponential distribution or, if that is too gross, a Weibull. In the majority of frequentist analyses there is little justification for the assumed likelihood, and it is as subjective as any prior. 
--- D. V. Lindley, "Discussion", The American Statistician, August 1997, Vol. 51, page 265. 


In contrast to the logical development and intuitive interpretations of the Bayesian approach, frequentist methods are nearly impossible to understand, even for the best students. Consider confidence intervals. Many instructors err in describing confidence intervals and even some texts err. But whether texts or instructors err in explaining them, students do not understand them. And they carry this misunderstanding with them into later life. Calculating a confidence interval is easy. But everyone except the cognoscenti believes that when one calculates 95% confidence limits of 2.6 and 7.9, say, the probability is 95% that the parameter in question lies in the interval from 2.6 to 7.9. P values are nearly as obscure as confidence intervals. Diamond and Forrester (1983) make it clear that MDs [Medical Doctors] do not understand P values (and in particular that they themselves do not understand P values!). Students in frequentist courses may learn very well how to calculate confidence intervals and P values, but they cannot give them correct interpretations. I stopped teaching frequentist methods when I decided that they could not be learned.
--- Donald A. Berry, "Teaching Elementary Bayesian Statistics with Real Applications in Science", The American Statistician, 51, p 242.


Uniform priors on probabilities are ubiquitous. I agree that they can be useful. However, if the probability in question is the prevalence of HIV in California, it is ridiculous to assert that a prevalence of 100% is equally plausible with 0%, or that the chance that it is above 50% is the same as the chance that it is below 50%. It is particularly noxious to call such a prior noninformative. Instead, it is disinformative. The likelihood notwith standing, positing a uniform prior for the probability that the sun will rise tomorrow is equally ridiculous, given that we are all confident that it has risen all the days of our lives.
--- Wesley O. Johnson, "Comment: Bayesian Statistics in the Twenty First Century", The American Statistician, Feb 2013, 67, p 10.


The best way to convey to the experimenter what the data tell him about theta is to show him a picture of the posterior distribution.
--- G. E. P. Box & G. C. Tiao, "Bayesian Inference in Statistical Analysis" (1973)


If one could get some rational basis for obtaining the prior, then there would be no problem.  But people have seminars these days about something where someone says, 'I am going to use such and such a prior'.  Where does he get the prior?  It is not data based.  It is a mathematical convenience or something like that.  It is not even obtained by using Bayes' theorem. Why one should believe the outcome of using this seems to be a very moot point.
--- Oscar Kempthorne, "A conversation with Oscar Kempthorne", Statistical Science, 1995, V 10, p. 333.


In the design of experiments, one has to use some informal prior knowledge.  How does one construct blocks in a block design problem for instance?  It is stupid to think that use is not made of a prior.  But knowing that this prior is utterly casual, it seems ludicrous to go through a lot of integration, etc., to obtain 'exact' posterior probabilities resulting from this prior.  So, I believe the situation with respect to Bayesian inference and with respect to inference, in general, has not made progress.  Well, Bayesian statistics has led to a great deal of theoretical research.  But I don't see any real utilizations in applications, you know.  Now no one, as far as I know, has examined the question of whether the inferences that are obtained are, in fact, realized in the predictions that they are used to make.
--- Oscar Kempthorne, "A conversation with Oscar Kempthorne", Statistical Science, 1995, V 10, p. 334.


I sometimes think that the only real difference between Bayesian and non-Bayesian hierarchical modelling is whether random effects are labeled with Greek or Roman letters.
--- Peter Diggle, "Comment on Bayesian analysis of agricultural field experiments", 1999, J. Royal Statistical Society B, 61, 691--746.


The practicing Bayesian is well advised to become friends with as many numerical analysts as possible.
--- James Berger, "Statistical Decision Theory and Bayesian Analysis", p. 202.


You just say "Bayesian," and people think you are some kind of genius.
--- Gary Churchill, "Bayes offers a new way to make sense of numbers", Science, 19 Nov 1999.


Bayesian computations give you a straightforward answer you can understand and use. It says there is an X% probability that your hypothesis is true-not that there is some convoluted chance that if you assume the null hypothesis is true, you'll get a similar or more extreme result if you repeated your experiment thousands of times. How does one interpret THAT!
--- Steven Goodman, "Bayes offers a new way to make sense of numbers", Science, 19 Nov 1999.


Bayesian methods are complicated enough, that giving researchers user-friendly software could be like handing a loaded gun to a toddler; if the data is crap, you won't get anything out of it regardless of your political bent.
--- Brad Carlin, "Bayes offers a new way to make sense of numbers", Science, 19 Nov 1999.


If a study, even a statistically significant one, suggests that pigs can fly, Bayes's theorem allows researchers to combine the study's results mathematically with hundreds of years of knowledge about the travel habits of swine.
--- David Leonhardt, New York Times, April 28, 2001 


If the prior distribution, at which I am frankly guessing, has little or no effect on the result, then why bother; and if it has a large effect, then since I do not know what I am doing how would I dare act on the conclusions drawn?
--- Richard W Hamming, "The Art of Probability for Scientists and Engineers", 1991, p. 298.


I believe that there are many classes of problems where Bayesian analyses are reasonable, mainly classes with which I have little acquaintance.
--- John Tukey, The life and professional contributions of John W. Tukey, The Annals of Statistics, 2001, Vol 30, p. 45.
%% Note: Given Tukey's breadth of knowledge, this may have been a subtle jab at Bayesians.


If you read Bayesian polemics from the 1970s and 1980s—including my own—it's usually arrogant and even insulting. Some of the terms were excessively pointed. For example, Bayesians identified which frequentist methods were "incoherent", or more accurately, lamented that none seemed to be coherent. On the other hand, Bayesians were accused of being "biased". The rhetoric was not all that different from that of the Fisher/Pearson duels. But we Bayesians have stopped saying derogatory things, partly because we have changed and partly because frequentists have been listening. When you're walking beside someone you tend to be cordial; when you're trying to catch up to tell them something and they are ignoring what you say, you sometimes yell.
--- Don Berry, "Celebrating 70: An Interview with Don Berry", Dalene Stangl, Lurdes Y. T. Inoue and Telba Z. Irony. Statistical Science, 2012, Vol. 27, No. 1, 144–159. 
% DOI: 10.1214/11-STS366.


## Statistics
### Experimental design


The traditional methods design of experiments are taught and/or discussed in textbooks are not the ways design of experiments are or should be used for real-world applications.
--- George Milliken, Applied Statistics in Agriculture Conference, 2009


The statistician who supposes that his main contribution to the planning of an experiment will involve statistical theory, finds repeatedly that he makes his most valuable contribution simply by persuading the investigator to explain why he wishes to do the experiment, by persuading him to justify the experimental treatments, and to explain why it is that the experiment, when completed, will assist him in his research.
--- Gertrude Cox, Lecture in Washington 11 January 1951
% Found in Statistically Speaking, p. 84


An important distinction needs to be made between experimental designs using complete blocks and those using incomplete blocks as regards to three functions: 1. reducing the error mean square 2. adjusting estimates closer to true values, and  3. refining rankings. Complete blocks include all treatments whereas incomplete blocks include a subset of the treatments. Both can reduce the residual error, but only incomplete blocks can also adjust estimates of treatment effects closer to the true values and thereby refine rankings among the treatments. These adjusted estimates are usually more accurate than the raw averages over replicates, but not always (exactly as is the case for accuracy gain through more replication).  Likewise, these adjusted rankings are more likely to identify correctly the best treatments. By declaring a smaller error but doing nothing to sharpen estimates or refine rankings, complete block designs are rather impotent. It is ironic that scientists rarely understand this huge difference between getting one benefit or three from blocking.
--- Hugh G Gauch, "Three Strategies for Gaining Accuracy", American Scientist.


On a final note, we would like to stress the importance of design, which often does not receive the attention it deserves. Sometimes, the large number of modeling options for spatial analysis may raise the false impression that design does not matter, and that a sophisticated analysis takes care of everything. Nothing could be further from the truth.
--- Hans-Peter Piepho, Martin P. Boer, Emlyn R. Williams, Two-dimensional P-spline smoothing for spatial analysis of plant breeding trials, "Biometrical Journal", Feb 2022.
% https://doi.org/10.1002/bimj.202100212.


At this meeting for the hybrid corn industry, we have no reservation about recommending a design which consists of a single replicate of treatments at a given location.  For some audiences, such a statement can severely damage the reputation of the person making the statement.  University experiment station personnel in particular regard replications within an environment as a necessary part of good research.  They are not.  Sprague (1955) and many others have shown most researchers otherwise.
--- R. E. Stucker & D. R. Hicks. "Experimental Design and Plot Size Considerations for On-Farm Research", Proceedings of the 46th Annual Corn and Sorghum Industry Research Conference , 1991, p. 60.


The message from a statistician's point of view is very clear.  Replicate over environments, do not replicate within environments.  This is not news.  At North Carolina State in the early 60s, any graduate student interested in quantitative aspects of plant breeding and genetics had a standard answer for the number of replicates needed in an experiment: use one replicate if you're estimating means, and use two replicates if you're estimating variances.  Implicit in the answer was, "the experiment will be evaluated in more than one environment".
--- R. E. Stucker & D. R. Hicks. "Experimental Design and Plot Size Considerations for On-Farm Research", Proceedings of the 46th Annual Corn and Sorghum Industry Research Conference , 1991, p. 62.


## Statistics
### Significance


Which I would like to stress are: (1) A significant effect is not necessarily the same thing as an interesting effect. (2) A non-significant effect is not necessarily the same thing as no difference.
--- Christopher Chatfield, 1988, "Problem solving : a statistician's guide", p. 51.


In medical research, too often the first published study testing a new treatment provides the strongest evidence that will ever be found for that treatment.  As better controlled studies--less vulnerable to the enthusiasms of researchers and their sponsors--are then conducted, the treatment's reported efficacy declines.  Years after the initial study [...] sometimes the only remaining issue is whether the treatment is in fact harmful.
--- Edward Tufte, "Beautiful Evidence", p. 144.


Rejection of a true null hypothesis at the 0.05 level will occur only one in 20 times. The overwhelming majority of these false rejections will be based on test statistics close to the borderline value. If the null hypothesis is false, the inter-ocular traumatic test ["hit between the eyes"] will often suffice to reject it; calculation will serve only to verify clear intuition.
--- W. Edwards, Harold Lindman, Leonard J. Savage (1962), "Bayesian Statistical Inference for Psychological Research", University of Michigan


When statistical inferences, such as p-values, follow extensive looks at the data, they no longer have their usual interpretation. Ignoring this reality is dishonest: it is like painting a bull’s eye around the landing spot of your arrow. This is known in some circles as p-hacking, and much has been written about its perils and pitfalls
---  Kass RE, Caffo BS, Davidian M, Meng X-L, Yu B, Reid N (2016). "Ten Simple Rules for Effective Statistical Practice", PLoS Comput Biol 12(6):e1004961.
% doi:10.1371/journal.pcbi.1004961


The difference between "statistically significant" and "not statistically significant" is not in itself necessarily statistically significant. By this, I mean more than the obvious point about arbitrary divisions, that there is essentially no difference between something significant at the 0.049 level or the 0.051 level. I have a bigger point to make. It is common in applied research--in the last couple of weeks, I have seen this mistake made in a talk by a leading political scientist and a paper by a psychologist--to compare two effects, from two different analyses, one of which is statistically significant and one which is not, and then to try to interpret/explain the difference. Without any recognition that the difference itself was not statistically significant.
--- Andrew Gelman, "The difference between 'statistically significant' and 'not statistically significant' is not in itself necessarily statistically significant, 2005
% http://www.stat.columbia.edu/~cook/movabletype/archives/2005/06/the_difference.html"
% Also: The Difference Between "Significant" and "Not Significant" is not Itself Statistically Significant,
% The American Statistician, 2006, Vol 60, 328--331.


The p-value is a concept so misaligned with intuition that no civilian can hold it firmly in mind.  Nor can many statisticians.
--- Matt Briggs, "Why do statisticians answer silly questions that no one ever asks?", Significance, Vol 9, No 1, p. 30.


A quotation of a p-value is part of the ritual of science, a sprinkling of the holy waters in an effort to sanctify the data analysis and turn consumers of the results into true believers.
--- William Cleveland, "Visualizing Data", p. 177.


We should push for de-emphasizing some topics, such as statistical significance tests--an unfortunate carry-over from the traditional elementary statistics course. We would suggest a greater focus on confidence intervals---these achieve the aim of formal hypothesis testing, often provide additional useful information, and are not as easily misinterpreted.
--- Gerry Hahn et. al, "The Impact of Six Sigma Improvement--A Glimpse Into the Future of Statistics", The American Statistician, August 1999.


We statisticians must accept much of the blame for cavalier attitudes toward Type I errors. When we teach practitioners in other scientific fields that multiplicity is not important, they believe us, and feel free to thrash their data set mercilessly, until it finally screams "uncle" and relinquishes significance. The recent conversion of the term "data mining" to mean a statistical *good* rather than a statistical *evil* also contributes to the problem.
--- Peter Westfall. "Applied Statistics in Agriculture (Proceedings of the 13th annual conference)", page 5.


While the main emphasis in the development of power analysis has been to provide methods for assessing and increasing power, it should also be noted that it is possible to have too much power. If your sample is too large, nearly any difference, no matter how small or meaningless from a practical standpoint, will be 'statistically significant'. 
--- Clay Helberg
% Original source (now broken) http://www.execpc.com/~helberg/pitfalls/


...as I pointed out earlier, significance (in the statistical sense) is really as much a function of sample size and experimental design as it is a function of strength of relationship. With low power, you may be overlooking a really useful relationship; with excessive power, you may be finding microscopic effects with no real practical value.
--- Clay Helberg
% Original source (now broken) http://www.execpc.com/~helberg/pitfalls/


Remember that a p-value merely indicates the probability of a particular set of data being generated by the null model--it has little to say about the size of a deviation from that model (especially in the tails of the distribution, where large changes in effect size cause only small changes in p-values).
--- Clay Helberg
% Original source (now broken) http://www.execpc.com/~helberg/pitfalls/


Given what I know about data, models, and assumptions, I find more than 2 significant digits of printout for a p-value to be indefensible. (I actually think 1 digit is about the max).
--- Terry Therneau, S-news mailing list, 8 Nov 2000


In the calculus of real statistical inference, and by that I mean actual data problems (which S was designed for), all p-values < 10^-6 or so are identical. This is one of the few areas in fact where I like SAS better: the creators of their PROCs are smart enough to print these numbers as zero and leave it at that. There are no Gaussian distributions in the real world, and the central limit theorem has failed long, long before 10^-17.
--- Terry Therneau, S-news mailing list, 4 Apr 2002


It's a commonplace among statisticians that a chi-squared test (and, really, any p-value) can be viewed as a crude measure of sample size: When sample size is small, it's very difficult to get a rejection (that is, a p-value below 0.05), whereas when sample size is huge, just about anything will bag you a rejection. With large n, a smaller signal can be found amid the noise. In general: small n, unlikely to get small p-values. Large n, likely to find something. Huge n, almost certain to find lots of small p-values.
--- Andrew Gelman, "The sample size is huge, so a p-value of 0.007 is not that impressive", 2009
% http://www.stat.columbia.edu/~cook/movabletype/archives/2009/06/the_sample_size.html


Work by Bickel, Ritov, and Stoker (2001) shows that goodness-of-fit tests have very little power unless the direction of the alternative is precisely specified. The implication is that omnibus goodness-of-fit tests, which test in many directions simultaneously, have little power, and will not reject until the lack of fit is extreme.
--- Leo Breiman, "Statistical Modeling: The Two Cultures", Statistical Science, Vol 16, p. 203.
